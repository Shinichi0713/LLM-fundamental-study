RLHF（Reinforcement Learning from Human Feedback：人間からのフィードバックを用いた強化学習）は、ChatGPTなどの高性能な生成AIを、**「より人間に役立ち、安全で、自然な対話ができるように調整する」**ための非常に重要な技術です。

従来のAIが抱えていた「理屈は合っているが、使いにくい」という課題を解決するために導入されました。


## 1. 解決したかった従来の課題（なぜRLHFが必要だったのか）

AIモデル（大規模言語モデル）の最初の学習ステージである「事前学習」だけでは、以下の3つの大きな問題がありました。

* 目的の不一致（アライメント問題）:
  事前学習の目的は「次の単語を予測すること」です。そのため、AIは「爆弾の作り方を教えて」という問いに対し、ネット上の情報を元に「親切に詳しく答えてしまう」ことがありました。これは人間がAIに求めている「安全性」や「倫理性」とは一致しません。
* 「正解」を数式で定義できない:
  「面白い文章」「親切な回答」「簡潔な要約」といった基準は主観的であり、「単語の一致率」などの従来の数学的な指標（損失関数）では正確に評価できませんでした。
* 嘘（ハルシネーション）や有害な表現:
  ネット上の膨大なデータを学習するため、データに含まれる偏見や差別、誤った情報をそのまま出力してしまう傾向がありました。


## 2. RLHFの3つのステップ

RLHFは、AIが「人間の好み」を学習するために、大きく分けて以下の3段階で進められます。

1. SFT（教師ありファインチューニング）:
   人間が作成した「理想的な回答例」をAIに学習させます。これにより、AIは「対話の形式」を学びます。
2. 報酬モデル（Reward Model）の作成:
   AIが複数の回答を生成し、人間がそれらを「良い順」にランク付けします。その評価結果を元に、**「人間の好みをスコア化する別のAI（報酬モデル）」**を訓練します。
3. 強化学習（PPOなどによる最適化）:
   メインのAIが回答を生成し、それを「報酬モデル」が採点します。スコアが高ければ褒められ、低ければ修正されるというサイクル（強化学習）を何百万回と繰り返すことで、人間の価値観に沿った振る舞いを身につけます。


## 3. RLHFの主な特徴とメリット

* 「人間らしさ」の獲得:
  単なる事実の羅列ではなく、人間が心地よいと感じるトーンや、文脈を汲み取った回答が可能になります。
* 安全性の向上（ガードレールの構築）:
  不適切な要求に対して「その質問には答えられません」と断るような、倫理的な判断力をAIに持たせることができます。
* 主観的な品質の向上:
  「要約の質」や「プログラミングコードの読みやすさ」など、定量化しにくいスキルの向上に非常に効果的です。


## まとめ：AIを「賢い百科事典」から「信頼できるアシスタント」へ

RLHFは、AIの知能を「ネット情報の統計的な模倣」から、**「人間の意図（Intent）を理解し、社会的なルールに従う」**レベルへと引き上げた、生成AI革命の影の立役者と言えます。

> **補足:** > RLHFには、人間の評価コストが高いことや、評価者のバイアスがAIに反映されてしまうといった新たな課題もあり、最近ではAI自身が評価を行う「RLAIF（AIからのフィードバックを用いた強化学習）」などの研究も進んでいます。

RLHFにおける強化学習のステップは、単に「AIを褒める」だけではなく、数学的に非常に緻密な設計がなされています。特に中心となるアルゴリズム**PPO (Proximal Policy Optimization)**がどのように構築され、どのようなロジックで動いているのかを解説します。

---

## 1. RLHFを構成する「3つのモデル」

アルゴリズムの構築を理解するために、まず登場人物（モデル）を整理しましょう。

1. **Policy（方策）モデル**: 訓練対象のLLM（ChatGPTの元となるモデル）。
2. **Reference（参照）モデル**: 学習前のSFTモデル。Policyが「変な方向」に学習しすぎないよう見守る役。
3. **Reward（報酬）モデル**: 人間の好みを学習した「採点用」モデル。

---

## 2. 報酬モデル (Reward Model) の構築ロジック

「どの回答がより良いか」をAIに教えるために、報酬モデルは以下の数式（損失関数）を用いて構築されます。

人間が回答Aを回答Bより好んだ場合、報酬モデル  の出力が  となるように学習します。具体的には、**Pairwise Ranking Loss**を使用します。

* : 選ばれた回答
* : 却下された回答
* : シグモイド関数

これにより、人間が好む回答に高い「スコア（報酬）」を出す採点AIが出来上がります。

---

## 3. PPOアルゴリズムによる最適化

次に、この報酬モデルを使ってPolicyモデル（LLM）を強化します。ここで使われるのが**PPO**です。PPOを構築する際、開発者が解決したかったのは**「強化学習の不安定さ」**でした。

### なぜPPOなのか？

従来の強化学習では、一度の更新でモデルが大きく変わりすぎると、性能が急激に崩壊する問題がありました。PPOは「モデルの変化を一定の範囲内に収める」ことで、安定した学習を実現します。

### PPOの目的関数（Objective Function）

PPOの学習では、以下の合計値を最大化するように計算します。

1. **報酬の最大化 ()**: 報酬モデルから高いスコアをもらえる回答をする。
2. **KLダイバージェンス（ペナルティ）**: 右側の項は、学習中のモデル () が、元のモデル () から離れすぎないようにする制約です。

> **なぜ制約が必要か（報酬ハッキングの防止）**:
> 制約がないと、AIは報酬モデルの「穴」を見つけ出し、人間から見れば支離滅裂だが報酬モデルだけが高い点数をつけてしまう「意味不明な単語の羅列」を生成するようになります。これを**Reward Hacking**と呼びます。

---

## 4. アルゴリズム構築のフロー（実装のコツ）

実装上、PPOを適用する際は以下のループを回します。

1. **データ生成**: 学習中のLLMにプロンプトを与え、回答を生成させる。
2. **採点**: 生成した回答を「報酬モデル」に渡し、スコアを得る。
3. **KL計算**: 「参照モデル」と「学習中のモデル」の確率分布を比較し、変化量を計算する。
4. **モデル更新**: 報酬が高く、かつ変化が大きすぎない範囲で、勾配法を用いてモデルの重みを更新する。

### PPOの特徴：クリッピング (Clipping)

PPOの最もユニークな点は、確率比を  （通常 ）の範囲に強制的に収める「クリッピング」機能です。これにより、極端なパラメータ更新が抑制され、言語モデルのような巨大なモデルでも安定して強化学習を行うことが可能になりました。

---

## まとめ：RLHFの数学的な美しさ

RLHFとPPOの組み合わせは、**「主観的な人間の好み（Reward Model）」**を**「数学的に安定した確率的な更新（PPO）」**に落とし込んだ点が画期的です。

* **全結合層**: 特徴を混ぜる。
* **活性化関数**: 非線形性を入れる。
* **PPO/RLHF**: 「人間社会のルール」に適応させる。

このように、階層的にモデルが形作られていると考えると、生成AIの構造がよりクリアに見えてくるはずです。

---

**次にお手伝いできることはありますか？**
例えば、「報酬ハッキングが実際に起きた時の具体例」や、「PPOに代わる新しい手法（DPOなど）」について解説することも可能です。


