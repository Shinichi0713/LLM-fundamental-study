RLHF（Reinforcement Learning from Human Feedback：人間からのフィードバックを用いた強化学習）は、ChatGPTなどの高性能な生成AIを、 **「より人間に役立ち、安全で、自然な対話ができるように調整する」** ための非常に重要な技術です。

従来のAIが抱えていた「理屈は合っているが、使いにくい」という課題を解決するために導入されました。

![1767137098530](image/explanation/1767137098530.png)

## 解決したかった従来の課題（なぜRLHFが必要だったのか）

LLMはネットワークを構築後に「事前学習」と呼ばれる学習をします。
事前学習は自己回帰予測と呼ばれる、文章の並びから次に来るべき言葉を予測するということです。

LLMの最初の学習ステージである事前学習だけでは、以下の3つの大きな問題がありました。

* 目的の不一致（アライメント問題）:
  事前学習の目的は「次の単語を予測すること」です。そのため、AIは「爆弾の作り方を教えて」という問いに対し、ネット上の情報を元に「親切に詳しく答えてしまう」ことがありました。これは人間がAIに求めている「安全性」や「倫理性」とは一致しません。
* 「正解」を数式で定義できない:
  「面白い文章」「親切な回答」「簡潔な要約」といった基準は主観的であり、「単語の一致率」などの従来の数学的な指標（損失関数）では正確に評価できませんでした。
* 嘘（ハルシネーション）や有害な表現:
  ネット上の膨大なデータを学習するため、データに含まれる偏見や差別、誤った情報をそのまま出力してしまう傾向がありました。

>**数式で定義できないとは**  
>1. 数式で書ける（客観的な正解がある）
>例. これらは計算機が100%の自信を持って「正解・不正解」を判定できるため、RLHFは不要です。
>算数・数学: $1 + 1$ の答えが $2$ である。
>2. 計算式: $Loss = |Output - 2|$
>答えが $2$ から離れるほど、機械的に「間違い」と判定できます。
>プログラミング: コードがエラーなく実行でき、期待されるテストケースをパスするか。
>判定式: if output == expected_result: return 1.0 else: return 0.0単語の一致
>（BLEUスコアなど）: 翻訳タスクなどで、参考訳の単語とどれだけ一致しているか。統計的な一致率（パーセンテージ）で計算できます。
>2. 数式で書けない（主観的な正解しかない）例
>ここからが、RLHF（人間のフィードバック）が必要になる領域です。
>「親切な」お断りメール:
>「無理です」と「せっかくのお申し出ですが、今回はご希望に添いかねます」
>どちらも「拒絶」という事実は同じですが、後者の方が「良い」とされます。しかし、「丁寧さ」を算出する万能な数式は存在しません。
>「面白い」ジョーク:
>単語の並びをどれだけ計算しても、それが「笑えるかどうか」を判定する物理定数は存在しません。
>「安全な」回答:
>爆弾の作り方を教えない、差別的な表現を含まない。これらは文脈や社会的な通念に依存するため、単純な「NGワードリスト（数式の一種）」だけでは回避しきれません。

人が欲しい回答は事前学習だけでは学習できません。

## RLHFの仕組み
**RLHF（Reinforcement Learning from Human Feedback：人間からのフィードバックによる強化学習）**は、ChatGPTなどのLLMが「より人間らしく、安全で、役に立つ回答」ができるようにするための調律技術です。

理屈をシンプルに整理すると、**「教科書で学んだ秀才（LLM）に、人間の好みという『正解のない感覚』を教え込むプロセス」**と言えます。

RLHFは、AIが「人間の好み」を学習するために、大きく分けて以下の3段階で進められます。

### ステップ1：指示学習（SFT: Supervised Fine-Tuning）

まず、ベースとなるAIモデルに「質問に対してどう答えるべきか」の模範解答を学習させます。

* **やること:** 人間が書いた「高品質なQ&Aリスト」をAIに読み込ませます。
* **状態:** この時点では、AIは「それっぽい回答」ができるようになりますが、まだ「正確だけど冷たい」「有害な情報を平気で出す」といった問題が残っています。

### ステップ2：報酬モデル（RM: Reward Model）の作成

ここがRLHFの肝です。 **何が人間にとって良い回答か」を判定する別のAI（審判員）** を作ります。

* **やること:**
1. AIに同じ質問に対して複数の回答（A案、B案、C案）を作らせます。
2. **人間**がそれを見て「Aが一番良い、Cはダメ」とランキングをつけます。
3. この「人間のランキング」を学習させた**「報酬モデル（審判AI）」**を構築します。


* **役割:** この審判AIは、回答を受け取ると「人間なら何点つけるか」を予測してスコアを出せるようになります。

### ステップ3：強化学習（PPOなどのアルゴリズム）

最後に、AI（モデル）と審判（報酬モデル）を戦わせて、AIを鍛え上げます。

* **やること:** AIが回答を生成し、それを審判AIが採点します。
* **仕組み:**
* 高い点数をもらえたら：その方向性を強化する。
* 低い点数だったら：その話し方を修正する。


* **工夫:** 元のAIの良さ（知識量）が壊れないように、元のモデルから離れすぎないように制御（KL発散の抑制）しながら更新します。


## RLHFの主な特徴とメリット
RLHFは、AIの知能を「ネット情報の統計的な模倣」から、 **「人間の意図（Intent）を理解し、社会的なルールに従う」** レベルへと引き上げた、生成AI革命の影の立役者と言えます。

* 「人間らしさ」の獲得:
  単なる事実の羅列ではなく、人間が心地よいと感じるトーンや、文脈を汲み取った回答が可能になります。
* 安全性の向上（ガードレールの構築）:
  不適切な要求に対して「その質問には答えられません」と断るような、倫理的な判断力をAIに持たせることができます。
* 主観的な品質の向上:
  「要約の質」や「プログラミングコードの読みやすさ」など、定量化しにくいスキルの向上に非常に効果的です。

> **補足:** 
> RLHFには、人間の評価コストが高いことや、評価者のバイアスがAIに反映されてしまうといった新たな課題もあり、最近ではAI自身が評価を行う「RLAIF（AIからのフィードバックを用いた強化学習）」などの研究も進んでいます。

## 詳細な仕組み

RLHFにおける強化学習のステップは、単に「AIを褒める」だけではなく、数学的に非常に緻密な設計がなされています。
特に中心となるアルゴリズム **PPO (Proximal Policy Optimization)** がどのように構築され、どのようなロジックで動いているのかを解説します。

>**PPO（Proximal Policy Optimization：近接方策最適化）**
>PPOは、OpenAIが2017年に発表した強化学習アルゴリズムです。現在、ChatGPTなどのLLM（大規模言語モデル）の微調整（RLHF）において、実質的な標準（スタンダード）として採用されています。
>一言でいうと、**「学習が爆発したり、極端に下手になったりしないように、変化の幅を『ほどほど』に抑えながら着実に賢くなる」**賢い学習ルールです。
>__1. PPOの核心メカニズム：クリッピング（Clipping）__  
>PPOの最大の特徴は、 **「前回の自分と、今の自分を比較して、変わりすぎを禁止する」** という数式（目標関数）にあります。
>1. **変化率の計算:** 「新しい自分」がその行動をとる確率が、「古い自分」に比べて何倍になったかを計算します。
>2. **リミッター（クリッピング）:** もし変化率が1.2倍（20%増）を超えたり、0.8倍（20%減）を下回ったりしたら、それ以上の更新は「無効」として切り捨てます。
>3. **理屈:** 「ちょっとずつ改善する分には信じるけど、一気に変えるのは怪しいから認めない」という**慎重なアップデート**を強制します。
>__2. PPOの構造：アクター・クリティック（Actor-Critic）__  
>PPOは通常、2つの役割分担を持つ構造で動きます。
>* **Actor（役者）:** 実際に回答を生成するLLM本体。「次はこう言おう」という「方策」を決めます。
>* **Critic（評論家）:** Actorが生成した回答に対して、「この状況ならこれくらいの点数がもらえるはずだ」と予測するモデルです。
>**なぜCriticが必要か？**
>「予想外に良い点数をもらった！」という驚き（アドバンテージ）を計算するためです。予想通りならあまり学習せず、**「予想を上回る良さ」**があった時だけ、その方向へActorを成長させます。
>__3. PPOの数式__  
>もっとも重要な「クリップされた目的関数」は以下のように書かれます。
>$$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]$$
>* $r_t(\theta)$: 新旧モデルの確率比
>* $\hat{A}_t$: アドバンテージ（予想よりどれだけ良かったか）
>* $\epsilon$: 変化を許容する幅（通常0.1〜0.2）


### 1. RLHFを構成する「3つのモデル」

アルゴリズムの構築を理解するために、まず登場人物（モデル）を整理しましょう。

1. **Policy（方策）モデル**: 訓練対象のLLM（ChatGPTの元となるモデル）。
2. **Reference（参照）モデル**: 学習前のSFTモデル。Policyが「変な方向」に学習しすぎないよう見守る役。
3. **Reward（報酬）モデル**: 人間の好みを学習した「採点用」モデル。

### 2. 報酬モデル (Reward Model) の構築ロジック

「どの回答がより良いか」をAIに教えるために、報酬モデルは以下の数式（損失関数）を用いて構築されます。

人間が回答Aを回答Bより好んだ場合、報酬モデル $r_{\phi}$ の出力が $r_{\phi}(x, A) > r_{\phi}(x, B)$ となるように学習します。具体的には、 **Pairwise Ranking Loss（ペア比較損失）** を使用します。

__Pairwise Ranking Lossの数式__

$$\mathcal{L}(\phi) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log(\sigma(r_{\phi}(x, y_w) - r_{\phi}(x, y_l))) \right]$$

- $y_w$ ($w$in): 人間に選ばれた回答（Chosen）
- $y_l$ ($l$ose): 人間に却下された回答（Rejected）
- $\sigma$: シグモイド関数
- $x$: 入力されたプロンプト

__この数式の「理屈」__

この数式は、「選ばれた回答のスコア」と「選ばれなかった回答のスコア」の差を広げることを目的としています。
1. スコアの差: $r_{\phi}(x, y_w) - r_{\phi}(x, y_l)$ が大きければ大きいほど、シグモイド関数の出力は $1$ に近づきます。
2. 対数とマイナス: $\log(1)$ は $0$ になるため、損失（Loss）は最小化されます。
3. 結果: 報酬モデルは、人間が「良い」とした回答には高い点数を、「悪い」とした回答には低い点数をつける「目」を持つようになります。

これにより、人間が好む回答に高い「スコア（報酬）」を出す採点AIが出来上がります。


### 3. PPOアルゴリズムによる最適化

次に、この報酬モデルを使ってPolicyモデル（LLM）を強化します。ここで使われるのが**PPO**です。PPOを構築する際、開発者が解決したかったのは **強化学習の不安定さ** でした。

#### なぜPPOなのか？

従来の強化学習では、一度の更新でモデルが大きく変わりすぎると、性能が急激に崩壊する問題がありました。PPOは「モデルの変化を一定の範囲内に収める」ことで、安定した学習を実現します。

#### PPOの目的関数（Objective Function）

PPOの学習では、以下の合計値を最大化するように計算します。

1. **報酬の最大化 ($E[R_{\phi}(x, y)]$)**: 報酬モデルから高いスコアをもらえる回答をする。
2. **KLダイバージェンス（ペナルティ）**:  目的関数の右側に配置されるこの項は、学習中のモデル ($\pi_{\theta}$) が、元のモデル ($\pi_{ref}$) から離れすぎないようにする制約です。

> **なぜ制約が必要か（報酬ハッキングの防止）**:
> 制約がないと、AIは報酬モデルの「穴」を見つけ出し、人間から見れば支離滅裂だが報酬モデルだけが高い点数をつけてしまう「意味不明な単語の羅列」を生成するようになります。これを**Reward Hacking**と呼びます。

### 4. アルゴリズム構築のフロー（実装のコツ）

実装上、PPOを適用する際は以下のループを回します。

1. **データ生成**: 学習中のLLMにプロンプトを与え、回答を生成させる。
2. **採点**: 生成した回答を「報酬モデル」に渡し、スコアを得る。
3. **KL計算**: 「参照モデル」と「学習中のモデル」の確率分布を比較し、変化量を計算する。
4. **モデル更新**: 報酬が高く、かつ変化が大きすぎない範囲で、勾配法を用いてモデルの重みを更新する。

#### PPOの特徴：クリッピング (Clipping)

PPOの最もユニークな点は、確率比を  （通常 ）の範囲に強制的に収める「クリッピング」機能です。これにより、極端なパラメータ更新が抑制され、言語モデルのような巨大なモデルでも安定して強化学習を行うことが可能になりました。


### まとめ：RLHFの数学的な美しさ
 
RLHFとPPOの組み合わせは、 **「主観的な人間の好み（Reward Model）」** を **「数学的に安定した確率的な更新（PPO）」** に落とし込んだ点が画期的です。

* **全結合層**: 特徴を混ぜる。
* **活性化関数**: 非線形性を入れる。
* **PPO/RLHF**: 「人間社会のルール」に適応させる。






