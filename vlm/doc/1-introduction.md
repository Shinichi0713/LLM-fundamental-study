# 目的

VLMを学習する前に、現状一般的なモデルや、学習用データ、注意点に関する情報を集める。

## モデル
### 人気のあるモデル

ローカルで動作するVLM（Vision-Language Model、視覚・言語モデル）において、最近人気のあるモデルには、日本語性能や軽量性を重視したものが含まれます。 
主な人気モデルは以下の通りです。
Heron-NVILA-Lite-15B: 日本語VLMの新しい基準として注目されており、150億パラメータでクラス最高水準の性能を持ちながら、iPhone上でのローカル推論も実現しています。
Qwen (特にQwen2-VLシリーズ): Alibaba社が開発したモデルで、その効率性と性能のバランスから世界的に人気があります。2B、7B、72Bといった複数のパラメータサイズがあり、テキスト認識や動画理解能力が高いとされています。
Gemma 3: Googleがリリースしたモデルで、マルチモーダルをサポートしています。特に軽量なGemma 3 270Mは、基本的なタスクをこなすことができ、高い汎用性を持っています。
DeepSeek-VL2 / Phi-4 Multimodal / Pixtral: これらは特定の用途（科学分野、モバイルAI、ドローンなど）に特化しつつ、効率的なMoE（Mixture of Experts）設計や超軽量性を特徴としており、ローカル環境での実行に適しています。
LLaVA-OneVision: オープンソースコミュニティで人気があり、軽量でアクセスしやすいため、視覚的なQ&Aシステムなどの迅速なプロジェクトに適しています。
Sarashina2.2-Vision-3B: 日本語に特化したコンパクトかつ高性能なVLMとして、日本の企業によって公開されています。



## ビジョン系モデルをローカルで扱う際のお勧めと注意点
ローカル系ビジョンモデル環境構築に関するお勧めと注意点です。

・テキスト系に比べてメモリとストレージをかなり消費するので、ColabやSageMaker等との併用がお勧めです。
・detasetのデータはParquetファイル等で圧縮し、HuggingFaceのdatasetsモジュールで読み込み、編集・加工、アップロードし、ローカルには極力保存しない方が良い。
・Llamaのような準標準的なモデルが無く、強いて言うとLlava系モデルが比較的多くなっているが、各モデルとも独自実装色が強いので、inferenceはまだしもFine Tuningになるとモデル毎の個別対応部分がかなりのウェイトを占めるようになる。
・テキスト系ではかなり便利なOllama等のプラットフォームもビジョン系で使えるモデルが現時点少なく、コードを自分で作成する必要が高くなるので、コード生成AIや先駆者の記事等を参考にした方が早い。
・DeepSpeed、flash_attention2、bitsandbytes、mixed_precision等の処理効率化や省メモリのツールが殆どMPS（Apple silicon）非対応で、mlx-lm/vlmも対象ビジョン系モデルが少ないので、多くのリソースを必要とするビジョン系モデルのファインチューニングはMacのMPSには向いて無いように思われます（間違っていたらご指摘下さい）。

## The Cauldron データセットに関して
The Cauldron データセットは、huggingface が idefics2 の公開と同時にリリースした VLM (Vision-Language Model) の fine-tuning 用の V&L (Vision & Language) データセットです。

VLM を fine-tuning する際には、Captioning, VQA, Chart/Figure understanding など様々なタスク指向の V&L データセットが使用されますが、これらの既存のデータセットは多くの場合、ファイル形式やデータ形式が異なっていることや、データセットの保存場所が点在してしまっているといったような問題を抱えています。

そのため、これまでは点在した V&L データセットを手元にダウンロードし、すべてのデータ形式をユーザー側で統一する必要がありました。

しかし、この作業には予想以上の手間がかかるので、VLM の fine-tuning によく使われる V&L データセットが同じデータ形式で１つの保存場所にまとまっていたらいいのになぁ … と思うわけです。
この願いを叶えたデータセットが、The Cauldron データセットです！


