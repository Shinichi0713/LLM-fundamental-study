マルチモーダル学習は、画像、テキスト、音声など、異なる種類のデータ（モダリティ）を統合して処理する手法です。
現在のVLM（視覚言語モデル）の隆盛はこの分野の進化によるものです。

今回はそんなVLMの概要、構成について説明します。

## 概要

### 1. 統合のタイミング（いつ混ぜるか）

データがモデルを通過するどの段階で融合させるかによって、大きく3つに分類されます。

* **初期融合 (Early Fusion):**

入力段階でデータを結合します。例えば、画像のピクセルデータとテキストのベクトルを単純につなげて1つの巨大なベクトルとして入力します。相関を直接学べますが、データの性質が違いすぎると学習が困難です。
* **中間融合 (Intermediate Fusion):**

画像はCNNやViT、テキストはBERTなどの「専用の目や耳（エンコーダ）」で一度処理し、その途中の特徴量同士をTransformerなどで混ぜ合わせる手法です。**現在の主流**（BLIP-2やLLaVAなど）です。
* **後期融合 (Late Fusion):**

それぞれのモダリティで個別に予測を出し、最後にその結果を多数決や平均で統合します。実装は簡単ですが、データの深い関係性を捉えるのは苦手です。


### 2. 統合のアプローチ（どう混ぜるか）

具体的にどうやって異なる空間のデータを結びつけるか、代表的な手法です。

* **対照学習 (Contrastive Learning):**
画像とそれに対応するテキストのペアを「近く」、関係ないペアを「遠く」配置するように学習します。代表例は**CLIP**です。
* **生成ベース (Generative Approach):**
画像を入力として、それに対応するテキストをLLMに生成させます。Q-FormerなどのConnector（橋渡し役）を用いて、画像の情報をLLMが理解できる形式に変換します。
* **クロス・アテンション (Cross-Attention):**
Transformerの仕組みを使い、テキストが画像内の「どこ」に注目すべきかを計算させます。


### 3. 学習のステップ（どう育てるか）


VLMの学習（ドッキング）は、多くの場合 **「段階的学習（Multi-stage Training）」** という手法をとります。

最初からすべてを繋げて学習させるのではなく、まずは「目（Encoder）」と「脳（LLM）」の言葉を合わせることから始めます。一般的に以下の2つのステージで行われます。


__ステージ 1：視覚と言語のアライメント（事前学習）__

この段階の目的は、 **「Connector（通訳）」に画像と単語の対応関係を叩き込む** ことです。

* **何をするか:** 膨大な「画像と短い説明文（キャプション）」のペアを学習させます。
* **学習のルール:**
* **Vision Encoder:** フリーズ（重みを固定）。
* **LLM:** フリーズ（重みを固定）。
* **Connector:** **ここだけを学習（更新）させます。**


* **効果:** LLMが画像データを見たときに、「これは『猫』という概念だな」と理解できるための「翻訳能力」がConnectorに備わります。

__ステージ 2：インストラクション・チューニング（微調整）__

基本の翻訳ができるようになったら、次は「人間の複雑な指示（命令）」に従えるように訓練します。

* **何をするか:** 「この写真のどこが面白いの？」「このレシートの合計金額を教えて」といった、対話形式のデータセットを使います。
* **学習のルール:**
* **Vision Encoder:** 基本はフリーズ（高精度化のために一部解禁することもある）。
* **Connector:** 学習を継続。
* **LLM:** **ここでLLMの重みも一部、あるいは全部解禁して学習させます（LoRAなどの軽量化手法がよく使われます）。**


* **効果:** 単なる単語の羅列ではなく、文脈を読み取った高度な回答ができるようになります。

__ステップの理由__

もし、最初からすべてを全開で学習させようとすると、以下のような問題が起きてしまいます。

1. **壊滅的忘却:** LLMが元々持っていた高度な言語能力（推論や知識）が、新しい画像データの入力によって壊れてしまう。
2. **計算コストの爆発:** 数十億パラメータを一度に動かすには、膨大なGPUリソースが必要になる。
3. **役割の混乱:** Connectorが翻訳を覚える前にLLMが無理やり合わせようとして、学習が収束しなくなる。


### 4. なぜ今これが熱いのか？

かつては「犬」という画像から「Dog」というラベルを当てるだけでしたが、今のマルチモーダルは、LLM（大規模言語モデル）という「強力な脳」に、ビジョンエンコーダという「高性能な目」を接続できるようになったからです。

これにより、ただの認識を超えて **「画像を見て、その背後にあるストーリーを推論する」** ことが可能になりました。


## 一般的な構成

VLM（Vision-Language Model：視覚言語モデル）の基本的な構成は、人間で例えると **「目」「神経（通訳）」「脳」** という3つの主要なコンポーネントを繋ぎ合わせたものと考えると非常に理解しやすくなります。

現代の主要なVLM（LLaVAやBLIP-2など）の多くはこの構成をとっています。


### 1. Vision Encoder（目）

画像を受け取り、コンピュータが理解できる数値（特徴量ベクトル）に変換する役割です。

* **主なモデル:** **CLIP-ViT** や **SigLIP** など。
* **役割:** 画像をパッチ（小さな断片）に分割し、それぞれのパッチに何が写っているか、それらがどう関連しているかを抽出します。
* **特徴:** 多くのVLMでは、膨大な画像データで既に学習済みの「既存の強力なモデル」をそのまま、あるいは少しだけ調整して使用します。


### 2. Connector / Projector（神経・通訳）

「目（ビジョンエンコーダ）」が出力するデータ形式を、「脳（LLM）」が理解できる言語形式に翻訳する橋渡し役です。ここが**VLMのアーキテクチャ設計で最も工夫される部分**です。

* **主な手法:**
* **MLP（多層パーセプトロン）:** 単純な変換層。画像の情報をそのまま言語空間へ投影します（LLaVAなどで採用）。
* **Q-Former (Querying Transformer):** 少数の「クエリ」を使って画像から重要なエッセンスだけを抽出・圧縮します（BLIP-2などで採用）。


* **役割:** 画像の「特徴ベクトル」を、LLMにとっての「単語（トークン）と同じようなもの（ソフト・プロンプト）」に変換します。


### 3. Large Language Model (LLM)（脳）

変換された画像情報を受け取り、ユーザーの質問（テキスト）と組み合わせて思考・回答を行う部分です。

* **主なモデル:** **Llama 3**, **Vicuna**, **Qwen** など。
* **役割:** 「画像から翻訳された情報」をあたかも「文章の続き」のように扱い、知識に基づいた推論や自然な文章生成を行います。
* **特徴:** 基本的にはテキストのみで訓練された強力なLLMを流用します。


### 全体の動作フロー
データが入力されてからVLMの中で行われる処理のざっくりとした流れです。

1. **入力:** ユーザーが「画像」と「この写真の面白い点は？」という「テキスト」を入力します。
2. **視覚処理:** **Vision Encoder**が画像を解析し、特徴データを出力します。
3. **変換:** **Connector**が、そのデータをLLMが読める「視覚トークン」に変換します。
4. **推論:** **LLM**が「視覚トークン」＋「ユーザーの質問」を読み込み、答えを生成します。


### なぜこの構成なのか？

この「モジュール型」の構成には大きなメリットがあります。それは **「既存の最強モデルを合体させられる」** という点です。

* すでに世界一賢いLLM（脳）がある。
* すでに世界一目が良いビジョンモデル（目）がある。
* それらを**Connector（神経）で繋ぐだけで、ゼロから全てを学習させるより遥かに効率的にVLMが作れる**のです。

## 所感
重要なことは以下です。

- VLMは言語を理解するLLM、視覚を理解するVision Encoder、接続するConnectorで構成されます。
- モデルの学習は既にあるモデルをそのままにして、接続部をチューニング→指示学習で賢くします。

つくるのは既にあるものを持ってきて上手に接続すれば、あとは調整して使えるということになります。
次回以降はチューニングを扱っていきます。

# VLMの手法

VLM（視覚言語モデル）の「目（Vision Encoder）」と「脳（LLM）」を繋ぐ **Connector（接続手法）** には、Q-Former以外にも多様なアプローチがあります。これらを「情報の処理方法」という観点から体系立てて整理すると、大きく3つのグループに分類できます。


## VLM接続手法の体系化

### 1. マッピング型（Mapping）

「画像情報をそのまま、あるいは最小限の変換でLLMに渡す」という最もシンプルな手法です。

* **代表的な手法:** **MLP (Multi-Layer Perceptron)**
* **採用モデル:** **LLaVA (v1.5以降)**, **BakLLaVA**
* **特徴:**  Vision Encoderが出力した画像パッチ（断片）を、単語ベクトルと同じ次元に変換してそのままLLMに入力します。
* **長所:** 情報の欠落が少なく、文字認識（OCR）や細かい物体の検出に強い。
* **短所:** 画像のトークン数（パッチ数）がそのままLLMに渡されるため、高解像度画像や動画を扱うとLLMの処理が非常に重くなる。



### 2. 圧縮・集約型（Compression / Resampling）

「膨大な画像パッチから、重要なエッセンスだけを絞り込んでLLMに渡す」という効率重視の手法です。Q-Formerもここに含まれます。

* **代表的な手法:**
* **Perceiver Resampler (Flamingo等):** 学習可能な少数の「クエリ」を使い、画像全体から情報を吸い上げます。
* **C-Abstractor (Honeybee等):** 畳み込み層（Convolution）を使い、周辺のピクセル情報をまとめながら圧縮します。


* **特徴:**
* 画像がどれだけ大きくても、LLMに渡すトークン数を一定（例：64個や144個）に固定できます。
* **長所:** LLMの計算コストを劇的に抑えられる。動画（多フレーム）の処理に非常に向いている。
* **短所:** 圧縮過程で「右上に何があったか」といった細かい空間情報が失われやすい。


### 3. ハイブリッド・高度融合型（Advanced Holistic Designs）

マッピングと圧縮の良いとこ取りをしたり、LLMの内部構造自体に手を加えたりする進化形です。

* **代表的な手法:**
* **BLIVA:** Q-Formerによる「凝縮された特徴」と、MLPによる「生の特徴」の両方をLLMに同時に入力します。
* **Gated Cross-Attention (IDEFICS等):** LLMの各レイヤーの間に画像を参照する専用の窓（Attention層）を設けます。


* **特徴:**
* **長所:** 物体の全体像（圧縮情報）と、細部のディテール（生の情報）の両方を扱える。
* **短所:** アーキテクチャが複雑になり、学習や推論のスピードが落ちることがある。




## 各手法の比較と使い分け

どの手法を採用するかは、そのVLMの「目的」によって決まります。

| 手法グループ | 重視する点 | 適した用途 | 主なモデル |
| --- | --- | --- | --- |
| **マッピング型 (MLP)** | **精細さ** | 文書読み取り、詳細な対話 | LLaVA, DeepSeek-VL |
| **圧縮型 (Q-Former系)** | **効率・文脈** | 動画解析、大量画像検索 | BLIP-2, Flamingo, Video-LLaMA |
| **ハイブリッド型** | **万能性** | 高精度なOCR + 複雑な推論 | BLIVA, InternVL |


### 現代のトレンド：動的な圧縮

最近（2025年〜）のモデルでは、画像の複雑さに応じてトークン数を変える「動的圧縮（Dynamic Compression）」も登場しています。
白い壁のような情報が少ない場所は1トークン、複雑な街並みは多くのトークンを使う、といった **「情報の密度に合わせた適応」** が進化の最前線です。

次は、これらの中で特に注目されている **「動画用VLM（Video-VLM）での情報のまとめ方」** や、 **「LLaVAがなぜあえてシンプルなMLPを選んだのか」** という背景について詳しく解説しましょうか？
