マルチモーダル学習は、画像、テキスト、音声など、異なる種類のデータ（モダリティ）を統合して処理する手法です。
現在のVLM（視覚言語モデル）の隆盛はこの分野の進化によるものです。

今回はそんなVLMの概要、構成について説明します。

## 概要

### 1. 統合のタイミング（いつ混ぜるか）

データがモデルを通過するどの段階で融合させるかによって、大きく3つに分類されます。

* **初期融合 (Early Fusion):**

入力段階でデータを結合します。例えば、画像のピクセルデータとテキストのベクトルを単純につなげて1つの巨大なベクトルとして入力します。相関を直接学べますが、データの性質が違いすぎると学習が困難です。
* **中間融合 (Intermediate Fusion):**

画像はCNNやViT、テキストはBERTなどの「専用の目や耳（エンコーダ）」で一度処理し、その途中の特徴量同士をTransformerなどで混ぜ合わせる手法です。**現在の主流**（BLIP-2やLLaVAなど）です。
* **後期融合 (Late Fusion):**

それぞれのモダリティで個別に予測を出し、最後にその結果を多数決や平均で統合します。実装は簡単ですが、データの深い関係性を捉えるのは苦手です。


### 2. 統合のアプローチ（どう混ぜるか）

具体的にどうやって異なる空間のデータを結びつけるか、代表的な手法です。

* **対照学習 (Contrastive Learning):**
画像とそれに対応するテキストのペアを「近く」、関係ないペアを「遠く」配置するように学習します。代表例は**CLIP**です。
* **生成ベース (Generative Approach):**
画像を入力として、それに対応するテキストをLLMに生成させます。Q-FormerなどのConnector（橋渡し役）を用いて、画像の情報をLLMが理解できる形式に変換します。
* **クロス・アテンション (Cross-Attention):**
Transformerの仕組みを使い、テキストが画像内の「どこ」に注目すべきかを計算させます。


### 3. 学習のステップ（どう育てるか）


VLMの学習（ドッキング）は、多くの場合 **「段階的学習（Multi-stage Training）」** という手法をとります。

最初からすべてを繋げて学習させるのではなく、まずは「目（Encoder）」と「脳（LLM）」の言葉を合わせることから始めます。一般的に以下の2つのステージで行われます。


__ステージ 1：視覚と言語のアライメント（事前学習）__

この段階の目的は、 **「Connector（通訳）」に画像と単語の対応関係を叩き込む** ことです。

* **何をするか:** 膨大な「画像と短い説明文（キャプション）」のペアを学習させます。
* **学習のルール:**
* **Vision Encoder:** フリーズ（重みを固定）。
* **LLM:** フリーズ（重みを固定）。
* **Connector:** **ここだけを学習（更新）させます。**


* **効果:** LLMが画像データを見たときに、「これは『猫』という概念だな」と理解できるための「翻訳能力」がConnectorに備わります。

__ステージ 2：インストラクション・チューニング（微調整）__

基本の翻訳ができるようになったら、次は「人間の複雑な指示（命令）」に従えるように訓練します。

* **何をするか:** 「この写真のどこが面白いの？」「このレシートの合計金額を教えて」といった、対話形式のデータセットを使います。
* **学習のルール:**
* **Vision Encoder:** 基本はフリーズ（高精度化のために一部解禁することもある）。
* **Connector:** 学習を継続。
* **LLM:** **ここでLLMの重みも一部、あるいは全部解禁して学習させます（LoRAなどの軽量化手法がよく使われます）。**


* **効果:** 単なる単語の羅列ではなく、文脈を読み取った高度な回答ができるようになります。

__ステップの理由__

もし、最初からすべてを全開で学習させようとすると、以下のような問題が起きてしまいます。

1. **壊滅的忘却:** LLMが元々持っていた高度な言語能力（推論や知識）が、新しい画像データの入力によって壊れてしまう。
2. **計算コストの爆発:** 数十億パラメータを一度に動かすには、膨大なGPUリソースが必要になる。
3. **役割の混乱:** Connectorが翻訳を覚える前にLLMが無理やり合わせようとして、学習が収束しなくなる。


### 4. なぜ今これが熱いのか？

かつては「犬」という画像から「Dog」というラベルを当てるだけでしたが、今のマルチモーダルは、LLM（大規模言語モデル）という「強力な脳」に、ビジョンエンコーダという「高性能な目」を接続できるようになったからです。

これにより、ただの認識を超えて **「画像を見て、その背後にあるストーリーを推論する」** ことが可能になりました。


## 一般的な構成

VLM（Vision-Language Model：視覚言語モデル）の基本的な構成は、人間で例えると **「目」「神経（通訳）」「脳」** という3つの主要なコンポーネントを繋ぎ合わせたものと考えると非常に理解しやすくなります。

現代の主要なVLM（LLaVAやBLIP-2など）の多くはこの構成をとっています。


### 1. Vision Encoder（目）

画像を受け取り、コンピュータが理解できる数値（特徴量ベクトル）に変換する役割です。

* **主なモデル:** **CLIP-ViT** や **SigLIP** など。
* **役割:** 画像をパッチ（小さな断片）に分割し、それぞれのパッチに何が写っているか、それらがどう関連しているかを抽出します。
* **特徴:** 多くのVLMでは、膨大な画像データで既に学習済みの「既存の強力なモデル」をそのまま、あるいは少しだけ調整して使用します。


### 2. Connector / Projector（神経・通訳）

「目（ビジョンエンコーダ）」が出力するデータ形式を、「脳（LLM）」が理解できる言語形式に翻訳する橋渡し役です。ここが**VLMのアーキテクチャ設計で最も工夫される部分**です。

* **主な手法:**
* **MLP（多層パーセプトロン）:** 単純な変換層。画像の情報をそのまま言語空間へ投影します（LLaVAなどで採用）。
* **Q-Former (Querying Transformer):** 少数の「クエリ」を使って画像から重要なエッセンスだけを抽出・圧縮します（BLIP-2などで採用）。


* **役割:** 画像の「特徴ベクトル」を、LLMにとっての「単語（トークン）と同じようなもの（ソフト・プロンプト）」に変換します。


### 3. Large Language Model (LLM)（脳）

変換された画像情報を受け取り、ユーザーの質問（テキスト）と組み合わせて思考・回答を行う部分です。

* **主なモデル:** **Llama 3**, **Vicuna**, **Qwen** など。
* **役割:** 「画像から翻訳された情報」をあたかも「文章の続き」のように扱い、知識に基づいた推論や自然な文章生成を行います。
* **特徴:** 基本的にはテキストのみで訓練された強力なLLMを流用します。


### 全体の動作フロー
データが入力されてからVLMの中で行われる処理のざっくりとした流れです。

1. **入力:** ユーザーが「画像」と「この写真の面白い点は？」という「テキスト」を入力します。
2. **視覚処理:** **Vision Encoder**が画像を解析し、特徴データを出力します。
3. **変換:** **Connector**が、そのデータをLLMが読める「視覚トークン」に変換します。
4. **推論:** **LLM**が「視覚トークン」＋「ユーザーの質問」を読み込み、答えを生成します。


### なぜこの構成なのか？

この「モジュール型」の構成には大きなメリットがあります。それは **「既存の最強モデルを合体させられる」** という点です。

* すでに世界一賢いLLM（脳）がある。
* すでに世界一目が良いビジョンモデル（目）がある。
* それらを**Connector（神経）で繋ぐだけで、ゼロから全てを学習させるより遥かに効率的にVLMが作れる**のです。

## 所感
重要なことは以下です。

- VLMは言語を理解するLLM、視覚を理解するVision Encoder、接続するConnectorで構成されます。
- モデルの学習は既にあるモデルをそのままにして、接続部をチューニング→指示学習で賢くします。

つくるのは既にあるものを持ってきて上手に接続すれば、あとは調整して使えるということになります。
次回以降はチューニングを扱っていきます。

