ä»¥ä¸‹ã§ã¯ã€**å…ˆã»ã©æç¤ºã—ãŸ CLIP å®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ LLaVA å½¢å¼ã¸æ”¹é€ ã™ã‚‹æœ€å°æ§‹æˆã‚³ãƒ¼ãƒ‰**ã‚’ç¤ºã—ã¾ã™ã€‚
ç›®çš„ã¯ã€Œ **CLIP â†’ LLaVA ã¸ã®æ§‹é€ å¤‰æ›ãŒè…¹è½ã¡ã™ã‚‹ã“ã¨** ã€ã§ã‚ã‚Šã€ç ”ç©¶ãƒ»å®Ÿè£…ã®å‡ºç™ºç‚¹ã¨ã—ã¦ãã®ã¾ã¾ä½¿ãˆã‚‹å½¢ã«ã—ã¦ã„ã¾ã™ã€‚

---

# 0. CLIP â†’ LLaVA ã§ä½•ãŒå¤‰ã‚ã‚‹ã®ã‹ï¼ˆæ•´ç†ï¼‰

| é …ç›®           | CLIP             | LLaVA                            |
| -------------- | ---------------- | -------------------------------- |
| Vision Encoder | ViT              | ViTï¼ˆåŒã˜ï¼‰                      |
| Text Encoder   | BERT             | **LLMï¼ˆLLaMAç³»ï¼‰**         |
| å­¦ç¿’æå¤±       | Contrastive Loss | **Language Modeling Loss** |
| å‡ºåŠ›           | åŸ‹ã‚è¾¼ã¿         | **æ–‡ç« ç”Ÿæˆ**               |
| èåˆ           | å…±é€šç©ºé–“         | **Image token injection**  |

**é‡è¦**

* Vision Encoder ã¯ã»ã¼æµç”¨
* Text Encoder ã‚’ã€Œæ¶ˆã™ã€
* Projection ã‚’ã€ŒLLMå…¥åŠ›ç”¨ã€ã«å¤‰æ›´

> VisionTransformerã¯å¤‰åŒ–ãªã—
>
> Text Encoderã¯BERTâ†’LLM
>
> å­¦ç¿’æå¤±ï¼šå¯¾ç…§å­¦ç¿’â†’LML
>
> åŸ‹ã‚è¾¼ã¿â†’æ–‡ç« 

---

# 1. å…¨ä½“æ§‹æˆï¼ˆLLaVAæœ€å°ï¼‰

```
Image â†’ CLIP ViT â†’ patch features
                    â†“
             Projection (Linear)
                    â†“
           Image Tokens (æ“¬ä¼¼ãƒˆãƒ¼ã‚¯ãƒ³)
                    â†“
Text Tokens + Image Tokens
                    â†“
LLM (LLaMA / Vicuna)
                    â†“
Text Output
```

---

# 2. å‰æãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```bash
pip install torch torchvision transformers accelerate
```

---

# 3. Vision Encoderï¼ˆCLIPã‹ã‚‰æµç”¨ï¼‰

**â€» CLS ã§ã¯ãªã patch feature ã‚’ä½¿ã†ç‚¹ãŒé‡è¦**

```python
import torch
import torch.nn as nn
from torchvision.models import vit_b_16
```

```python
class VisionEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.vit = vit_b_16(pretrained=True)
        self.vit.heads = nn.Identity()

    def forward(self, images):
        # ViT forward hack: get patch embeddings
        x = self.vit._process_input(images)
        n = x.shape[0]

        cls_token = self.vit.class_token.expand(n, -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        x = self.vit.encoder(x)

        # remove CLS â†’ use patch tokens only
        return x[:, 1:, :]   # (B, N_patches, 768)
```

---

# 4. Projection Layerï¼ˆCLIP â†’ LLaVAã®æ ¸å¿ƒï¼‰

```python
class VisionProjection(nn.Module):
    def __init__(self, vision_dim=768, llm_dim=4096):
        super().__init__()
        self.proj = nn.Linear(vision_dim, llm_dim)

    def forward(self, vision_feats):
        # (B, N, vision_dim) â†’ (B, N, llm_dim)
        return self.proj(vision_feats)
```

---

# 5. LLaVA æœ¬ä½“ï¼ˆLLM + Image Token Injectionï¼‰

ã“ã“ã§ã¯ **LLaMA ç³»ãƒ¢ãƒ‡ãƒ«**ã‚’æƒ³å®šã—ã¾ã™ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
```

```python
class LLaVA(nn.Module):
    def __init__(self, llm_name="meta-llama/Llama-2-7b-hf"):
        super().__init__()

        self.vision_encoder = VisionEncoder()
        self.vision_proj = VisionProjection()

        self.llm = AutoModelForCausalLM.from_pretrained(
            llm_name,
            torch_dtype=torch.float16
        )
        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def forward(self, images, input_ids, attention_mask, labels=None):
        # 1. image â†’ vision tokens
        vision_feats = self.vision_encoder(images)
        image_tokens = self.vision_proj(vision_feats)

        # 2. text embeddings
        text_embeds = self.llm.model.embed_tokens(input_ids)

        # 3. concatenate (image tokens first)
        inputs_embeds = torch.cat(
            [image_tokens, text_embeds], dim=1
        )

        # 4. adjust attention mask
        image_mask = torch.ones(
            image_tokens.size()[:-1],
            device=attention_mask.device
        )
        attention_mask = torch.cat(
            [image_mask, attention_mask], dim=1
        )

        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs
```

---

# 6. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼ˆLLaVAå‹ï¼‰

```python
prompt = "ã“ã®ç”»åƒã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
answer = "çŠ¬ãŒèŠç”Ÿã®ä¸Šã§èµ°ã£ã¦ã„ã¾ã™ã€‚"
```

```python
enc = tokenizer(
    prompt,
    return_tensors="pt",
    padding=True
)

labels = tokenizer(
    answer,
    return_tensors="pt",
    padding=True
)["input_ids"]
```

---

# 7. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆAlignment / Instruction Tuningï¼‰

```python
model = LLaVA().cuda()
optimizer = torch.optim.AdamW(
    model.vision_proj.parameters(), lr=1e-4
)
```

```python
outputs = model(
    images.cuda(),
    enc["input_ids"].cuda(),
    enc["attention_mask"].cuda(),
    labels=labels.cuda()
)

loss = outputs.loss
loss.backward()
optimizer.step()
optimizer.zero_grad()
```

---

# 8. CLIP â†’ LLaVA ã§å¤±ã‚ã‚ŒãŸã‚‚ã®ï¼å¾—ãŸã‚‚ã®

### å¤±ã‚ã‚ŒãŸã‚‚ã®

* å¯¾ç…§å­¦ç¿’ã«ã‚ˆã‚‹æ˜ç¤ºçš„ãªè·é›¢åˆ¶ç´„
* å¤§è¦æ¨¡ãƒãƒƒãƒå‰æã®å­¦ç¿’

### å¾—ãŸã‚‚ã®

* æ–‡ç« ç”Ÿæˆèƒ½åŠ›
* Instruction-following
* æ¨è«–æŸ”è»Ÿæ€§

---

# 9. å®Ÿå‹™ã§å¿…ãšå…¥ã‚Œã‚‹æ”¹è‰¯ç‚¹

æœ€ä½é™ã€æ¬¡ã¯å¿…é ˆã§ã™ï¼š

* Vision Encoder freeze
* LLM ã¯ **LoRA / QLoRA**
* Image token æ•°å‰Šæ¸›ï¼ˆPooling / Samplingï¼‰
* FP16 + gradient checkpointing

---

# 10. ç†è§£ã®è¦ç‚¹ï¼ˆè¶…é‡è¦ï¼‰

* **CLIP = ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®åŸºç¤**
* **LLaVA = ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ + ç”Ÿæˆ**
* Projection ãŒã€Œå”¯ä¸€ã®æ©‹ã€
* LLMã¯ã€Œç”»åƒã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ã—ã¦èª­ã‚“ã§ã„ã‚‹ã ã‘ã€

---

## æ¬¡ã«é€²ã‚€ã¨è‰¯ã„ãƒ†ãƒ¼ãƒ

* Projection ã‚’ MLP / Perceiver ã«å¤‰ãˆã‚‹
* Flamingoï¼ˆCross-Attentionï¼‰ã¨ã®å·®åˆ†å®Ÿè£…
* Image token ã« attention ãŒå‘ã„ã¦ã„ã‚‹ã‹ã®å¯è¦–åŒ–
* LLaVA-1.5 / NeXT ã®æ§‹é€ å·®åˆ†

ã©ã“ã¾ã§æ˜ã‚Šä¸‹ã’ã¾ã™ã‹ã€‚



ä»¥ä¸‹ã§ã¯ã€**Google Colab ä¸Šã§å®Ÿéš›ã«å‹•ä½œç¢ºèªã§ãã‚‹ã€Œæœ€å°æ§‹æˆ LLaVAã€ã‚³ãƒ¼ãƒ‰**ã‚’ã€
**ãã®ã¾ã¾ã‚»ãƒ«åˆ†å‰²ã—ã¦å®Ÿè¡Œã§ãã‚‹å½¢**ã§æç¤ºã—ã¾ã™ã€‚

æ–¹é‡ã¯ä»¥ä¸‹ã§ã™ã€‚

* Colabï¼ˆç„¡æ–™ or T4ï¼‰ã§å‹•ãã“ã¨ã‚’æœ€å„ªå…ˆ
* LLaVAã® **æ§‹é€ ç†è§£ç”¨ãƒ»æ¤œè¨¼ç”¨**
* å­¦ç¿’ã§ã¯ãªã **æ¨è«–ï¼ˆInferenceï¼‰** ã‚’ã¾ãšæˆç«‹ã•ã›ã‚‹
* é‡é‡ç´š LLaMA-7B ã¯ä½¿ã‚ãšã€**å°å‹ LLMï¼ˆOPT / LLaMA-2-7Bã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰**

---

# å…¨ä½“æ§‹æˆï¼ˆColabå‘ã‘ï¼‰

```
Image â†’ CLIP ViT â†’ Projection â†’ Image Tokens
                                     â†“
                           Text Tokens + Image Tokens
                                     â†“
                            Causal LM â†’ Text
```

---

# ğŸ”¹ Colab ã‚»ãƒ«1ï¼šç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```python
!pip install -q torch torchvision transformers accelerate pillow
```

---

# ğŸ”¹ Colab ã‚»ãƒ«2ï¼šãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿

```python
import torch
import torch.nn as nn
from torchvision.models import vit_b_16
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import torchvision.transforms as T
```

---

# ğŸ”¹ Colab ã‚»ãƒ«3ï¼šç”»åƒå‰å‡¦ç†

```python
image_transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
```

---

# ğŸ”¹ Colab ã‚»ãƒ«4ï¼šVision Encoderï¼ˆViTï¼‰

```python
class VisionEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.vit = vit_b_16(weights="IMAGENET1K_V1")
        self.vit.heads = nn.Identity()

    def forward(self, images):
        x = self.vit._process_input(images)
        n = x.shape[0]

        cls_token = self.vit.class_token.expand(n, -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        x = self.vit.encoder(x)

        return x[:, 1:, :]  # (B, N_patches, 768)
```

---

# ğŸ”¹ Colab ã‚»ãƒ«5ï¼šProjection Layer

```python
class VisionProjection(nn.Module):
    def __init__(self, vision_dim=768, llm_dim=768):
        super().__init__()
        self.proj = nn.Linear(vision_dim, llm_dim)

    def forward(self, x):
        return self.proj(x)
```

â€» **ã“ã“ã§ã¯ LLM ã¨åŒæ¬¡å…ƒã«åˆã‚ã›ã¦ã„ã¾ã™**

---

# ğŸ”¹ Colab ã‚»ãƒ«6ï¼šLLaVA æœ€å°ãƒ¢ãƒ‡ãƒ«

Colabã§ç¢ºå®Ÿã«å‹•ã‹ã™ãŸã‚ã€
**LLM ã¯ `facebook/opt-350m` ã‚’ä½¿ç”¨**ã—ã¾ã™ã€‚

```python
class MiniLLaVA(nn.Module):
    def __init__(self, llm_name="facebook/opt-350m"):
        super().__init__()

        self.vision_encoder = VisionEncoder()
        self.vision_proj = VisionProjection()

        self.llm = AutoModelForCausalLM.from_pretrained(
            llm_name,
            torch_dtype=torch.float16
        )
        self.tokenizer = AutoTokenizer.from_pretrained(llm_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def forward(self, images, input_ids, attention_mask):
        vision_feats = self.vision_encoder(images)
        image_tokens = self.vision_proj(vision_feats)

        text_embeds = self.llm.model.decoder.embed_tokens(input_ids)

        inputs_embeds = torch.cat(
            [image_tokens, text_embeds], dim=1
        )

        image_mask = torch.ones(
            image_tokens.size()[:-1],
            device=attention_mask.device
        )
        attention_mask = torch.cat(
            [image_mask, attention_mask], dim=1
        )

        return self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask
        )
```

---

# ğŸ”¹ Colab ã‚»ãƒ«7ï¼šãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–

```python
device = "cuda" if torch.cuda.is_available() else "cpu"

model = MiniLLaVA().to(device)
model.eval()
```

---

# ğŸ”¹ Colab ã‚»ãƒ«8ï¼šç”»åƒèª­ã¿è¾¼ã¿

```python
img = Image.open("/content/sample_data/cat.jpg").convert("RGB")
image_tensor = image_transform(img).unsqueeze(0).to(device)
```

â€» Colab ã® `sample_data` ã«ã‚ã‚‹ç”»åƒã§OK

---

# ğŸ”¹ Colab ã‚»ãƒ«9ï¼šãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›

```python
prompt = "Describe this image."

enc = model.tokenizer(
    prompt,
    return_tensors="pt"
).to(device)
```

---

# ğŸ”¹ Colab ã‚»ãƒ«10ï¼šç”Ÿæˆï¼ˆæ¨è«–ï¼‰

```python
with torch.no_grad():
    outputs = model(
        image_tensor,
        enc["input_ids"],
        enc["attention_mask"]
    )

    generated = model.llm.generate(
        inputs_embeds=outputs.inputs_embeds,
        attention_mask=outputs.attention_mask,
        max_new_tokens=50
    )

print(model.tokenizer.decode(generated[0], skip_special_tokens=True))
```

---

# âœ… ã“ã“ã¾ã§ã§å‡ºæ¥ã‚‹ã“ã¨

* ç”»åƒã‚’ **ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ã—ã¦ LLM ã«æ³¨å…¥**
* Self-Attention ã«ã‚ˆã‚‹ç”»åƒï¼‹ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
* Colab ä¸Šã§ **LLaVAæ§‹é€ ã‚’å®Ÿä½“é¨“**

â€» å‡ºåŠ›å“è³ªã¯ **æœªå­¦ç¿’ Projection** ã®ãŸã‚ä½ã„ã§ã™
â†’ **æ§‹é€ æ¤œè¨¼ãŒç›®çš„**

---

# ğŸ”§ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦

### CUDA OOM

* `opt-350m` ã‚ˆã‚Šå¤§ããã—ãªã„
* batch size = 1

### dtype ã‚¨ãƒ©ãƒ¼

```python
torch_dtype=torch.float16
```

ã‚’å¿…ãšæŒ‡å®š

---

# æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨ï¼ˆå¼·ãæ¨å¥¨ï¼‰

1. **Vision Encoder freeze**
2. Projection ã®ã¿å­¦ç¿’ï¼ˆCOCO Captionï¼‰
3. LoRA ã‚’ LLM ã«é©ç”¨
4. Image token æ•°å‰Šæ¸›
5. LLaVA Instruction ãƒ‡ãƒ¼ã‚¿æŠ•å…¥

---

## ã¾ã¨ã‚

* ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ **LLaVAã®æœ€å°ãƒ»å¯å‹•ãƒ»ç†è§£ç”¨**
* CLIP â†’ LLaVA å¤‰æ›ã®æ§‹é€ ãŒæ˜ç¢ºã«è¦‹ãˆã‚‹
* Colab ã§ã€Œã¾ãšå‹•ãã€ã“ã¨ãŒæœ€å„ªå…ˆ

---

æ¬¡ã®æ®µéšã¨ã—ã¦ã€

* **Projection ã‚’å®Ÿéš›ã«å­¦ç¿’ã™ã‚‹ Colab ã‚³ãƒ¼ãƒ‰**
* **LoRA ä»˜ã LLaVA**
* **LLaVA-1.5 ç›¸å½“æ§‹æˆã¸ã®æ‹¡å¼µ**

ã®ã©ã‚Œã«é€²ã¿ã¾ã™ã‹ã€‚

