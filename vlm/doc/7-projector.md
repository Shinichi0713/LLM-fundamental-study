マルチモーダルモデルをビジョンエンコーダとLLMを結合するモジュールをprojector / connectorと呼びます。

## Q-Former

マルチモーダル学習、特に画像と言語を繋ぐ技術において、**Q-Former**は非常に重要な役割を果たすコンポーネントです。

簡単に言うと、Q-Formerは **「膨大な画像情報の中から、言語モデル（LLM）が理解しやすい重要なエッセンスだけを抽出する通訳者」** のような存在です。Salesforceが開発した「BLIP-2」というモデルで初めて導入されました。


### Q-Formerの役割と仕組み

画像データは非常に情報量が多いですが、そのすべてがテキストでの理解に必要なわけではありません。Q-Formerは以下のステップで情報を整理します。

1. **情報の圧縮（ボトルネック）**:
画像エンコーダ（CLIPなど）が出力する膨大な特徴量を、数個〜数十個の「Learnable Queries（学習可能なクエリ）」に集約します。これにより、LLMに渡すデータ量を劇的に減らしつつ、重要な意味情報を保持します。
2. **2つのトランスフォーマー構造**:
Q-Formerの内部は、画像を見るための「Self-Attention」と、テキストとの関連性を探る「Cross-Attention」が組み合わさった構造をしています。
3. **共通空間へのマッピング**:
画像と言語という異なる形式のデータを、同じ意味空間で扱えるように橋渡しをします。

### なぜQ-Formerが画期的なのか？

Q-Formerが登場する前は、画像エンコーダとLLMを直接繋ぐ方法が主流でしたが、それにはいくつかの課題がありました。Q-Formerはそれを解決しています。

* **LLMの負担を軽減**:
LLMに画像をそのまま流し込むのではなく、Q-Formerが「固定長（例えば32トークンなど）」に情報を凝縮するため、計算コストが抑えられます。
* **既存モデルの活用（Frozen Weights）**:
すでに賢い「画像モデル」と「言語モデル」を**重みを固定したまま**使い、その間を繋ぐQ-Formerだけを訓練すればよいため、非常に効率的に学習が可能です。
* **精度の向上**:
画像の中の「何が重要か」を言語の文脈に合わせて抽出するため、画像キャプショニングやVQA（画像応答）の精度が飛躍的に向上しました。


### 代表的な活用例：BLIP-2

Q-Formerの有効性を証明したのが**BLIP-2**です。このモデルは、Q-Formerを介して画像モデルとLLM（OPTやFlan-T5など）を結合することで、当時、はるかに巨大なパラメータを持つモデルよりも高い性能を発揮しました。

### 構造

Q-Formerの具体的な実装は、標準的な **Transformer（特にBERTベース）** をベースにしつつ、画像と言語を融合させるために「クエリ」と「アテンション」の仕組みを独自にカスタマイズしたものになっています。

![1767395503697](image/7-projector/1767395503697.png)

ざっと概要を説明すると以下のようになります。

- 画像エンコーダと相互作用して視覚特徴を抽出する画像変換モジュール
- テキストエンコーダとテキストデコーダの両方として機能するテキスト変換モジュール
- Self Attention層はこれらのサブモジュール間で共有
- 画像変換モジュールのために、学習可能なクエリ埋め込みを作成
- クエリはSelf Attention層で互いに作用した後、Cross Attention層にて画像エンコーダの特徴と作用

構造による主要な特徴を説明します。

### Self Attention層の共有

Q-Formerにおいて、画像処理（クエリ側）とテキスト処理（テキスト入力側）で **Self-Attention層の重みを共有（Shared Self-Attention）** することには、主に以下の3つの重要な効果があります。

### 1. 「マルチモーダルな共通空間」の強制的な学習

画像由来の情報（クエリ）と、言語由来の情報（テキスト）が同じSelf-Attention層を通ることで、モデルは**画像の特徴と言語の概念を同じ次元の「意味」として扱う**ことを余儀なくされます。

* **効果:** 画像の中の「犬」という特徴と、テキストの「犬」という単語が、モデル内部で近い表現（ベクトル）としてマッピングされやすくなります。これにより、画像と言語の橋渡し（アライメント）がよりスムーズかつ強力になります。

### 2. インタラクション（相互作用）の効率化

重みを共有することで、画像クエリとテキストが互いに情報を補完し合うことができます。

* **効果:** テキスト側から得た文脈（例：「色について説明して」という意図）が、共有されたAttention層を通じて画像クエリ側の処理にも反映されます。その結果、Cross-Attentionで画像から情報を抽出する際に、「何に注目すべきか（この場合は『色』）」をより正確に判断できるようになります。

### 3. パラメータの節約と過学習の防止

画像用とテキスト用に別々のTransformerを用意する場合、パラメータ数が膨大になり、学習に必要なデータ量や計算リソースも増えてしまいます。

* **効果:**
* **軽量化:** 2つの独立したエンコーダを持つよりもパラメータ数を抑えられます。
* **汎化性能の向上:** 同じ重みで異なるモダリティ（画像とテキスト）を処理することで、特定のデータ形式に依存しすぎない、より抽象的で汎用的な特徴を学習するようになります（正則化のような効果）。



---

### 具体的な挙動のイメージ

Q-Formerの学習中、アテンション・マスクを制御することで以下のようなことが同時に起こります。

* **画像クエリに対して:** 「自分たち（32個のクエリ）の間で情報を整理しつつ、画像から必要な情報を取ってこよう」
* **テキストに対して:** 「画像クエリが持っている情報を参照しながら、言語としての整合性を整えよう」

このプロセスが**「同じ脳（Self-Attention層）」**で行われるため、画像とテキストがバラバラに理解されるのではなく、**「画像の意味を含んだ言語」**として統合された表現が作られるのです。

これが、後のプロセスでLLM（大規模言語モデル）に情報を渡した際に、LLMが「これは画像の説明だな」と即座に理解できる質の高い入力（ソフトプロンプト）になる理由の一つです。


## 実装

主要な実装のポイントを3つに整理して解説します。

### 1. 内部構造：2つのサブモジュールの共有

Q-Formerは、内部的に「画像用」と「テキスト用」の2つの役割を1つのモデルで切り替えて動かせる構造をしています。

* **Self-Attention層の共有**: 内部のSelf-Attention（自己注意）層は、画像情報の処理とテキスト情報の処理の両方で共通の重みを使います。これにより、画像とテキストを共通の概念空間で捉えられるようになります。

* **Cross-Attention層の挿入**: Transformerブロックの途中に「Cross-Attention」が配置されています。ここで、学習可能な**Query Embeddings**が画像エンコーダから出力された視覚特徴（Visual Features）を参照し、必要な情報を吸い上げます。

### 2. 学習可能なクエリ (Learnable Query Embeddings)

実装上の最も特徴的な部分です。

* **固定数の入力**: モデルの入力として、あらかじめ決められた数（BLIP-2では通常 **32個**）の学習可能なベクトル（Query）を用意します。
* **情報集約のボトルネック**: 画像がどんなに高解像度でも、Q-Formerはこの32個のベクトルに情報を凝縮します。これが「ボトルネック」として機能し、LLMにとってノイズの少ない「純粋な意味情報」だけを抽出します。

### 3. アテンション・マスキングによる多機能化

実装レベルでは、 **アテンション・マスク（Attention Mask）** を切り替えることで、1つのQ-Formerに3つの異なる学習目的を持たせています。

| 学習目的 | マスクの仕組み | 役割 |
| --- | --- | --- |
| **画像とテキストの対照学習** | 双方向マスク | 画像とテキストの「一致度」を測る |
| **画像に基づくテキスト生成** | 因果的マスク | 画像を見て説明文（キャプション）を書く訓練 |
| **画像と言語の照合** | 特殊なマスク | 画像と特定の単語が合っているか細かく判定する |

### 実装のスペック（BLIP-2の場合）

Hugging Faceなどのライブラリで公開されている実装値は以下の通りです。

* **ベースモデル**: BERT-base（約188Mパラメータ）
* **クエリ数**: 32個
* **隠れ層の次元**: 768次元
* **レイヤー数**: 12層（Cross-Attentionは通常2層おきに挿入）




