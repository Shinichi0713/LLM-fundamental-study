LLaVAï¼ˆLarge Language and Vision Assistantï¼‰ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®VLMã¨ã—ã¦æœ€ã‚‚æˆåŠŸã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ä¸€ã¤ã§ã™ã€‚ãã®æ§‹æˆã¯ã€éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤åˆç†çš„ãª**ã€Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€**ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

äººé–“ã§ã„ã†ã€Œç›®ã€ã€Œç¥çµŒã€ã€Œè„³ã€ã®3ã¤ã®ãƒ‘ãƒ¼ãƒ„ãŒã©ã®ã‚ˆã†ã«çµ„ã¿åˆã‚ã•ã£ã¦ã„ã‚‹ã‹ã€å…·ä½“çš„ã«è§£èª¬ã—ã¾ã™ã€‚

---

### 1. 3ã¤ã®ä¸»è¦æ§‹æˆè¦ç´ 

LLaVAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æˆã‚Šç«‹ã£ã¦ã„ã¾ã™ã€‚

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å…·ä½“çš„ãªãƒ¢ãƒ‡ãƒ«ä¾‹ | å½¹å‰²ï¼ˆä¾‹ãˆï¼‰ |
| --- | --- | --- |
| **Vision Encoder** | **CLIP (ViT-L/14)** | **ã€Œç›®ã€**ï¼šç”»åƒã‚’è¦‹ã¦ã€ãã®ç‰¹å¾´ã‚’æ•°å€¤ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚ |
| **Connector (Projector)** | **MLP (å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³)** | **ã€Œç¥çµŒã€**ï¼šç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€LLMãŒç†è§£ã§ãã‚‹å½¢å¼ã«ç¿»è¨³ãƒ»å¤‰æ›ã—ã¾ã™ã€‚ |
| **LLM (Backbone)** | **Vicuna (LLaMAãƒ™ãƒ¼ã‚¹)** | **ã€Œè„³ã€**ï¼šç”»åƒã®æƒ…å ±ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’çµ„ã¿åˆã‚ã›ã¦è€ƒãˆã€æ–‡ç« ã‚’ä½œã‚Šã¾ã™ã€‚ |

---

### 2. å„ãƒ‘ãƒ¼ãƒ„ã®è©³ç´°ã¨å½¹å‰²

#### â‘  Vision Encoderï¼ˆè¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰

LLaVAã§ã¯ã€OpenAIãŒé–‹ç™ºã—ãŸ**CLIP**ã®Vision Transformerï¼ˆViTï¼‰ãƒ¢ãƒ‡ãƒ«ãŒæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

* ç”»åƒã‚’ã€Œ14x14ãƒ”ã‚¯ã‚»ãƒ«ã€ãªã©ã®ãƒ‘ãƒƒãƒã«åˆ†å‰²ã—ã€ãã‚Œãã‚Œã®æ–­ç‰‡ãŒä½•ã‚’è¡¨ã—ã¦ã„ã‚‹ã‹ï¼ˆã‚¨ãƒƒã‚¸ã€è‰²ã€ç‰©ä½“ã®ä¸€éƒ¨ãªã©ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
* å­¦ç¿’åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ãŸã‚ã€å¤šãã®å ´åˆã“ã®ã€Œç›®ã€ã®éƒ¨åˆ†ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯**å›ºå®šï¼ˆFrozenï¼‰**ã•ã‚Œã¦ãŠã‚Šã€ã™ã§ã«ã‚ã‚‹é«˜ã„ç”»åƒèªè­˜èƒ½åŠ›ã‚’ãã®ã¾ã¾åˆ©ç”¨ã—ã¾ã™ã€‚

#### â‘¡ Connector / Projection Layerï¼ˆæ¥ç¶šå±¤ï¼‰

Vision EncoderãŒå‡ºåŠ›ã—ãŸãƒ‡ãƒ¼ã‚¿ã¯ã€ãã®ã¾ã¾ã§ã¯LLMã«å…¥åŠ›ã§ãã¾ã›ã‚“ï¼ˆå½¢å¼ãŒç•°ãªã‚‹ãŸã‚ï¼‰ã€‚

* **åˆæœŸã®LLaVA:** å˜ç´”ãªã€Œç·šå½¢è¡Œåˆ—ï¼ˆLinear Layerï¼‰ã€1å±¤ã®ã¿ã€‚
* **LLaVA-1.5ä»¥é™:** 2å±¤ã®**MLPï¼ˆMulti-Layer Perceptronï¼‰**ã«é€²åŒ–ã—ã€ã‚ˆã‚Šè¤‡é›‘ãªæƒ…å ±ã®å¤‰æ›ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚
* ã“ã®å±¤ãŒã€Œç”»åƒã®ãƒ‘ãƒƒãƒã€ã‚’ã€Œå˜èªï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨åŒã˜ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã€ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€LLMã¯ç”»åƒã‚’ã€Œç‰¹æ®Šãªå˜èªã®ç¾…åˆ—ã€ã¨ã—ã¦èª­ã¿å–ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

#### â‘¢ LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰

å®Ÿè³ªçš„ãªã€ŒçŸ¥èƒ½ã€ã‚’å¸ã‚‹éƒ¨åˆ†ã§ã™ã€‚

* æœ€åˆæœŸã®ãƒ¢ãƒ‡ãƒ«ã§ã¯**Vicuna**ï¼ˆLLaMAã‚’å¯¾è©±ç”¨ã«å¾®èª¿æ•´ã—ãŸã‚‚ã®ï¼‰ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã—ãŸãŒã€ç¾åœ¨ã¯ **Llama 3** ã‚„ **Mistral**, **Qwen** ãªã©ã€æ§˜ã€…ãªå¼·åŠ›ãªLLMã¨çµ„ã¿åˆã‚ã›ãŸæ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã™ã€‚
* ç”»åƒã‹ã‚‰å¤‰æ›ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸€ã¤ã®æ–‡ç« ã¨ã—ã¦ç¹‹ã’ã¦å‡¦ç†ã—ã¾ã™ã€‚

---

### 3. LLaVAãŒã€Œè³¢ã„ã€ç†ç”±ï¼šå­¦ç¿’ã®2æ®µéš

ã“ã®æ§‹æˆã‚’æœ€å¤§é™æ´»ã‹ã™ãŸã‚ã«ã€LLaVAã¯2ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚

1. **Stage 1: æ¦‚å¿µã®ç´ä»˜ã‘ (Alignment)**
å¤§é‡ã®ã€Œç”»åƒã¨çŸ­ã„èª¬æ˜æ–‡ã€ã®ãƒšã‚¢ã‚’ä½¿ã„ã€**Connectorã ã‘ã‚’å­¦ç¿’**ã•ã›ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è„³ï¼ˆLLMï¼‰ãŒç›®ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆVisionãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’å˜èªã¨ã—ã¦èªè­˜ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
2. **Stage 2: æŒ‡ç¤ºã¸ã®é©å¿œ (Instruction Tuning)**
GPT-4ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸã€Œç”»åƒã«é–¢ã™ã‚‹è¤‡é›‘ãªå¯¾è©±ãƒ‡ãƒ¼ã‚¿ã€ã‚’ä½¿ã„ã€**LLMã¨Connectorã®ä¸¡æ–¹ã‚’å­¦ç¿’**ã•ã›ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã€Œã“ã®å†™çœŸã®ã©ã“ãŒé¢ç™½ã„ã®ï¼Ÿã€ã¨ã„ã£ãŸé«˜åº¦ãªæŒ‡ç¤ºã«å¾“ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

## ãƒ†ã‚¹ãƒˆ
VQAã§ãƒ†ã‚¹ãƒˆã‚’è¡Œã†ã€‚

ç›®çš„ãŒã¯ã£ãã‚Šã—ã¦ã„ã¾ã™ã­ã€‚
**ã€Œæ–°è¦VLMã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ãŒâ€œæœ¬å½“ã«åŠ¹ã„ã¦ã„ã‚‹ã‹â€ã‚’çŸ­æ™‚é–“ã§ç¢ºèªã—ãŸã„ã€**ã¨ã„ã†å‰æã§ã€
**åŠ¹æœãŒè¦‹ãˆã‚„ã™ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹æˆ**ã‚’ã€ç†ç”±ä»˜ãã§å…·ä½“ææ¡ˆã—ã¾ã™ã€‚

---

# çµè«–ï¼ˆæœ€çŸ­ã§åŠ¹æœç¢ºèªã§ãã‚‹æ§‹æˆï¼‰

ğŸ‘‰ **LLaVA-Instructï¼ˆå°ï¼‰ï¼‹ COCO Captions ï¼‹ VQA v2ï¼ˆå°‘é‡ï¼‰**

ã“ã‚ŒãŒ **æœ€ã‚‚å†ç¾æ€§ãŒé«˜ãã€å·®åˆ†ãŒè¦‹ãˆã‚„ã™ã„**æ§‹æˆã§ã™ã€‚

---

# ãªãœã€ŒåŠ¹æœç¢ºèªã€ã«ã“ã®3ã¤ãŒè‰¯ã„ã‹

åŠ¹æœç¢ºèªã§é‡è¦ãªã®ã¯ï¼š

1. **å­¦ç¿’å‰å¾Œã§æŒ™å‹•ãŒæ˜ç¢ºã«å¤‰ã‚ã‚‹**
2. **å®šé‡è©•ä¾¡ã¨å®šæ€§è©•ä¾¡ã®ä¸¡æ–¹ãŒã§ãã‚‹**
3. **å­¦ç¿’ã‚³ã‚¹ãƒˆãŒå°ã•ã„**

ã“ã®æ¡ä»¶ã‚’æœ€ã‚‚æº€ãŸã™ã®ãŒä»¥ä¸‹ã§ã™ã€‚

---

## 1ï¸âƒ£ COCO Captionsï¼ˆè¦–è¦šè¨€èªã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆç¢ºèªï¼‰

### å½¹å‰²

* ã€Œç”»åƒ â†’ æ­£ã—ã„è¨€èªè¡¨ç¾ã€ãŒå‡ºã‚‹ã‹

### ãªãœåŠ¹ãã‹

* å­¦ç¿’å‰ï¼š
  â†’ æŠ½è±¡çš„ãƒ»çš„å¤–ã‚Œãªèª¬æ˜
* å­¦ç¿’å¾Œï¼š
  â†’ **ç‰©ä½“ãƒ»å±æ€§ãƒ»é–¢ä¿‚ãŒæ˜ç¢ºã«æ”¹å–„**

### ä½¿ã„æ–¹ï¼ˆæœ€å°ï¼‰

* 5kã€œ10k images ã§ååˆ†
* 1ã€œ2 epoch

### ç¢ºèªæŒ‡æ¨™

* BLEU / CIDErï¼ˆå®šé‡ï¼‰
* ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…å®¹ï¼ˆå®šæ€§ï¼‰

---

## 2ï¸âƒ£ LLaVA-Instructï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMåŒ–ã®ç¢ºèªï¼‰

### å½¹å‰²

* ã€Œç”»åƒã«ã¤ã„ã¦ä¼šè©±ã§ãã‚‹ã‹ã€

### ãªãœåŠ¹æœãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã‹

* å­¦ç¿’å‰ï¼š
  â†’ è³ªå•ã«å¯¾ã—ã¦ **è¨€èªçš„ã«ãã‚Œã£ã½ã„ãŒç”»åƒç„¡è¦–**
* å­¦ç¿’å¾Œï¼š
  â†’ **ç”»åƒã«åŸºã¥ãå…·ä½“å›ç­”**

### æœ€å°æ§‹æˆ

* **10kã€œ20k samples** ã§ååˆ†
* full 150k ã¯ä¸è¦

### ç¢ºèªæ–¹æ³•

```text
Q: ã“ã®ç”»åƒã«ä½•ãŒå†™ã£ã¦ã„ã¾ã™ã‹ï¼Ÿ
Q: èµ¤ã„ç‰©ä½“ã¯ä½•ã§ã™ã‹ï¼Ÿ
Q: å³å´ã«ã„ã‚‹å‹•ç‰©ã¯ï¼Ÿ
```

â†’ ç”»åƒä¾å­˜æ€§ãŒæ˜ç¢ºã«å‡ºã‚‹

---

## 3ï¸âƒ£ VQA v2ï¼ˆæ¨è«–èƒ½åŠ›ã®å·®åˆ†ç¢ºèªï¼‰

### å½¹å‰²

* ã€Œè³ªå• â†’ ç”»åƒ â†’ æ­£ç­”ã€ã®å› æœç¢ºèª

### ãªãœè‰¯ã„ã‹

* é¸æŠå¼ã§ **Accuracy ãŒæ˜ç¢º**
* ç”»åƒç„¡è¦–ãƒ¢ãƒ‡ãƒ«ã¯ 25% å‰å¾Œã«å¼µã‚Šä»˜ã

### æœ€å°æ§‹æˆ

* train: 10k
* val: 5k

### æŒ‡æ¨™

* Accuracyï¼ˆå³å·®ãŒå‡ºã‚‹ï¼‰

---

# å®Ÿé¨“æ§‹æˆï¼ˆãŠã™ã™ã‚ï¼‰

```text
Stage 0: Vision encoder + LLM æœªå­¦ç¿’
Stage 1: COCO Captionï¼ˆalignmentï¼‰
Stage 2: LLaVA-Instructï¼ˆä¼šè©±ï¼‰
Stage 3: VQA v2ï¼ˆæ¨è«–ï¼‰
```

ğŸ‘‰ **Stageã”ã¨ã«è©•ä¾¡**ã™ã‚‹ã¨åŠ¹æœãŒæ¥µã‚ã¦è¦‹ã‚„ã™ã„ã€‚

---

# å®šé‡è©•ä¾¡ï¼ˆVLMEvalKitã¨ç›´çµï¼‰

å­¦ç¿’å¾Œã«ï¼š

* **MMBench_DEV_ENï¼ˆ100 samplesï¼‰**
* **POPEï¼ˆå¹»è¦šï¼‰**

ã‚’å›ã™ã¨ã€

* ç”»åƒç„¡è¦– â†’ é©åˆ‡å‚ç…§
* å¹»è¦šç‡ã®ä½ä¸‹

ãŒã¯ã£ãã‚Šå‡ºã¾ã™ã€‚

---

# è»½é‡å®Ÿé¨“å‘ã‘ã€Œæœ¬å½“ã«æœ€å°ã€ã‚»ãƒƒãƒˆ

GPU 1æšãƒ»çŸ­æ™‚é–“ãªã‚‰ï¼š

```text
COCO Captions 5k
LLaVA-Instruct 10k
VQA v2 5k
```

â†’ æ•°æ™‚é–“ã€œåŠæ—¥ã§å·®ãŒè¦‹ãˆã‚‹

---

# ã‚ˆãã‚ã‚‹å¤±æ•—ï¼ˆé¿ã‘ã‚‹ã¹ãï¼‰

âŒ Instructionã ã‘ã§å­¦ç¿’
â†’ ç”»åƒãŒç„¡è¦–ã•ã‚Œã‚‹

âŒ VQAã ã‘
â†’ ä¼šè©±èƒ½åŠ›ãŒè‚²ãŸãªã„

âŒ Webãƒã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿
â†’ åŠ¹æœãŒåˆ†ã‹ã‚Šã«ãã„

---

# ãƒ¢ãƒ‡ãƒ«æ§‹é€ åˆ¥ã®ç›¸æ€§

| æ§‹é€                 | ç‰¹ã«åŠ¹ããƒ‡ãƒ¼ã‚¿             |
| ----------------- | ------------------- |
| Linear Projection | COCO + LLaVA        |
| Q-Former          | Caption + VQA       |
| Resampler         | Caption + Reasoning |
| Cross-Attn        | VQA + GQA           |

---

# åŠ¹æœç¢ºèªã®ã€Œè¦³ç‚¹ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã€

* [ ] åŒä¸€è³ªå•ã§ç”»åƒã‚’å¤‰ãˆã‚‹ã¨ç­”ãˆãŒå¤‰ã‚ã‚‹
* [ ] ç”»åƒã‚’éš ã™ã¨æ€§èƒ½ãŒè½ã¡ã‚‹
* [ ] CaptionãŒå…·ä½“åŒ–ã™ã‚‹
* [ ] VQA AccuracyãŒãƒ©ãƒ³ãƒ€ãƒ ä»¥ä¸Š

---

## æ¬¡ã«ã§ãã‚‹å…·ä½“æ”¯æ´

* **LLaVA-Instructã®10kæŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
* **COCO / VQA ã‚’å…±é€šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æƒãˆã‚‹æ–¹æ³•**
* **å­¦ç¿’å‰å¾Œã®æ¯”è¼ƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé›†**
* **VLMEvalKitã§ã®æœ€å°è©•ä¾¡ã‚»ãƒƒãƒˆ**


äº†è§£ã—ã¾ã—ãŸã€‚
ä»¥ä¸‹ã¯ **ã€ŒGoogle Colabï¼ˆT4 / A100 1æšï¼‰ã§ â€œç¢ºå®Ÿã«å›ã‚‹â€ Q-Former å­¦ç¿’ã®æœ€å°æ§‹æˆã€**ã§ã™ã€‚
**BLIP-2 è«–æ–‡ã®æ€æƒ³ã‚’ä¿ã¡ã¤ã¤ã€ç ”ç©¶ãƒ»å‹•ä½œç¢ºèªã«å¿…è¦ãªè¦ç´ ã ã‘**æ®‹ã—ã¦ã„ã¾ã™ã€‚

> ç›®çš„
>
> * Q-Former ãŒ **æœ¬å½“ã« Vision â†’ LLM ã‚’æ©‹æ¸¡ã—ã§ãã¦ã„ã‚‹ã‹**ã‚’ç¢ºèª
> * æ•°æ™‚é–“ä»¥å†…ãƒ»VRAM 12â€“16GB ã§å®Œèµ°

---

# å…¨ä½“è¨­è¨ˆï¼ˆæœ€å°æ§‹æˆï¼‰

```text
ç”»åƒ
 â†“
Frozen Vision Encoderï¼ˆCLIP ViT-B/16ï¼‰
 â†“
Q-Formerï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰
 â†“
Frozen LLMï¼ˆOPT-1.3B or GPT2ï¼‰
```

âœ” å­¦ç¿’ã™ã‚‹ã®ã¯ **Q-Formerã®ã¿**
âœ” LLMãƒ»Vision Encoder ã¯å®Œå…¨ freeze
âœ” ã‚¿ã‚¹ã‚¯ã¯ **Image Captioningï¼ˆæœ€ã‚‚åŠ¹æœãŒè¦‹ãˆã‚„ã™ã„ï¼‰**

---

# Colab æ¨å¥¨æ¡ä»¶

| é …ç›®         | è¨­å®š                |
| ---------- | ----------------- |
| GPU        | T4 / A100         |
| Precision  | FP16              |
| Image size | 224               |
| #Queries   | 16                |
| Dataset    | COCO Captionsï¼ˆ5kï¼‰ |

---

# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæº–å‚™ï¼ˆColabï¼‰

```bash
pip install torch torchvision transformers timm einops
```

---

# 2. Vision Encoderï¼ˆFrozenï¼‰

```python
import torch
import torch.nn as nn
from transformers import CLIPVisionModel

class FrozenCLIPVision(nn.Module):
    def __init__(self):
        super().__init__()
        self.vision = CLIPVisionModel.from_pretrained(
            "openai/clip-vit-base-patch16"
        )
        for p in self.parameters():
            p.requires_grad = False

    def forward(self, images):
        out = self.vision(pixel_values=images)
        return out.last_hidden_state  # [B, N, 768]
```

---

# 3. Q-Formerï¼ˆæœ€å°å®Ÿè£…ï¼‰

### ãƒã‚¤ãƒ³ãƒˆ

* Learnable Query Tokens
* Cross-Attentionï¼ˆQuery â†’ Visionï¼‰
* å‡ºåŠ›ã¯ **LLMåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ**

```python
from transformers import BertConfig, BertModel

class QFormer(nn.Module):
    def __init__(self, num_queries=16, vision_dim=768, llm_dim=768):
        super().__init__()

        self.query_tokens = nn.Parameter(
            torch.randn(1, num_queries, llm_dim)
        )

        config = BertConfig(
            hidden_size=llm_dim,
            num_hidden_layers=6,
            num_attention_heads=12,
            encoder_width=vision_dim,
            add_cross_attention=True,
            is_decoder=True
        )
        self.qformer = BertModel(config)

    def forward(self, vision_feats):
        B = vision_feats.size(0)
        queries = self.query_tokens.expand(B, -1, -1)

        out = self.qformer(
            inputs_embeds=queries,
            encoder_hidden_states=vision_feats,
            encoder_attention_mask=torch.ones(
                vision_feats.shape[:-1],
                device=vision_feats.device
            )
        )
        return out.last_hidden_state  # [B, Q, D]
```

---

# 4. LLMï¼ˆFrozen, è»½é‡ï¼‰

Colabã§ã¯ **OPT-1.3B or GPT2** ãŒç¾å®Ÿçš„ã§ã™ã€‚

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
llm = AutoModelForCausalLM.from_pretrained("gpt2")

for p in llm.parameters():
    p.requires_grad = False
```

---

# 5. Q-Former â†’ LLM æ¥ç¶š

Q-Former å‡ºåŠ›ã‚’ **prefix token** ã¨ã—ã¦ LLM ã«æ¸¡ã—ã¾ã™ã€‚

```python
def forward_llm(q_tokens, text_ids):
    inputs_embeds = llm.transformer.wte(text_ids)
    inputs_embeds = torch.cat([q_tokens, inputs_embeds], dim=1)

    outputs = llm(
        inputs_embeds=inputs_embeds,
        labels=text_ids
    )
    return outputs.loss
```

---

# 6. å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆæœ€å°ï¼‰

```python
optimizer = torch.optim.AdamW(
    qformer.parameters(), lr=1e-4
)

for images, captions in dataloader:
    vision_feats = vision_encoder(images)
    q_tokens = qformer(vision_feats)

    text_ids = tokenizer(
        captions,
        return_tensors="pt",
        padding=True
    ).input_ids.to(images.device)

    loss = forward_llm(q_tokens, text_ids)

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

---

# 7. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ€å°ï¼‰

### COCO Captions æŠ½å‡ºï¼ˆ5kï¼‰

```text
image, caption
```

ä¾‹ï¼š

```json
{"image": "0001.jpg", "caption": "A dog is running on the beach."}
```

ğŸ‘‰ **5k ã§ååˆ†ã«åŠ¹æœãŒè¦‹ãˆã‚‹**

---

# 8. åŠ¹æœç¢ºèªã®æ–¹æ³•ï¼ˆé‡è¦ï¼‰

### å­¦ç¿’å‰

```
Q: Describe the image.
A: I am not sure what is in the image.
```

### å­¦ç¿’å¾Œ

```
Q: Describe the image.
A: A dog running on the beach near the ocean.
```

âœ” **ç”»åƒä¾å­˜æ€§ãŒå‡ºã‚Œã°æˆåŠŸ**

---

# 9. Colabã§è½ã¡ãªã„ãŸã‚ã®è¨­å®š

```python
torch.backends.cuda.matmul.allow_tf32 = True
torch.cuda.empty_cache()
```

```bash
export TOKENIZERS_PARALLELISM=false
```

---

# 10. ã“ã®æ§‹æˆã§ã€Œåˆ†ã‹ã‚‹ã“ã¨ / åˆ†ã‹ã‚‰ãªã„ã“ã¨ã€

### åˆ†ã‹ã‚‹

* Q-Former ãŒ vision ã‚’è¨€èªã«å¤‰æ›ã§ãã‚‹ã‹
* Query æ•°ã®å½±éŸ¿
* Linear Projection ã¨ã®æ¯”è¼ƒ

### åˆ†ã‹ã‚‰ãªã„

* å¤§è¦æ¨¡æ€§èƒ½
* é«˜åº¦æ¨è«–ï¼ˆMMMUç­‰ï¼‰

---

# 11. æ¬¡ã«é€²ã‚€ãªã‚‰ï¼ˆå¼·ããŠã™ã™ã‚ï¼‰

* ğŸ”¬ **Q-Former vs Linear Projection ablation**
* ğŸ”¬ **Queryæ•° {8,16,32} æ¯”è¼ƒ**
* ğŸ”¬ **VQA loss è¿½åŠ **
* ğŸ”¬ **VLMEvalKit ã§ MMBench 50å•è©•ä¾¡**

---

## ã¾ã¨ã‚ï¼ˆè¶…é‡è¦ï¼‰

* **ã“ã®æ§‹æˆã¯ã€Œæœ€å°ã ãŒæ­£ã—ã„ã€Q-Former**
* Colabã§ç¢ºå®Ÿã«å‹•ã
* åŠ¹æœãŒå®šæ€§çš„ã«ã‚‚å®šé‡çš„ã«ã‚‚ç¢ºèªã§ãã‚‹
* è«–æ–‡ãƒ»å®Ÿè£…ã®åœŸå°ã«ãªã‚‹

---

å¿…è¦ã§ã‚ã‚Œã°æ¬¡ã«ï¼š

* **ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’1ã‚»ãƒ«ã§å‹•ã Colab Notebook åŒ–**
* **Linear Projection ç‰ˆã¨ã®å®Œå…¨æ¯”è¼ƒã‚³ãƒ¼ãƒ‰**
* **LLaVA / BLIP-2 å½¢å¼ã¸ã®æ‹¡å¼µ**

ã©ã“ã¾ã§ã‚„ã‚Šã¾ã™ã‹ï¼Ÿ


### ã¾ã¨ã‚

LLaVAã®æœ€å¤§ã®ç‰¹å¾´ã¯ã€**ã€Œæ—¢å­˜ã®å„ªã‚ŒãŸç›®ï¼ˆCLIPï¼‰ã¨ã€å„ªã‚ŒãŸè„³ï¼ˆLLaMAç³»LLMï¼‰ã‚’ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã§ç¹‹ã„ã ã€**ã¨ã„ã†è¨­è¨ˆã®ç¾ã—ã•ã«ã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚·ãƒ³ãƒ—ãƒ«ã•ã‚†ãˆã«ã€å¤šãã®ç ”ç©¶è€…ãŒæ”¹è‰¯ã‚’åŠ ãˆã‚„ã™ãã€ç¾åœ¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹VLMã®æ¨™æº–çš„ãªå½¢ã¨ãªã‚Šã¾ã—ãŸã€‚

æ¬¡ã¯ã€ã“ã®LLaVAã®æ§‹æˆã‚’ãƒ™ãƒ¼ã‚¹ã«**ã€Œã‚ˆã‚Šé«˜è§£åƒåº¦ãªç”»åƒã«å¯¾å¿œã•ã›ãŸLLaVA-NeXTã€**ã‚„**ã€Œå‹•ç”»ã«å¯¾å¿œã—ãŸæ‹¡å¼µã€**ã«ã¤ã„ã¦è©³ã—ãè¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã‹ï¼Ÿ

[LLaVA Model Architecture Explanation](https://www.youtube.com/watch?v=r2jAGI9M0mo)
ã“ã®å‹•ç”»ã§ã¯ã€LLaVAã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¦–è¦šçš„ãªå›³è§£ã¨ã¨ã‚‚ã«è©³ã—ãè§£èª¬ã—ã¦ãŠã‚Šã€CLIPã¨LLaMAãŒã©ã®ã‚ˆã†ã«çµ±åˆã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚