LLaVA（Large Language and Vision Assistant）は、オープンソースのVLMとして最も成功しているモデルの一つです。その構成は、非常にシンプルかつ合理的な**「モジュール型アーキテクチャ」**を採用しています。

人間でいう「目」「神経」「脳」の3つのパーツがどのように組み合わさっているか、具体的に解説します。

---

### 1. 3つの主要構成要素

LLaVAのアーキテクチャは、以下の3つのコンポーネントで成り立っています。

| コンポーネント | 具体的なモデル例 | 役割（例え） |
| --- | --- | --- |
| **Vision Encoder** | **CLIP (ViT-L/14)** | **「目」**：画像を見て、その特徴を数値（ベクトル）に変換します。 |
| **Connector (Projector)** | **MLP (多層パーセプトロン)** | **「神経」**：画像のデータを、LLMが理解できる形式に翻訳・変換します。 |
| **LLM (Backbone)** | **Vicuna (LLaMAベース)** | **「脳」**：画像の情報とユーザーの質問を組み合わせて考え、文章を作ります。 |

---

### 2. 各パーツの詳細と役割

#### ① Vision Encoder（視覚エンコーダ）

LLaVAでは、OpenAIが開発した**CLIP**のVision Transformer（ViT）モデルが採用されています。

* 画像を「14x14ピクセル」などのパッチに分割し、それぞれの断片が何を表しているか（エッジ、色、物体の一部など）を抽出します。
* 学習効率を上げるため、多くの場合この「目」の部分のパラメータは**固定（Frozen）**されており、すでにある高い画像認識能力をそのまま利用します。

#### ② Connector / Projection Layer（接続層）

Vision Encoderが出力したデータは、そのままではLLMに入力できません（形式が異なるため）。

* **初期のLLaVA:** 単純な「線形行列（Linear Layer）」1層のみ。
* **LLaVA-1.5以降:** 2層の**MLP（Multi-Layer Perceptron）**に進化し、より複雑な情報の変換が可能になりました。
* この層が「画像のパッチ」を「単語（トークン）と同じようなデータ」に変換することで、LLMは画像を「特殊な単語の羅列」として読み取れるようになります。

#### ③ LLM（大規模言語モデル）

実質的な「知能」を司る部分です。

* 最初期のモデルでは**Vicuna**（LLaMAを対話用に微調整したもの）が使われていましたが、現在は **Llama 3** や **Mistral**, **Qwen** など、様々な強力なLLMと組み合わせた派生モデルが存在します。
* 画像から変換されたトークンと、ユーザーが入力したテキストプロンプトを一つの文章として繋げて処理します。

---

### 3. LLaVAが「賢い」理由：学習の2段階

この構成を最大限活かすために、LLaVAは2つのステップで学習されます。

1. **Stage 1: 概念の紐付け (Alignment)**
大量の「画像と短い説明文」のペアを使い、**Connectorだけを学習**させます。これにより、脳（LLM）が目のデータ（Visionトークン）を単語として認識できるようになります。
2. **Stage 2: 指示への適応 (Instruction Tuning)**
GPT-4によって生成された「画像に関する複雑な対話データ」を使い、**LLMとConnectorの両方を学習**させます。これにより、「この写真のどこが面白いの？」といった高度な指示に従えるようになります。

---

### まとめ

LLaVAの最大の特徴は、**「既存の優れた目（CLIP）と、優れた脳（LLaMA系LLM）を、シンプルなプロジェクターで繋いだ」**という設計の美しさにあります。このシンプルさゆえに、多くの研究者が改良を加えやすく、現在のオープンソースVLMの標準的な形となりました。

次は、このLLaVAの構成をベースに**「より高解像度な画像に対応させたLLaVA-NeXT」**や**「動画に対応した拡張」**について詳しく見てみましょうか？

[LLaVA Model Architecture Explanation](https://www.youtube.com/watch?v=r2jAGI9M0mo)
この動画では、LLaVAのアーキテクチャを視覚的な図解とともに詳しく解説しており、CLIPとLLaMAがどのように統合されているかを直感的に理解するのに役立ちます。