今回は前回までで扱ってきたVLMの試験について、研究室配属直後・新規参加者・将来の自分を想定してまとめた説明です。
「**なぜこの試験をするのか**」「**何をもって成功とするのか**」「**どういう順序で進めるのか**」が分かる構成にしています。


## 本試験の目的と進め方

Q-Former + LoRA を用いた最小構成 VLM 実験について以下に示します。


### 1. この試験で何を確認したいのか（試験意図）

#### 背景

Vision-Language Model（VLM）は、

* 画像エンコーダ（Vision Encoder）
* 言語モデル（LLM）
* それらをつなぐ中間機構（Q-Former など）

から構成されます。

しかし実装・学習が複雑なため、

> 「本当に“画像情報”が言語生成に使われているのか？」
> 「どのモジュールがどの役割を果たしているのか？」

が分かりにくくなりがちです。

#### 本試験の狙い

**この実験では、次の一点だけを明確に確認します。**

> **Q-Former を介して画像特徴を与えることで、
> LLM の出力が画像に依存して変化するようになるか？**

つまり、

* Vision Encoder は「特徴抽出器」
* Q-Former は「視覚情報の要約・選別器」
* LLM（LoRA）は「画像条件付き言語生成器」

として **役割分担が成立しているか** を検証します。

#### なぜ Q-Former + LoRA なのか

* LLM 全体を学習すると
  → *言語能力の再学習* と *視覚条件付け* が混ざってしまう
* Q-Formerのみだと
  → LLM 側が画像を活用しきれない場合がある

そこで、

* **Q-Former：画像情報を抽出・圧縮**
* **LoRA：LLMが画像トークンを“どう使うか”だけ学習**

という **最小限かつ因果関係が追いやすい構成**を採用します。

### 2. この試験で「やらないこと」

本試験は**性能競争が目的ではありません**。

以下は意図的に行いません。

* ❌ 大規模データセットでの高精度評価
* ❌ Vision Encoder や LLM 本体のフルファインチューニング
* ❌ 複雑なタスク（VQA、指示追従など）

目的はあくまで **構造理解と動作確認** です。


### 3. モデル構成の全体像（概念図）

```
画像
 ↓
Vision Encoder（CLIP, freeze）
 ↓
視覚特徴列
 ↓
Q-Former（学習）
 ↓
少数の視覚トークン
 ↓
Linear Projection（学習）
 ↓
LLM 入力空間
 ↓
LLM + LoRA（LoRAのみ学習）
 ↓
テキスト生成
```

### 4. 試験をどのように進めるか（手順）

#### Step 1: 最小データでの準備

* データ：COCO / Flickr30k のごく一部
* タスク：**画像 → 短いキャプション生成**
* 目的：

  * loss が下がるか
  * 学習が正常に流れるか

#### Step 2: Freeze / Train の切り分け確認

| モジュール          | 状態     |
| -------------- | ------ |
| Vision Encoder | freeze |
| LLM 本体         | freeze |
| Q-Former       | train  |
| LLM LoRA       | train  |
| Projection     | train  |

→ **学習パラメータが少量であること**を確認する

#### Step 3: 学習が成立しているかの確認

以下を確認します。

* loss が epoch ごとに減少する
* 同じ文章でも、画像を変えると生成が変わる
* Q-Former を固定すると性能が落ちる

これにより、

> **「画像 → Q-Former → LLM」
> という情報経路が実際に使われている**

ことを確認します。

#### Step 4: 対照実験（重要）

理解を深めるために以下を比較します。

| 実験                | 期待される結果  |
| ----------------- | -------- |
| Q-Formerあり        | 画像依存の生成  |
| Q-Formerなし（平均プール） | 表現力が低下   |
| LoRAなし            | 画像を無視しがち |

→ **各構成要素の役割を分離して理解**できます。

### 5. この試験で得られるもの

#### 技術的に理解できること

* Q-Former の役割（なぜ Query が必要か）
* LLM が画像トークンをどう条件として使うか
* LoRA が「適応」だけを担う仕組み

#### 実装的に得られること

* BLIP-2 系 VLM の最小実装テンプレート
* Colab で再現可能な実験環境
* 将来の拡張（VQA / Instruct / 大規模化）の土台

### 6. この実験の位置づけ（まとめ）

この試験は、

> **「VLM を“ブラックボックスとして使う”前に、
> “中で何が起きているかを理解する”ための通過点」**

です。

* 成功すれば
  → 自作 VLM の設計・改良が理論的にできる
* 失敗しても
  → どの接続・学習が効いていないかが分かる

ということを検証することを目的としています。





