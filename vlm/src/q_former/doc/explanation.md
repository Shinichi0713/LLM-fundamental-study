
1/3にVLMの手法でBlip-2に採用されている手法 [**Q-Former**](https://yoshishinnze.hatenablog.com/entry/2026/01/03/100210) について説明しました。
Q-FormerがVLMとして、画像の情報を上手に抽出できる理由にフォーカスをあてて説明しました。
ですが、その際は内部構造の実装法には触れていませんでした。

今回は改めて内部構造の実装法について扱っていきたいと思います。


## Q-Formerの構成
前回のおさらいを兼ねて、Q-Formerの構成を簡単に説明します。

### 1. 学習可能なクエリ (Learnable Queries)

Q-Formerの最大の特徴は、入力として「固定された32個（モデルにより数は異なる）の特別なベクトル」を持つことです。

* **技術の役割:** これらは最初はランダムな値ですが、学習が進むにつれて「画像の形に注目せよ」「画像の色を抽出せよ」といった **「情報の抽出ルール」** を学習します。
* **メリット:** 画像エンコーダから出力される数千個のパッチ（断片データ）をそのままLLMに投げると情報が多すぎますが、この「クエリ」が重要なエッセンスだけを吸い上げることで、情報の密度を飛躍的に高めます。


### 2. クロス・アテンション (Cross-Attention)

クエリが画像情報を実際に「吸い上げる」ための計算メカニズムです。

* **技術の仕組み:** クエリを「検索条件（Query）」、画像エンコーダの出力を「検索対象のデータベース（Key & Value）」として照合します。
* **メリット:** これにより、クエリが画像の中の「どこに注目すべきか」を柔軟に決定できます。例えば、テキストで「色」について問われていれば、画像内の色に関する特徴に強い重み（アテンション）を置くことができます。


### 3. 多目的・二重トランスフォーマー構造 (Dual-purpose Transformer)

Q-Formerは、内部のアテンション計算（Self-Attention）において、画像由来のクエリと、テキスト入力を**同じ層で同時に処理**できる構造を持っています。

* **技術の仕組み:** **アテンション・マスク**を切り替えることで、1つのモデルで3つのタスクを同時に学習します。

1. **ITC (Image-Text Contrastive Learning):** 画像とテキストが似ているか判定。
2. **ITG (Image-grounded Text Generation):** 画像を見て文字を書く。
3. **ITM (Image-Text Matching):** 画像とテキストのペアが正しいか細かくチェック。

* **メリット:** 「画像」と「言語」が全く同じネットワークで処理されるため、両者の概念的な境界が消え、LLMが理解しやすい「言語に近い画像表現」が作られます。

### モデル処理のイメージ
絵で見てみると尚、わかりやすいと思います。


<img src="image/explanation/1768618244495.png" alt="Q-Former form" width="850" style="display: block; margin: 0 auto;">


Q-Formerは大きく3パートで構成されます。
1. Query Token Embeddings
2. Self-Attention Block
3. Cross-Attention Block (to Vision Encoder)

__1. Query Tokens__

絵の右下にある"Learned Queries"の部分です。
動作のポイントは以下です。

- 位置依存ではない
- 画像コンテンツ依存で更新
- 非テキストだが後段でLLMに渡す形に射影可能

この要素によって、画像エンコーダより取得した視覚情報より、重要なエッセンスを抽出することが出来るようになります。

一旦以下のようにQと置きます。

```mathematica
Q = {q1, q2, ..., qQ}   with Q typically in [16, 64]
```

__2. Self-Attention Block__

1で入力されたクエリ自身がアテンション機構を通して情報交換されます。

QがSelfAttnに通されるので以下のように表現出来ます。

```mathematica
Q' = SelfAttn(Q)
```

この処理で想定される効果は次の通りです。

- Query間で冗長性除去
- 様々な視覚概念を分担
- 後段Cross-Attentionの情報要求を整形

__3. Cross-Attention Block (to Vision Encoder)__

CrossAttentionは:

- Query: 情報要求（何を見るか）
- Key/Value: 画像特徴（何が存在するか）

という役割分担になっています。

ViTに全ての情報保持を委譲し、Q-Formerが選択的に抽出する構図になります。

```mathematica
V = VisionEncoder(image)
K, V = proj(V)

Q'' = CrossAttention(Q', K, V)
```

この一連の処理により、テキストに比べて情報密度が低い画像情報より、エッセンスを抜き出します。

結果、LLMに不要な低レベルの視覚情報ではなく、画像の本質となる意味を届けることが出来るようになります。

__2と3の補足__

実際にはSelf-AttentionとCross-Attentionは交互に積層されることになります。

ですので、実際のモデルは以下の処理がされています。

```mathematica
for L in layers:
    Q = SelfAttention(Q)
    Q = CrossAttention(Q, V)
```


### 一連の処理による結果

Q-Formerが登場する前は、画像とLLMを単純に線形層（1層のニューラルネット）で繋ぐだけの手法が主流でした。しかし、それだけでは情報の整理が不十分でした。

Q-Formerは、 **「トランスフォーマーを使って、画像から言語的な意味を抽出する」** というプロセスを挟むことで、巨大なLLMを効率的、かつ賢く「目」を持たせることにつながりました。

## 実際のモデル構築
先程説明した処理フローを整理すると次の通りです。

```
image
  ↓
Vision Encoder (ViT)
  ↓    (K,V)
Q (learnable)
  ↓
Q-Former (SelfAttn + CrossAttn stack)
  ↓
Projected tokens
  ↓
Flan-T5 (Decoder)
  ↓
Caption text
```

実装コードを以下のレポジトリに保存しました。

[q-former](https://github.com/Shinichi0713/LLM-fundamental-study/tree/main/vlm/src/q_former)

使うLLMはT-Flan。vision-encoderはViTを用います。

動作にあたり説明です。

__1. 関係ライブラリのインストール__

```
!pip install transformers accelerate timm einops
```

__2. q-former__

レポジトリの"q-former.py"の内容をコーディング下さい。

__3. モデルの構築__

レポジトリの"model.py"の内容をコーディング下さい。

__4. 動作__

レポジトリの"predict.py"の内容をコーディング下さい。

※predict.pyのurlは適切なurlに変更ください。

今回はモデルの実装の一連をしました。
q-formerは未だLLMとViTに合わせたチューニングをしていないので、あまり良い出力は、現段階では得られないはずです。


## Q-Formerを採用した手法
補足情報としてQ-Formerを採用したVLMについて説明します。

Q-Formerは、2023年にSalesforceが発表した**BLIP-2**で初めて導入され、その圧倒的な効率の良さから、その後多くの有名なマルチモーダルモデル（VLM）に採用されました。

代表的なモデルとその特徴を整理して紹介します。


### 1. BLIP-2 (The Pioneer)

Q-Formerを世に知らしめた最初のモデルです。

* **特徴:** 巨大なLLM（OPTやFlan-T5）と、画像エンコーダ（CLIP ViT）をQ-Formerだけで接続しました。
* **役割:** LLMを完全に固定（フリーズ）したまま、Q-Formerだけを学習させることで、当時世界最高水準の性能を極めて低い計算コストで実現しました。

### 2. InstructBLIP

BLIP-2をさらに進化させ、「指示（命令）」に従う能力を高めたモデルです。

* **特徴:** ユーザーからの「指示テキスト」をQ-Formerのクエリと一緒に入力します。
* **役割:** 指示の内容に応じて、Q-Formerが画像から抽出する情報を動的に変えます（例：「色について教えて」と言われたら、色の情報を重点的に抽出する）。
* **用途:** 高度な画像対話、推論。

### 3. MiniGPT-4

VicunaというLLMとBLIP-2のビジョン部分を組み合わせた、初期の対話型マルチモーダルAIの代表格です。

* **特徴:** BLIP-2のQ-Formerと、強力なオープンソースLLMを統合しました。
* **用途:** 画像に基づいた詩の作成、ウェブサイトのコード生成など。

### 4. Video-LLaVA / Video-ChatGPT (動画対応モデル)

静止画だけでなく、**動画（ビデオ）**を理解するモデルにもQ-Former（またはその派生技術）が使われています。

* **特徴:** 動画はフレーム数が多いため、情報量が膨大になります。
* **役割:** Q-Formerが複数のフレームにまたがる情報を「数トークン」に凝縮することで、LLMが長い動画の文脈を処理できるようにしています。

### 5. 各種ドメイン特化モデル (医療・科学)

特定の専門分野向けにカスタマイズされたモデルでも採用例が多いです。

* **例:** **Med-BLIP**（医療画像診断補助）など。
* **理由:** 専門的な画像（レントゲン等）から、診断に必要な「言語化すべき特徴」だけを抽出するのに、Q-Formerのフィルタリング能力が非常に適しているためです。


### なぜこれほど多くのモデルで使われるのか？（採用の決め手）

多くの研究者がQ-Formerを採用する理由は、一言で言えば **「コスパと精度のバランス」** です。

| 理由 | 内容 |
| --- | --- |
| **情報の集約力** | どんなに情報量が多い画像/動画でも、固定数（32個など）のトークンに圧縮できる。 |
| **学習の軽さ** | 数十億、数千億パラメータのLLMをいじる必要がなく、Q-Former（約1.8億パラメータ）だけを訓練すれば良い。 |
| **プラグイン的性質** | お気に入りの「最強の目（ViT）」と「最強の脳（Llama 3等）」を後付けで合体させることができる。 |

## 結論

今回はVLMで効果的に視覚情報をLLMに渡す手法であるQ-Formerを扱いました。
この手法の良い点はLLMには一切手を付けず、かつ、LLMに合わせた視覚情報の抽出を出来るようになるということにあります。

また、得られる視覚情報も、単純な全結合では得られないような恣意を持ったものとなるため、より、LLMの性能を引き出すことが可能となります。

実装にあたり疑問あれば、コメント頂ければと思います。

