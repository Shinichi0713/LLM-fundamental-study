知識蒸留（Knowledge Distillation）とは、巨大で高性能なモデル（ **教師モデル / Teacher Model** ）の持つ知識を、より軽量でコンパクトなモデル（ **生徒モデル / Student Model** ）に継承させる手法のことです。

「精度は高いが計算が重い」モデルの知能を、「軽量で高速に動く」モデルに凝縮することを目的としています。

---

### 1. 知識蒸留の基本構造

通常の学習では、正解ラベル（Hard Target）のみを学習しますが、知識蒸留では教師モデルが出力する「予測の確率分布（Soft Target）」を利用します。

* **教師モデル（Teacher）** : すでに学習済みの巨大なモデル（例：BERT-Large, GPT-3）。
* **生徒モデル（Student）** : これから学習する軽量なモデル（例：DistilBERT, TinyBERT）。
* **Soft Targets** : 教師モデルが各クラスに対して出力した確率値。

---

### 2. なぜ「Soft Target」が重要なのか？

例えば、画像認識で「犬」を分類する場合、教師モデルは以下のような確率を出力します。

* 犬：90%
* 猫：9%
* 車：1%

この「猫である確率が、車である確率よりも高い」という情報は、単なる正解ラベル（犬：100%）からは得られない、**「クラス間の類似性や概念的な距離」**という重要な知識を含んでいます。生徒モデルはこの情報を手がかりにすることで、少ないパラメータ数でも効率的に学習できるのです。

---

### 3. 主な手法の種類

知識をどの段階で取り出すかによって、いくつかの手法に分類されます。

#### ① Response-Based Distillation (出力層の蒸留)

最も一般的な手法。教師モデルの最終的な予測値（ロジット）を真似るように学習します。この際、**Temperature（温度パラメータ）**という変数を使用して確率分布を滑らかにし、小さな確率の差異を強調することが多いです。

#### ② Feature-Based Distillation (中間層の蒸留)

最終結果だけでなく、モデルの「考え方」の途中経過（中間層の出力）を真似させます。生徒モデルの各層が、教師モデルの対応する層と同じような特徴量を抽出できるようにガイドします。

#### ③ Relation-Based Distillation (関係性の蒸留)

個々のデータに対する出力ではなく、「データAとデータBの似ている度合い」といった、データ間の関係性を教師モデルから継承します。

---

### 4. 知識蒸留のメリット

1. **軽量化・高速化** : 推論速度が劇的に向上し、モバイルデバイスやエッジ端末でも動作可能になります。
2. **精度の維持** : 軽量モデルを一から学習させるよりも、蒸留を用いたほうが高い精度に到達します。
3. **コスト削減** : クラウドサーバーの計算コストや消費電力を抑えることができます。

---

### 5. 代表的な活用例

* **DistilBERT / TinyBERT** : BERTの性能を維持しつつ、サイズを大幅に削減したモデル。
* **LLMの蒸留** : 巨大なLLM（Llama 3 70Bなど）が生成した高品質なデータを使い、より小さなモデル（Llama 3 8Bなど）を微調整する手法が広く使われています。

---

### まとめ

| **項目**         | **通常の学習**     | **知識蒸留**               |
| ---------------------- | ------------------------ | -------------------------------- |
| **学習目標**     | 正解ラベル (Hard Target) | 教師の予測分布 (Soft Target)     |
| **得られる知識** | 正解か不正解か           | クラス間の関係性や類似度         |
| **主な目的**     | 精度向上                 | **モデルの軽量化・高速化** |
