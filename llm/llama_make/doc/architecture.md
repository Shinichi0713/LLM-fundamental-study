Meta（旧Facebook）が開発した最新のLlama 3シリーズは、基本的な構造こそGPTと同じ「デコーダーのみのTransformer」ですが、**「より大規模なデータを効率的に、かつ高精度に処理する」**ために、GPT-3/3.5の時代には一般的ではなかった高度な工夫がいくつも取り入れられています。

主な違いを、技術的なポイントに絞って解説します。

---

## 1. GQA (Grouped-Query Attention) の採用

Llama 3において最も重要な構造的特徴の一つです。GPT-3が採用していた標準的な「Multi-Head Attention (MHA)」と、計算を極限まで削った「Multi-Query Attention (MQA)」の中間に位置する技術です。

* **仕組み:** クエリ（Query）のヘッド数はそのままに、キー（Key）と値（Value）のヘッドをグループ化して共有します。
* **メリット:** 計算精度をほとんど落とさずに、推論時の**KVキャッシュ（メモリ使用量）を大幅に削減**できます。これにより、長い文章を生成する際の速度が向上し、より大きなバッチサイズでの処理が可能になりました。

## 2. RoPE (Rotary Positional Embeddings)

GPT-3では絶対的な位置エンコーディング（単語が何番目かという情報を足し算する）を使用していましたが、Llama 3では**RoPE（回転位置埋め込み）**を採用しています。

* **仕組み:** ベクトルを回転させることで、単語間の「相対的な距離」を直接計算の中に組み込みます。
* **メリット:** モデルが学習時よりも長い文章（コンテキスト長）に対しても柔軟に対応しやすくなり、長文読解の性能が向上します。

## 3. SwiGLU 活性化関数

GPT-3で使用されていた「GELU」に代わり、Llama 3では **SwiGLU** という活性化関数が使われています。

* **特徴:** 計算量は少し増えますが、より複雑な非線形表現が可能になります。
* **効果:** 多くの研究で、ReLUやGELUよりもモデルの学習効率と最終的な精度が向上することが証明されています。

## 4. 語彙（Tokenizer）の劇的な拡張

構造そのものというよりはデータの扱い方ですが、ここが性能に大きく寄与しています。

* **Llama 3の進化:** GPT-3は約5万種類のトークン（語彙）でしたが、Llama 3は **128,256種類** という非常に大きな語彙サイズを持っています。
* **メリット:** 日本語を含む多言語の処理効率が上がり、一つのトークンで表現できる情報量が増えたため、同じ文章でもより少ない計算（トークン数）で処理できるようになりました。

---

## 5. 構造的な進化のまとめ

| **特徴**       | **GPT-3 / 3.5**        | **Llama 3**                  |
| -------------------- | ---------------------------- | ---------------------------------- |
| **Attention**  | Multi-Head Attention         | **Grouped-Query Attention**  |
| **位置情報**   | Absolute Positional Encoding | **RoPE (Rotary Embeddings)** |
| **活性化関数** | GELU                         | **SwiGLU**                   |
| **正規化**     | LayerNorm                    | **RMSNorm**(計算がより高速)  |
| **語彙数**     | 約 50,257                    | **128,256**                  |

---

### 次のステップへの提案

Llama 3の凄さは構造だけでなく、**「15兆トークン」という圧倒的な学習データ量**にもあります。

もしご興味があれば、Llama 3の特徴である **GQA（Grouped-Query Attention）** を、先ほどのPyTorchコードに組み込む書き方にアップデートしてみましょうか？
