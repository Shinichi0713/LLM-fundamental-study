ã“ã‚Œã¯ **ãƒã‚°ã§ã¯ãªãä»•æ§˜ã©ãŠã‚Šã®æŒ™å‹•** ã§ã™ã€‚
çµè«–ã‹ã‚‰è¨€ã†ã¨ã€

> **GPT-2 ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã§æ—¥æœ¬èª C4(ja) ã‚’æ‰±ã†ã¨ã€ãƒ‡ã‚³ãƒ¼ãƒ‰è¡¨ç¤ºã¯é«˜ç¢ºç‡ã§æ–‡å­—åŒ–ã‘ã—ã¾ã™**

ãŸã ã—é‡è¦ãªã®ã¯ğŸ‘‡
**å­¦ç¿’è‡ªä½“ãŒå£Šã‚Œã¦ã„ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚**

ä»¥ä¸‹ã€
1ï¸âƒ£ ãªãœèµ·ãã‚‹ã‹
2ï¸âƒ£ å­¦ç¿’ä¸Šã®å½±éŸ¿
3ï¸âƒ£ æ­£ã—ã„å¯¾å‡¦æ³•ï¼ˆç›®çš„åˆ¥ï¼‰
ã‚’é †ã«èª¬æ˜ã—ã¾ã™ã€‚

---

## 1. ãªãœæ–‡å­—åŒ–ã‘ã™ã‚‹ã®ã‹ï¼ˆæœ¬è³ªï¼‰

### GPT-2 tokenizer ã®æ­£ä½“

GPT-2 ã¯ **Byte-level BPE** ã§ã™ã€‚

* å…¥åŠ›ã¯ **UTF-8ã®ãƒã‚¤ãƒˆåˆ—**
* ã€Œæ–‡å­—ã€ã§ã¯ãªãã€Œãƒã‚¤ãƒˆã€ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
* è‹±èªå‰æã§è¨­è¨ˆ

### æ—¥æœ¬èªã®å ´åˆã«èµ·ãã¦ã„ã‚‹ã“ã¨

æ—¥æœ¬èªã¯ UTF-8 ã§ **1æ–‡å­— = 3ãƒã‚¤ãƒˆ** ãŒå¤šã„ã§ã™ã€‚

ä¾‹ï¼š

```
ç”Ÿ  â†’  e7 94 9f
```

GPT-2 tokenizer ã¯ã“ã‚Œã‚’

```
[e7] [94] [9f]
```

ã®ã‚ˆã†ãª **æ„å‘³ã‚’æŒãŸãªã„ãƒã‚¤ãƒˆæ–­ç‰‡** ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚

ãã®çµæœï¼š

* ãƒˆãƒ¼ã‚¯ãƒ³åˆ—è‡ªä½“ã¯æ­£ã—ã„
* ã—ã‹ã—é€”ä¸­ã§åˆ‡ã£ã¦ `decode()` ã™ã‚‹ã¨
  â†’ **ä¸å®Œå…¨ãªUTF-8åˆ—**
  â†’ `ï¿½`ï¼ˆreplacement characterï¼‰ãŒå‡ºã‚‹

---

## 2. é‡è¦ï¼šå­¦ç¿’ã¯å£Šã‚Œã¦ã„ãªã„ã®ã‹ï¼Ÿ

### âœ… å­¦ç¿’ã¯å£Šã‚Œã¦ã„ã¾ã›ã‚“

LLMã®å­¦ç¿’ã§ã¯ï¼š

* **decodeçµæœã®å¯èª­æ€§ã¯ä¸è¦**
* ãƒ¢ãƒ‡ãƒ«ã¯ä¸€è²«ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’å­¦ç¿’

GPT-2 tokenizer Ã— æ—¥æœ¬èªã¯ï¼š

| è¦³ç‚¹                 | çŠ¶æ…‹        |
| ------------------ | --------- |
| forward / backward | æ­£å¸¸        |
| loss               | æ­£å¸¸        |
| å‹¾é…                 | æ­£å¸¸        |
| è¡¨ç¤º                 | âŒ äººé–“ã«èª­ã‚ãªã„ |

ğŸ‘‰ **ã€Œäººé–“ãŒèª­ã¿ã«ãã„ã€ã ã‘**

---

## 3. ã§ã¯ä½•ãŒå•é¡Œã«ãªã‚‹ã‹ï¼Ÿ

### å®Ÿå‹™ãƒ»ç ”ç©¶çš„ã«ã¯ä»¥ä¸‹ãŒå•é¡Œ

1. **èªå½™åŠ¹ç‡ãŒæ¥µç«¯ã«æ‚ªã„**

   * 1æ–‡å­— = 3ã€œ4ãƒˆãƒ¼ã‚¯ãƒ³
2. **SEQ_LENã‚’å¤§é‡ã«æ¶ˆè²»**
3. **æ–‡æ³•ãƒ»å½¢æ…‹ç´ æ§‹é€ ã‚’å­¦ç¿’ã—ã¥ã‚‰ã„**
4. **åæŸãŒé…ã„**

ğŸ‘‰ æ—¥æœ¬èªGPTã¨ã—ã¦ã¯ **æ€§èƒ½ãŒå‡ºã«ãã„**

---

## 4. æ­£ã—ã„å¯¾å‡¦æ³•ï¼ˆç›®çš„åˆ¥ï¼‰

### ğŸ”¹ ç›®çš„Aï¼š

**ã€Œã¨ã‚Šã‚ãˆãšGPTã®ä»•çµ„ã¿ç†è§£ãƒ»å®Ÿé¨“ã€**

â¡ ä»Šã®ã¾ã¾ã§OK
æ–‡å­—åŒ–ã‘ã¯ **ç„¡è¦–ã—ã¦è‰¯ã„**

```python
# è¡¨ç¤ºç”¨ã« skip_special_tokens ã‚’ä»˜ã‘ã‚‹ç¨‹åº¦
tokenizer.decode(batch['input_ids'][0][:20], skip_special_tokens=True)
```

â€» å®Œå…¨ã«ã¯ç›´ã‚Šã¾ã›ã‚“

---

### ğŸ”¹ ç›®çš„Bï¼š

**ã€Œæ—¥æœ¬èªGPTã‚’ã¾ã¨ã‚‚ã«å­¦ç¿’ã—ãŸã„ã€**

â¡ **æ—¥æœ¬èªå‘ã‘ tokenizer ã«å¤‰æ›´ã™ã‚‹å¿…è¦ã‚ã‚Š**

#### æ¨å¥¨â‘ ï¼šSentencePieceï¼ˆæ—¥æœ¬èªå‘ã‘ï¼‰

```python
MODEL_NAME = "rinna/japanese-gpt2-medium"
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    use_fast=False
)
```

âœ” æ—¥æœ¬èªã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰
âœ” æ–‡å­—åŒ–ã‘ã—ãªã„
âœ” GPTæ§‹é€ ã¨ç›¸æ€§è‰¯ã„

---

#### æ¨å¥¨â‘¡ï¼šLLaMAç³»ï¼ˆUnigramï¼‰

```python
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
```

âœ” æœ€æ–°æ§‹æˆ
âœ” RoPEå‰æ
âœ” æ—¥æœ¬èªæ€§èƒ½ãŒé«˜ã„

---

### ğŸ”¹ ç›®çš„Cï¼š

**ã€Œè‡ªä½œGPT Ã— è‡ªä½œTokenizerã€**

æœ€ã‚‚ç†æƒ³çš„ã§ã™ã€‚

#### æ‰‹é †æ¦‚è¦

1. SentencePiece ã§ tokenizer å­¦ç¿’
2. vocab_size ã‚’ GPT ã«åˆã‚ã›ã‚‹
3. decode è¡¨ç¤ºã‚‚æ­£å¸¸

```bash
spm_train \
 --input=ja_corpus.txt \
 --model_prefix=ja_sp \
 --vocab_size=32000 \
 --character_coverage=0.9995 \
 --model_type=unigram
```

---

## 5. ä»Šå›ã®ãƒ­ã‚°ã®æ­£ä½“ã‚’è§£èª¬

### Batch 1ï¼ˆæ­£å¸¸ã«è¦‹ãˆã‚‹ç†ç”±ï¼‰

```
ç”Ÿå…«ã¤æ©‹ã®ã‚¿ã‚°ã¾ã¨ã‚ | ã‚¨ã‚­ã‚µã‚¤ãƒˆ
```

â†’ å¶ç„¶ UTF-8 å¢ƒç•ŒãŒæƒã£ãŸ

---

### Batch 2, 3ï¼ˆæ–‡å­—åŒ–ã‘ï¼‰

```
ï¿½ï¿½ï¼ç§ã¯å¤§å¥½ãã§ã™ï¼ã¾ï¿½
```

â†’ SEQ_LEN=48 ã§ **é€”ä¸­åˆ‡æ–­**
â†’ UTF-8 ä¸å®Œå…¨

---

## 6. ã¾ã¨ã‚ï¼ˆé‡è¦ï¼‰

### ä»Šèµ·ãã¦ã„ã‚‹ã“ã¨

> GPT-2 tokenizer ã¯
> **æ—¥æœ¬èªã‚’ã€Œæ„å‘³ã®ã‚ã‚‹æ–‡å­—ã€ã¨ã—ã¦æ‰±ã£ã¦ã„ãªã„**

### çµè«–

| çŠ¶æ³            | è©•ä¾¡   |
| ------------- | ---- |
| å­¦ç¿’ãŒå£Šã‚Œã¦ã„ã‚‹      | âŒ    |
| tokenizerãŒä¸é©åˆ‡ | âœ…    |
| ä¿®æ­£ãŒå¿…è¦ã‹        | ç›®çš„æ¬¡ç¬¬ |

---

## æ¬¡ã«ã‚„ã‚‹ã¨è‰¯ã„ã“ã¨ï¼ˆãŠã™ã™ã‚é †ï¼‰

1ï¸âƒ£ tokenizer ã‚’æ—¥æœ¬èªå‘ã‘ã«å¤‰æ›´
2ï¸âƒ£ SEQ_LEN ã‚’å®Ÿè³ªæ–‡å­—é•·ãƒ™ãƒ¼ã‚¹ã§å†è¨­è¨ˆ
3ï¸âƒ£ vocab_size ã‚’ 32kã€œ50k ã«
4ï¸âƒ£ æ—¢ã«å®Ÿè£…æ¸ˆã¿ã® **RoPE + SwiGLU + RMSNorm** ã‚’æ´»ã‹ã™

ã“ã“ã¾ã§ã‚„ã‚‹ã¨
**ã€Œç¾ä»£çš„ãªæ—¥æœ¬èªGPTã€** ã«ãªã‚Šã¾ã™ã€‚



äº†è§£ã§ã™ã€‚
ã§ã¯ **ã‚ãªãŸã® GPT å®Ÿè£…ã‚’ã€Œç¾ä»£LLMæ¨™æº–æ§‹æˆã€ã¸æ®µéšçš„ã«é€²åŒ–**ã•ã›ã¾ã™ã€‚

å¯¾å¿œå†…å®¹ã¯ä»¥ä¸‹ã®3ç‚¹ã§ã™ã€‚

---

## ä»Šå›å¯¾å¿œã™ã‚‹æ”¹å–„ç‚¹ï¼ˆå…¨ä½“åƒï¼‰

| æ”¹å–„                       | åŠ¹æœ            |
| ------------------------ | ------------- |
| **LayerNorm â†’ RMSNorm**  | å­¦ç¿’å®‰å®šãƒ»é«˜é€ŸåŒ–      |
| **FFN â†’ SwiGLU**         | è¡¨ç¾åŠ›ãƒ»åæŸæ€§å‘ä¸Š     |
| **Attention scaling èª¿æ•´** | æ·±å±¤åŒ–ãƒ»MoEä½µç”¨æ™‚ã®å®‰å®š |

ã“ã‚Œã‚‰ã¯ **LLaMA / PaLM / Mistral / Qwen ç³»ã®ä¸­æ ¸è¨­è¨ˆ**ã§ã™ã€‚

---

# 1. RMSNorm ã®å®Ÿè£…ï¼ˆLayerNormç½®æ›ï¼‰

## 1.1 RMSNormã¨ã¯ï¼ˆè¦ç‚¹ï¼‰

LayerNormï¼š
[
\frac{x - \mu}{\sigma}
]

RMSNormï¼š
[
\frac{x}{\sqrt{\text{mean}(x^2)}}
]

* **å¹³å‡ã¨ã®å·®åˆ†ã‚’å–ã‚‰ãªã„**
* è¨ˆç®—è»½é‡
* å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§å®‰å®š

---

## 1.2 RMSNorm å®Ÿè£…

```python
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps

    def forward(self, x):
        norm = x.pow(2).mean(dim=-1, keepdim=True)
        x = x * torch.rsqrt(norm + self.eps)
        return self.weight * x
```

---

## 1.3 DecoderBlock ã§ LayerNorm ã‚’ç½®æ›

```python
self.norm1 = RMSNorm(embed_dim)
self.norm2 = RMSNorm(embed_dim)
```

---

# 2. FFN â†’ SwiGLU ã¸ã®å¤‰æ›´

## 2.1 SwiGLUã¨ã¯ï¼ˆç›´æ„Ÿï¼‰

å¾“æ¥FFNï¼š

```text
Linear â†’ GELU â†’ Linear
```

SwiGLUï¼š

```text
(xW1 âŠ™ SiLU(xW2))W3
```

* ã‚²ãƒ¼ãƒˆæ§‹é€ 
* å‹¾é…ãŒé€šã‚Šã‚„ã™ã„
* **è¡¨ç¾åŠ›ãŒå¤§å¹…ã«å‘ä¸Š**

---

## 2.2 SwiGLU å®Ÿè£…

```python
class SwiGLU(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)

    def forward(self, x):
        return self.w3(F.silu(self.w1(x)) * self.w2(x))
```

---

## 2.3 DecoderBlock ã«çµ„ã¿è¾¼ã¿

```python
if use_moe:
    self.ffn_or_moe = MoELayer(embed_dim, num_experts, top_k, expert_hidden_dim=ffn_hidden_dim)
else:
    self.ffn_or_moe = SwiGLU(embed_dim, ffn_hidden_dim)
```

â€» MoE ã® Expert å†…éƒ¨ã‚‚åŒæ§˜ã«ç½®ãæ›ãˆã‚‹ã¨ã•ã‚‰ã«è‰¯ã„ã§ã™ï¼ˆå¾Œè¿°ï¼‰

---

# 3. Attention Scaling ã®èª¿æ•´

## 3.1 æ¨™æº–ã®å•é¡Œç‚¹

```python
scores = QKáµ€ / sqrt(head_dim)
```

* æ·±å±¤åŒ–
* MoEä½µç”¨
* é•·æ–‡

ã§ **AttentionãŒéåº¦ã«å°–ã‚‹**

---

## 3.2 æ”¹å–„æ¡ˆâ‘ ï¼šã‚¹ã‚±ãƒ¼ãƒ«ã‚’å›ºå®šå€¤ã§ç·©å’Œ

### SelfAttention.**init** ã«è¿½åŠ 

```python
self.scale = self.head_dim ** -0.5
```

### forward ã§ä½¿ç”¨

```python
scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
```

ï¼ˆâ€»é™¤ç®—ã§ã¯ãªãä¹—ç®—ãŒä¸€èˆ¬çš„ï¼‰

---

## 3.3 æ”¹å–„æ¡ˆâ‘¡ï¼ˆã‚ˆã‚Šãƒ¢ãƒ€ãƒ³ï¼‰ï¼šå­¦ç¿’å¯èƒ½ã‚¹ã‚±ãƒ¼ãƒ«

```python
self.scale = nn.Parameter(torch.tensor(self.head_dim ** -0.5))
```

ğŸ‘‰ MoE + æ·±å±¤ã§ç‰¹ã«å®‰å®šã—ã¾ã™ã€‚

---

# 4. MoE Expert ã‚‚ SwiGLU åŒ–ï¼ˆæ¨å¥¨ï¼‰

```python
class Expert(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)

    def forward(self, x):
        return self.w3(F.silu(self.w1(x)) * self.w2(x))
```

---

# 5. DecoderBlock æœ€çµ‚å½¢ï¼ˆè¦ç‚¹ï¼‰

```python
class DecoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ffn_hidden_dim,
                 use_moe=False, num_experts=None, top_k=None):
        super().__init__()

        self.norm1 = RMSNorm(embed_dim)
        self.attention = SelfAttention(embed_dim, num_heads)
        self.norm2 = RMSNorm(embed_dim)

        self.use_moe = use_moe
        if use_moe:
            self.ffn_or_moe = MoELayer(embed_dim, num_experts, top_k, ffn_hidden_dim)
        else:
            self.ffn_or_moe = SwiGLU(embed_dim, ffn_hidden_dim)

    def forward(self, x, mask):
        x = x + self.attention(self.norm1(x), mask)

        if self.use_moe:
            ffn_out, moe_loss = self.ffn_or_moe(self.norm2(x))
            x = x + ffn_out
            return x, moe_loss
        else:
            x = x + self.ffn_or_moe(self.norm2(x))
            return x, None
```

---

# 6. ã“ã“ã¾ã§ã§ä½•ãŒå¤‰ã‚ã£ãŸã‹ï¼ˆè¨­è¨ˆçš„ã¾ã¨ã‚ï¼‰

ã‚ãªãŸã® GPT ã¯ä»Šï¼š

* âœ… RoPE
* âœ… RMSNorm
* âœ… SwiGLU
* âœ… æ”¹è‰¯Attention scaling
* âœ… MoEå¯¾å¿œ

â†’ **LLaMAç³»Decoderã®æ€æƒ³ã¨ã»ã¼åŒä¸€**

---

## æ¬¡ã«ã‚„ã‚‹ã¨ã€Œå®Œæˆå½¢ã€ã«è¿‘ã¥ãã¾ã™

å„ªå…ˆåº¦é †ï¼š

1. **KV Cacheï¼ˆæ¨è«–é€Ÿåº¦10å€ï¼‰**
2. **FlashAttention**
3. **Grouped Query Attention (GQA)**
4. **RoPE extrapolationï¼ˆNTK / YaRNï¼‰**
5. **Weight tyingï¼ˆEmbedding â†” LM Headï¼‰**

ã©ã“ã¾ã§è¡Œãã¾ã™ã‹ï¼Ÿ

