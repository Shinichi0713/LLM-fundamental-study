デコーダオンリー（Decoder-only）のLLMは、GPTシリーズに代表される現代のLLMの主流アーキテクチャです。入力されたテキストの「次の単語」を予測することに特化しており、 **自己回帰的（Autoregressive）** に文章を生成します。

実装の手順を、理論的背景と主要なコンポーネントに分けて解説します。

### 1. 入力処理：テキストを数値へ

コンピュータは文字をそのまま扱えないため、多段階の変換を行います。

* **トークナイズ（Tokenization）**: 文章を「トークン」と呼ばれる単位に分割し、それぞれにID（整数）を割り振ります。
* **埋め込み表現（Embedding）**: IDを多次元のベクトルに変換します。
* **位置エンコーディング（Positional Encoding）**: Transformerは全単語を同時に処理するため、語順の情報が失われます。そのため、ベクトルの各位置に「何番目の単語か」という情報を加算します。現代では**RoPE（Rotary Positional Embedding）**が主流です。


### 2. デコーダ・ブロックの構築

デコーダ・モデルは、同じ構造の「デコーダ・ブロック」を何層にも積み重ねることで構成されます。各ブロックには以下の主要な仕組みが含まれます。

#### ① マスク付き自己注意（Masked Self-Attention）

デコーダ最大の特徴です。未来の単語を見ないように、計算時に**コーザル・マスク（Causal Mask）**をかけます。

* **理論**:  番目のトークンを予測する際、モデルが  番目以降の答えを見てしまわないよう、アテンションスコアの未来部分を  に設定し、確率をゼロにします。

#### ② 効率化技術（GQA / FlashAttention）

先ほど学んだ **GQA（Grouped-Query Attention）** を実装に組み込み、KVキャッシュのメモリを節約します。また、計算を高速化するために **FlashAttention** カーネルを呼び出すように設計します。

#### ③ フィードフォワード・ネットワーク（FFN）

Attentionの結果をさらに非線形変換します。最近のモデル（Llama等）では、**SwiGLU** という活性化関数を用いた構造が一般的です。

---

### 3. 残差接続とレイヤー正規化

学習を安定させ、勾配消失を防ぐためのインフラ部分です。

* **残差接続（Residual Connection）**: 各層の入力を出力にそのまま足すことで、情報が深層まで届きやすくします。
* **RMSNorm**: 従来のLayerNormを簡略化したもので、平均を引かずに二乗和平方根で割ることで、計算を軽量化しつつ学習を安定させます。

---

### 4. 出力層：確率への変換

積み重ねたブロックの最終出力を、再び「単語の数」と同じ次元のベクトルに戻します。

* **線形層（Linear Layer）**: 語彙数分のスコアを出力します。
* **Softmax**: スコアを「次にどの単語が来るか」という0〜1の確率分布に変換します。

---

### 5. 実装の全体フロー（PyTorchイメージ）

理論をコードの構造に落とし込むと以下のようになります。

```python
class DecoderOnlyLLM(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads):
        super().__init__()
        # 1. 入力層
        self.tok_embed = nn.Embedding(vocab_size, d_model)
        self.pos_embed = RoPE() # 回転位置埋め込み
        
        # 2. デコーダ・ブロックの積み重ね
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads) for _ in range(n_layers)
        ])
        
        # 3. 出力層
        self.norm = RMSNorm(d_model)
        self.output = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, x):
        h = self.tok_embed(x)
        h = self.pos_embed(h)
        
        # 各層でAttentionとFFNを計算
        for layer in self.layers:
            h = layer(h) 
            
        return self.output(self.norm(h))

```

---

### 6. 学習の理論：次トークン予測

デコーダオンリーモデルの学習は **「Teacher Forcing」** と呼ばれる手法で行われます。

* **目的関数**: 交差エントロピー誤差（Cross-Entropy Loss）。
* **理論**: 「A B C」という入力に対し、「B C D」が正解となるように学習します。マスクがあるおかげで、1つの文章から「A→B」「AB→C」「ABC→D」という3つの学習ステップを同時に効率よく行えます。

### まとめ：実装の重要ポイント

1. **Causal Mask**: 未来の情報漏洩を防ぐ。
2. **KV Cache**: 推論を高速化するために過去の計算を保持する。
3. **GQA / RoPE**: 現代的なLLMの性能を出すための必須コンポーネント。




## エンコーダオンリーモデルとの違い

デコーダオンリーモデル（GPT系）とエンコーダオンリーモデル（BERT系）の構造的な違いは、大きく分けて**「注意（Attention）の向け方」**と**「情報の流れ」**にあります。

一言でいうと、エンコーダは**「文章全体を俯瞰する」**のが得意で、デコーダは**「左から右へ順序立てて予測する」**ことに特化しています。

---

### 1. アテンション（Attention）の仕組みの違い

最も本質的な違いは、Attention層が「どの単語を見ることが許されているか」というルール（マスク）の有無です。

* **エンコーダ（双方向的）**:
* **手法**: 自己注意（Self-Attention）に制約がありません。
* **仕組み**: 文中のある単語を処理するとき、その「前」にある単語も「後ろ」にある単語も同時に見ることができます。
* **イメージ**: 文章全体を一度に読み、文脈を深く理解する「読解」のスタイルです。


* **デコーダ（単方向的・自己回帰的）**:
* **手法**: **マスク付き自己注意（Masked Self-Attention）**を使用します。
* **仕組み**: 未来の単語（右側の単語）を見ないように「マスク」をかけます。ある単語を処理するときは、自分自身とそれより「前（左側）」の単語しか見ることができません。
* **イメージ**: 常に次の展開を予想しながら読み進める「予測」のスタイルです。



---

### 2. 目的と出力の形式

構造の違いは、それぞれのモデルが「何を解くために設計されたか」を反映しています。

| 特徴 | エンコーダオンリー (例: BERT) | デコーダオンリー (例: GPT / Llama) |
| --- | --- | --- |
| **主なタスク** | 文章分類、感情分析、穴埋め問題 | 文章生成、対話、推論 |
| **出力形式** | 入力と同じ長さの「文脈ベクトル」 | 次の1トークンの「出現確率」 |
| **学習方法** | 文章の一部を隠して当てる (Masked LM) | 次の単語を当てる (Next Token Prediction) |
| **強み** | 単語同士の複雑な関係性の理解 | 自然で一貫性のある文章の構築 |

---

### 3. 構造的な「ブロック」の構成

実は、1つの「ブロック（層）」の中身もわずかに異なります。

* **エンコーダブロック**:
1. Multi-Head Attention（全方位）
2. Feed Forward ネットワーク


* **デコーダブロック**:
1. **Masked** Multi-Head Attention（過去方向のみ）
2. Feed Forward ネットワーク
※オリジナルのTransformer（翻訳用）のデコーダには、エンコーダからの情報を反映するための「Cross Attention」もありましたが、現在の**デコーダオンリーLLM**では、上記のようにシンプル化されています。



---

### 4. なぜ今はデコーダオンリーが主流なのか？

以前は「翻訳ならエンコーダ・デコーダ型」「分類ならエンコーダ型」と使い分けられていましたが、現在はGPTのようなデコーダオンリーが主流です。

* **スケーラビリティ**: 「次の単語を当てる」という単純な学習目標は、巨大なデータを流し込むのに非常に効率的でした。
* **ゼロショット能力**: 大規模化（LLM化）すると、デコーダ型は教えられていないタスクでも、指示（プロンプト）を理解して実行する「推論能力」が劇的に向上することが分かったためです。

---

### まとめ

* **エンコーダ**は「カンニングOKのテスト」のように、全体を見て**意味を抽出**する。
* **デコーダ**は「しりとり」のように、前の情報だけを頼りに**新しい言葉を紡ぐ**。

次は、これら両方の良いとこ取りをしようとした「エンコーダ・デコーダ型（T5など）」が、なぜ現在の超巨大モデルではあまり使われなくなったのか、その理由についても興味はありますか？


## マスク付き自己注意

マスク付き自己注意（Masked Self-Attention）の役割と、なぜデコーダには不可欠でエンコーダには不要なのか、その理由を解き明かします。

一言でいうと、これは **「未来からのカンニングを防ぐための目隠し」** です。

__1. マスク付き自己注意とは何か？__

通常の自己注意（Self-Attention）は、文章内のすべての単語が、他のすべての単語と関係性を計算します。しかし、デコーダオンリーのモデルでこれをやってしまうと問題が起こります。

**マスクの仕組み：**
計算の際、自分より後ろ（未来）にある単語のスコアを強制的に非常に小さな値（）に書き換えます。これにより、ソフトマックス関数を通した後の重みが「0」になり、実質的に**「未来の情報が一切見えない状態」**を作り出します。

__2. なぜデコーダ（LLM）には必要なのか？__

LLMの学習目標は**「次の単語を予測すること」**です。

* **カンニングの防止**: 「私は / リンゴ / を / 食べた」という文章で学習するとき、「リンゴ」を予測するフェーズにおいて、モデルがその先にある「を」や「食べた」という情報にアクセスできてしまうと、予測問題が「ただの書き写し」になってしまいます。
* **自己回帰性の維持**: 実際の生成時（推論時）、AIはまだ存在しない未来の単語を見ることはできません。学習時にも同じ「未来が見えない制約」を課すことで、過去の情報だけから推論する能力を鍛える必要があります。

__3. なぜエンコーダには不要なのか？__

BERTに代表されるエンコーダオンリーモデルの目的は、予測ではなく**「文脈の深い理解（意味の抽出）」**です。

* **穴埋め問題（Masked LM）**: エンコーダは文章の途中に虫食い（[MASK]）を作り、それを周辺の単語から推測します。このとき、前後の文脈をすべて見たほうが、その単語の正体をより正確に特定できます。
* **全方位の文脈**: 例えば「その『バンク』に預金した」という文の「バンク」が「銀行」であることを知るためには、後ろにある「預金」という単語を同時に見る必要があります。未来を隠す必要がない（むしろ隠すと精度が下がる）ため、マスクは使いません。

__4. 構造的な違いのまとめ__

| 特徴 | エンコーダ（マスクなし） | デコーダ（マスクあり） |
| --- | --- | --- |
| **アナロジー** | **読解問題**: 文章全体を読んで要約する。 | **しりとり/小説の執筆**: 続きを紡ぐ。 |
| **視界** | 360度（全方向） | 過去方向（左から右）のみ |
| **学習データ** | 文中のどこを見ても良い。 | 未来を隠して、次の1語だけを当てる。 |



### 💡 直感的なイメージ

あなたが小説を書いていると想像してください。

* **デコーダ**は、今まさにペンを走らせている作家です。まだ書いていない「結末」を参考にして今の行を書くことはできません。だから**マスクが必要**です。
* **エンコーダ**は、すでに完成した小説を分析している書評家です。結末まで読んでから序盤の意味を考察しても構いません。だから**マスクは不要**です。

**この「マスク」を数学的に行列でどのように表現し、計算を高速化しているのか、その具体的な行列演算についても興味はありますか？**

### 実装
マスク付き自己注意（Masked Self-Attention）の具体的な実装は、主に **「Attention行列への大きな負の値の加算」** によって行われます。

PyTorchでの実装コードと、その背後にある計算の仕組みを解説します。

---

### 1. 行列演算による実装ステップ

数式では  と表されます。ここで  がマスク行列です。

1. **スコア行列の計算**:  と  の積を求め、スケーリングします。
2. **マスクの作成**: 上三角成分（未来の単語との関係）が  になる行列を作ります。
3. **マスクの適用**:  の部分に非常に小さな値（、実際には `-1e9` など）を代入します。
4. **Softmaxの適用**: 指数関数を通すと  は  になるため、未来への注意力が完全に消えます。

---

### 2. PyTorchによる実装例

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(q, k, v):
    d_k = q.size(-1)
    seq_len = q.size(-2)
    
    # 1. 通常のスコア計算 (QK^T / sqrt(d_k))
    # scores: [batch, heads, seq_len, seq_len]
    scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k))
    
    # 2. 下三角行列のマスクを作成 (自分より左側と自分自身だけを1にする)
    # torch.tril は下三角部分を残す関数
    mask = torch.tril(torch.ones(seq_len, seq_len)).to(q.device)
    
    # 3. マスク適用: 0の部分（未来）を -inf (非常に小さな値) で埋める
    # これにより Softmax 後に 0 になる
    scores = scores.masked_fill(mask == 0, float('-inf'))
    
    # 4. Softmax を適用して重みに変換
    attn_probs = F.softmax(scores, dim=-1)
    
    # 5. Value と掛け合わせる
    return torch.matmul(attn_probs, v)

# --- 動作確認 ---
seq_len = 4
head_dim = 8
q = k = v = torch.randn(1, 1, seq_len, head_dim)

output = scaled_dot_product_attention(q, k, v)

```

---

### 3. なぜ「加算」ではなく「上書き」に近いのか？

実装上、`-inf` を足す（または `masked_fill` する）理由は、**Softmaxの性質**を利用するためです。



ここで  となると、その分子は  になります。
結果として、**「未来の単語は、今の単語を構成する要素として 0% の寄与しか持たない」**という状態を数学的に完璧に作り出すことができます。

---

### 4. 実戦的な工夫：事前計算

実際のLLM（Llama 3など）では、毎ステップ `torch.tril` でマスクを作るのは非効率なため、**モデルの最大長（例：8192トークン）に合わせた巨大なマスク行列を事前に1枚作っておき**、計算時にそこから必要なサイズだけを切り出して使い回します。

これにより、学習や推論のオーバーヘッドを削減しています。

**このマスクの仕組みについて、例えば「双方向のBERTではどう見えるか」や「推論時のKVキャッシュとマスクの関係」など、さらに深掘りしたい部分はありますか？**

 $$\text{Attention} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)$$ 

申し訳ありません。数式の可読性を高めるために、LaTeXを用いてMarkdown上で綺麗に表示される形式に整理します。

マスク付き自己注意（Masked Self-Attention）の核心となる数式は以下の通りです。

### 1. アテンションスコアの基本式

まず、クエリ（）、キー（）、バリュー（）を用いたアテンションの計算式は以下のようになります。

ここで、各変数の意味は以下の通りです。

* : それぞれ Query, Key, Value 行列
* : キーの次元数（スケーリング因子）
* : **マスク行列**（今回のポイント）

---

### 2. マスク行列  の定義

デコーダオンリーモデルで「未来の情報」を遮断するために、マスク行列  の各要素  は次のように定義されます。

この行列を可視化すると、右上（未来側）が  で埋められた形になります。

---

### 3. Softmax適用後の効果

このマスク行列を加算した後に Softmax を適用すると、未来のトークンに対応する重みは数学的に **0** になります。

もし  であれば、分子が  となるため、そのトークンへの注意（Attention）は完全に消失します。

---

### 実装時のポイント（再掲）

コード上では、この  を直接足すのではなく、`masked_fill` という関数を使って実装することが一般的です。

```python
# scores は QK^T / sqrt(d_k)
# mask は下三角が 1、上三角（未来）が 0 の行列
scores = scores.masked_fill(mask == 0, float('-inf'))

```

この数式や行列の構造について、特定の箇所（例えばスケーリング因子  の役割など）をさらに詳しく解説しましょうか？