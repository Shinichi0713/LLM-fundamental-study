# 「 **絶対位置のPositional Encoding (PE)** 」と「 **RoPE (Rotary Position Embedding)** 」の違い

「 **絶対位置のPositional Encoding (PE)** 」と「 **RoPE (Rotary Position Embedding)** 」の違いは、

**どのように「位置情報」をトークン表現に埋め込むか**にあります。

以下で、

✅ 数式

✅ 直感的なイメージ

✅ 実用上の違い

の3つの観点から比較していきます。

---

## 🔹 1. 背景：なぜPositional Encodingが必要か

Transformerは**順序情報を持たない**モデルです。

入力トークンが並列処理されるため、単語の「順序」をそのままは理解できません。

👉 そこで「トークンの位置」をベクトルに埋め込む必要があります。

これが **Positional Encoding (PE)** です。

---

## 🔸 2. 絶対位置型 Positional Encoding

### 🔹 数式（Vaswani et al., 2017）

[

PE_{(pos, 2i)} = \sin \left( \frac{pos}{10000^{2i/d}} \right), \quad

PE_{(pos, 2i+1)} = \cos \left( \frac{pos}{10000^{2i/d}} \right)

]

* `pos`: トークンの位置（0,1,2,…）
* `i`: 次元方向のインデックス
* `d`: 埋め込み次元数（例: 512）

つまり、「位置 pos ごとに固定のサイン・コサイン波」を生成します。

---

### 🔹 イメージ

* 各次元が「異なる波長の正弦波」として位置を表す
* 絶対的な座標のように「この単語は文の3番目」と直接教えるイメージ

📊 各単語の埋め込みに対して：

[

x_{\text{input}} = x_{\text{word}} + PE_{\text{pos}}

]

---

### 🔹 特徴

| 特徴                    | 内容                             |
| ----------------------- | -------------------------------- |
| ✅ シンプル             | 数式で簡単に計算できる           |
| ✅ 絶対位置がわかる     | 「何番目の単語か」を明示的に表現 |
| ❌ 相対関係が扱いにくい | 「AとBの距離」に敏感でない       |
| ❌ 文の長さに制約       | 固定のmax lengthでしか使えない   |

---

## 🔸 3. Rotary Position Embedding (RoPE)

RoPE（Su et al., 2021）は、

位置を「ベクトルの回転」として表現する方法です。

---

### 🔹 数式

入力ベクトル (x = [x_1, x_2, x_3, x_4, \dots]) の偶数次元を

2次元のペアごとに見ます。

各位置 (p) に対して：

# [

\begin{bmatrix}

x'_1 \

x'_2

\end{bmatrix}

\begin{bmatrix}

\cos \theta_p & -\sin \theta_p \

\sin \theta_p & \cos \theta_p

\end{bmatrix}

\begin{bmatrix}

x_1 \

x_2

\end{bmatrix}

]

ここで：

[

\theta_p = p \cdot \omega_i, \quad

\omega_i = 10000^{-2i/d}

]

つまり、各次元ごとに異なる角速度で「回転」します。

---

### 🔹 イメージ

* 単語ベクトルを、**位置ごとに異なる角度で回転させる**
* 回転の差（角度差）＝「相対的な位置関係」を自然に表す

---

### 🔹 相対位置を自然に表現できる理由

もし2つの位置 (p) と (q) があるとき：

[

x_p^\top x_q = x^\top R(\theta_{p-q})

]

この式により、**相対距離 (p−q)** が直接表現されることがわかります。

つまり：

> RoPEは「位置差（相対関係）」がAttentionスコアに自然に反映される。

---

## 🔹 4. 絶対位置PEとRoPEの比較表

| 観点                        | 絶対位置PE               | RoPE                            |
| --------------------------- | ------------------------ | ------------------------------- |
| **位置表現の方法**    | サイン波の加算           | ベクトルの回転                  |
| **表す位置情報**      | 絶対位置                 | 相対位置（p−q）                |
| **Attentionとの統合** | ベクトルに加算する       | Query/Keyベクトルに直接埋め込む |
| **長文対応**          | 限定的（固定max length） | 拡張しやすい（スケーラブル）    |
| **理論的特徴**        | 加算的                   | 幾何学的（回転演算）            |
| **直感**              | 「座標の足し算」         | 「位相の回転」                  |

---

## 🔸 5. 可視化イメージ（簡略）

### 絶対位置PE

```
位置:  0   1   2   3   4
角度:  0° 20° 40° 60° 80°
ベクトル: ➡ ↗ ↑ ↖ ⬅
```

→ 単語の位置ごとにベクトルが一定方向を示す

---

### RoPE

```
位置:  0   1   2   3   4
角度:  0° 20° 40° 60° 80°
でも「回転」はQuery/Keyの関係で使う
```

→ Attentionで「回転角の差」だけが効く

---

## 🔹 6. まとめ

| 項目              | 絶対位置PE       | RoPE                                               |
| ----------------- | ---------------- | -------------------------------------------------- |
| 位置情報          | 絶対位置         | 相対位置                                           |
| Attentionへの影響 | 位置の加算で変化 | 回転の差で自然に反映                               |
| 汎用性            | 位置固定         | 長文・連続位置に強い                               |
| 主な用途          | 初期BERTなど     | GPT-NeoX / LLaMA / ModernBERT / Transformer-XLなど |

---

💡 **要するに：**

> 絶対PEは「文中の何番目か」を明示的に教える。
>
> RoPEは「単語間の距離」そのものをAttention計算に自然に組み込む。

---

ご希望があれば、

「絶対位置PE vs RoPE の空間的変化の可視化グラフ（ベクトル回転図）」

を生成して違いを視覚的に比較できます。

作ってみましょうか？
