ä»¥ä¸‹ã§ã¯ **QLoRAï¼ˆ4bité‡å­åŒ–ï¼‹LoRA ã§çœãƒ¡ãƒ¢ãƒªå¾®èª¿æ•´ï¼‰ã‚’å®Ÿéš›ã«å‹•ã‹ã™ãŸã‚ã®ã€æœ€å°é™ã§å‹•ãã‚µãƒ³ãƒ—ãƒ«æ§‹æˆ** ã‚’ã€

**PyTorch + HuggingFace Transformers + PEFT** ã‚’ä½¿ã£ã¦ã¾ã¨ã‚ã¦èª¬æ˜ã—ã¾ã™ã€‚

---

# âœ… QLoRAã¨ã¯ï¼ˆè¶…è¦ç‚¹ï¼‰

QLoRA =

1. **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’4bité‡å­åŒ–ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰**
2. **LoRA ã®è¿½åŠ å±¤ã ã‘ã‚’å­¦ç¿’ï¼ˆåŠ¹ç‡çš„ï¼‰**

ã“ã‚Œã«ã‚ˆã‚Šã€**70B ã‚¯ãƒ©ã‚¹ã§ã‚‚ A100 1 æšï¼ˆã¾ãŸã¯ã‚ˆã‚Šå¼±ã„GPUï¼‰ã§å¾®èª¿æ•´ãŒå¯èƒ½**ã«ãªã‚Šã¾ã™ã€‚

---

# âœ… QLoRA å®Ÿè£…ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```bash
pip install transformers accelerate bitsandbytes peft datasets
```

* **bitsandbytes** â†’ 4bit é‡å­åŒ–ã‚’æä¾›
* **peft** â†’ LoRA / QLoRA ã®ç°¡å˜ãªå®Ÿè£…
* **transformers** â†’ LLM å–ã‚Šæ‰±ã„

---

# âœ… QLoRA å®Ÿè£…ã®ãƒ•ãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆãã®ã¾ã¾ã‚³ãƒ”ãƒšã§å‹•ä½œï¼‰

ä»¥ä¸‹ã¯ã€LLaMAãƒ»Mistralãƒ»Gemmaãƒ»Mixtral ãªã©ã©ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å‹•ã **æœ€å°æ§‹æˆ** ã§ã™ã€‚

```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import torch

# ------------------------------
# 1. 4bité‡å­åŒ–ã®è¨­å®š
# ------------------------------
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,             # 4bit é‡å­åŒ–
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",     # é€šå¸¸ã¯ "nf4"
    bnb_4bit_compute_dtype=torch.bfloat16,  # è¨ˆç®—ç²¾åº¦
)

# ------------------------------
# 2. ãƒ¢ãƒ‡ãƒ«ã‚’4bité‡å­åŒ–ã—ã¦èª­ã¿è¾¼ã¿
# ------------------------------
model_name = "meta-llama/Llama-3-8b-instruct"  # â† ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´OK

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",             # GPUã¸è‡ªå‹•é…ç½®
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# ------------------------------
# 3. LoRA ã®è¨­å®šï¼ˆé‡è¦ï¼‰
# ------------------------------
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)

# ------------------------------
# 4. ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
# ------------------------------
dataset = load_dataset("ybelkada/ultrachat_200k", split="train[:2000]")

def format_example(example):
    prompt = example["prompt"]
    answer = example["answer"]
    text = f"<s>Instruction: {prompt}\nAnswer: {answer}</s>"
    tokenized = tokenizer(text, truncation=True, padding="max_length", max_length=512)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

dataset = dataset.map(format_example)

# ------------------------------
# 5. å­¦ç¿’è¨­å®š
# ------------------------------
training_args = TrainingArguments(
    output_dir="qlora-output",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=1,
    fp16=False,
    bf16=True,
    optim="paged_adamw_8bit",  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
    logging_steps=10,
    save_steps=200,
    ddp_find_unused_parameters=False,
)

# ------------------------------
# 6. Trainer ã§å­¦ç¿’é–‹å§‹
# ------------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()

# ------------------------------
# 7. LoRA ã®ã¿ã‚’ä¿å­˜
# ------------------------------
model.save_pretrained("qlora-lora-adapter")
```

---

# âœ… è§£èª¬ï¼šQLoRA ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆã ã‘ã‚’åˆ†ã‹ã‚Šã‚„ã™ãæ•´ç†

---

## 1. 4bité‡å­åŒ–ã§ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›

```python
bnb_config = BitsAndBytesConfig(load_in_4bit=True)
```

ã“ã‚Œã«ã‚ˆã‚Šã€

8B ãƒ¢ãƒ‡ãƒ« â†’ ç´„ 16GB â†’ **4ã€œ6GB**ã«ç¸®ã‚€ã“ã¨ã‚‚å¯èƒ½ã€‚

---

## 2. LoRA è¿½åŠ éƒ¨åˆ†ã ã‘å­¦ç¿’

QLoRA ã®è‚ã¯ **å…ƒãƒ¢ãƒ‡ãƒ«ï¼ˆ4bitï¼‰ã¯å‡çµ**ã—ã¦ **LoRA ã® MLP ã ã‘å­¦ç¿’**ã™ã‚‹ã“ã¨ã€‚

ãã®è¨­å®šãŒã“ã‚Œï¼š

```python
lora_config = LoraConfig(r=16, lora_alpha=32)
```

r=16 ã¯ã€Œä½ãƒ©ãƒ³ã‚¯æ¬¡å…ƒã€

åŸºæœ¬ã¯ 8ã€œ64 ã®é–“ã§èª¿æ•´ã™ã‚‹ã€‚

---

## 3. optimizer ãŒç‰¹æ®Š

QLoRA ã®æ¨å¥¨ optimizerï¼š

```python
optim="paged_adamw_8bit"
```

4bité‡å­åŒ–ã¨ç›¸æ€§ãŒè‰¯ãã€GPUãƒ¡ãƒ¢ãƒªã‚’å¤§å¹…ã«ç¯€ç´„ã§ãã¾ã™ã€‚

---

# âœ… ã‚ˆãã‚ã‚‹è³ªå•ï¼ˆç°¡æ˜“ã¾ã¨ã‚ï¼‰

---

### â“ ã©ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚ QLoRA å¯èƒ½ï¼Ÿ

åŸºæœ¬çš„ã«:

* LLaMAã€Mistralã€Mixtral
* Qwen
* Gemma
* GPT-Jã€GPT-NeoX ç³»
* Falcon

ãªã©ã»ã¼å…¨ã¦ã® **Decoder-only ãƒ¢ãƒ‡ãƒ«**ã§å¯èƒ½ã€‚

Encoder-only (BERT) ã‚‚ã§ãã‚‹ãŒç”¨é€”ãŒç•°ãªã‚‹ã€‚

---

### â“ LoRA ã®å­¦ç¿’éƒ¨åˆ†ã¯ã©ã“ï¼Ÿ

Trainer ãŒè‡ªå‹•çš„ã«

ã€ŒLoRA ã®é‡ã¿ã ã‘ optimizer ã«æ¸¡ã™ã€

ãŸã‚ã€ç‰¹åˆ¥ãªå‡¦ç†ã¯ä¸è¦ã€‚

---

### â“ å®Œæˆãƒ¢ãƒ‡ãƒ«ã¯ã©ã†ã‚„ã£ã¦æ¨è«–ã™ã‚‹ã®ï¼Ÿ

ä¾‹ï¼š

```python
from peft import PeftModel

model = AutoModelForCausalLM.from_pretrained(
    model_name, quantization_config=bnb_config, device_map="auto"
)
model = PeftModel.from_pretrained(model, "qlora-lora-adapter")

text = "Explain QLoRA simple."
inputs = tokenizer(text, return_tensors="pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
```

---

# âœ… ã•ã‚‰ã«æ€§èƒ½ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

| é …ç›®        | æ¨å¥¨å€¤             | èª¬æ˜               |
| ----------- | ------------------ | ------------------ |
| LoRA r      | 16ã€œ64             | å¤§ãã„ã»ã©è¡¨ç¾åŠ›UP |
| LoRA Î±     | r ã® 2ã€œ4å€        | LoRA ã®ã‚¹ã‚±ãƒ¼ãƒ«    |
| lr (å­¦ç¿’ç‡) | 1e-4ã€œ3e-4         | å¤§ãã‚ã§è‰¯ã„       |
| é‡å­åŒ–      | nf4 + double_quant | ã»ã¼ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆ     |

---

# ğŸ”¥ å¿…è¦ãªã‚‰ã€ã‚ãªãŸã® GPU / ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦ æœ€é©ãª QLoRA æ§‹æˆ ã‚’ä½œæˆã§ãã¾ã™

ä»¥ä¸‹ã‚’æ•™ãˆã¦ãã ã•ã„ï¼š

* ä½¿ç”¨ GPU æ•° / VRAM
* å¾®èª¿æ•´ã—ãŸã„ãƒ¢ãƒ‡ãƒ«åï¼ˆLlama3ãƒ»Mixtralãƒ»Gemmaãªã©ï¼‰
* å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆè¡Œæ•°ï¼‰
* ç›®çš„ï¼ˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€åˆ†é¡å™¨ã€è¦ç´„ãªã©ï¼‰

â†’ æœ€é©ãª batch size / LoRA è¨­å®š / è¨ˆç®—è³‡æºèª¿æ•´ ã‚’ä½œæˆã—ã¾ã™ã€‚
