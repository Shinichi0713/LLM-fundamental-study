# å…·ä½“çš„ãªå®Ÿè£…ä¾‹

ä»¥ä¸‹ã¯ **QLoRAï¼ˆ4-bitï¼‰ + LoRA + è’¸ç•™ï¼ˆLwFï¼‰ + Rehearsalï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿æ··åˆï¼‰** ã‚’çµ„ã¿åˆã‚ã›ãŸ **å®Ÿè¡Œå¯èƒ½ãª PyTorch + Hugging Face Transformers å®Ÿè£…ä¾‹**ã§ã™ã€‚

ç›®çš„ã¯ã€Œ **ä½ãƒ¡ãƒ¢ãƒªï¼ˆ4-bitï¼‰ã§LoRAã®ã¿å­¦ç¿’ã—ã€ã‹ã¤å…ƒãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’ç¶­æŒã™ã‚‹ï¼ˆè’¸ç•™ï¼‰** ã€ã“ã¨ã§ã™ã€‚

å®Ÿå‹™çš„ãªè¨­å®šï¼ˆgradient accumulationã€mixed precisionã€checkpointing ãªã©ï¼‰ã‚‚çµ„ã¿è¾¼ã‚“ã§ã‚ã‚Šã¾ã™ã€‚

---

## äº‹å‰æº–å‚™ï¼ˆå¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰

```bash
pip install -U transformers accelerate datasets peft bitsandbytes sentencepiece
# ï¼ˆç’°å¢ƒã«ã‚ˆã£ã¦ã¯ torch ã‚‚å¿…è¦ï¼‰
```

> **æ³¨æ„** : bitsandbytes (bnb) ã¯ CUDA å¯¾å¿œã®ç’°å¢ƒã§å‹•ãã¾ã™ã€‚Google Colab / GPU ã‚µãƒ¼ãƒç­‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚CPU å®Ÿè¡Œã¯éæ¨å¥¨ã§ã™ã€‚

---

## æ¦‚è¦ï¼ˆå®Ÿè£…æ–¹é‡ï¼‰

* ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ **4-bit** ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆ`load_in_4bit=True` via transformers + bitsandbytesï¼‰ã€‚
* **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯å‡çµï¼ˆfreezeï¼‰**ã—ã¦ eval ãƒ¢ãƒ¼ãƒ‰ã§è’¸ç•™ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ä¿æŒã€‚

  ï¼ˆè’¸ç•™ã®è¨ˆç®—ã¯è»½é‡åŒ–ã®ãŸã‚ batch-wise ã«è¡Œã†ï¼‰
* LoRAï¼ˆPEFTï¼‰ã‚’ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ã€LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’å­¦ç¿’ã€‚
* å­¦ç¿’ãƒãƒƒãƒã¯ `new_task` ã¨ `rehearsal`ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’æ··ãœã¦ä½œæˆï¼ˆæ¯”ç‡ã¯ãƒã‚¤ãƒ‘ãƒ©ã§æŒ‡å®šï¼‰ã€‚
* æå¤±é–¢æ•°ï¼š

  * `loss_task`ï¼šæ–°ã‚¿ã‚¹ã‚¯ã®æ¨™æº–ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆmasked / causal ã«åˆã‚ã›ï¼‰
  * `loss_distill`ï¼š`KL(model_logits || base_logits)`ï¼ˆLwFï¼‰ã‚’æ–°ã‚¿ã‚¹ã‚¯ãƒãƒƒãƒã«ã‚‚é©ç”¨
  * `loss_total = loss_task + lambda_distill * loss_distill`
* æœ€å¾Œã« LoRA ã‚¦ã‚§ã‚¤ãƒˆã ã‘ä¿å­˜ï¼ˆ`PeftModel.save_pretrained`ï¼‰ã€‚

---

## ãƒ•ãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰

ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹ï¼š`qlora_lwf_train.py`

```python
# qlora_lwf_train.py
import os
import math
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, ConcatDataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    get_linear_schedule_with_warmup,
    logging as hf_logging
)
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

hf_logging.set_verbosity_info()

# ---------------------------
# Config
# ---------------------------
MODEL_NAME = "meta-llama/Llama-2-7b"  # ä¾‹: Mistral / LLaMA2 ç­‰ï¼ˆHFã«ã‚ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰
TOKENIZER_NAME = MODEL_NAME
OUTPUT_DIR = "./qlora_lwf_out"
BATCH_SIZE = 4                 # å®Ÿæ©Ÿã«åˆã‚ã›ã¦èª¿æ•´
ACCUMULATION_STEPS = 8         # effective batch = BATCH_SIZE * ACCUMULATION_STEPS
EPOCHS = 3
LR = 2e-4
WARMUP_STEPS = 100
WEIGHT_DECAY = 0.0
MAX_LENGTH = 512
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# LoRA settings
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # ãƒ¢ãƒ‡ãƒ«ä¾å­˜

# Distillation weight
LAMBDA_DISTILL = 0.5

# Rehearsal ratio: 0.2 means 20% of training examples come from base data
REHEARSAL_RATIO = 0.2

# Quantization config for 4-bit (transformers+bnb)
LOAD_4BIT = True

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ---------------------------
# Dataset classes (ä¾‹)
# ---------------------------
class TextDataset(Dataset):
    """Very simple dataset: expects list of strings"""
    def __init__(self, texts, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.texts = texts
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        txt = self.texts[idx]
        enc = self.tokenizer(txt, truncation=True, max_length=self.max_length, return_tensors="pt")
        # convert to 1D tensors
        input_ids = enc["input_ids"].squeeze(0)
        attention_mask = enc["attention_mask"].squeeze(0)
        return {"input_ids": input_ids, "attention_mask": attention_mask}

# ---------------------------
# Utility: collate for variable-length sequences (pad to max in batch)
# ---------------------------
def collate_fn(batch, pad_token_id):
    max_len = max(x["input_ids"].size(0) for x in batch)
    input_ids = []
    attention_mask = []
    for x in batch:
        l = x["input_ids"].size(0)
        pad_len = max_len - l
        if pad_len > 0:
            input_ids.append(torch.cat([x["input_ids"], torch.full((pad_len,), pad_token_id, dtype=torch.long)]))
            attention_mask.append(torch.cat([x["attention_mask"], torch.zeros(pad_len, dtype=torch.long)]))
        else:
            input_ids.append(x["input_ids"])
            attention_mask.append(x["attention_mask"])
    input_ids = torch.stack(input_ids, dim=0)
    attention_mask = torch.stack(attention_mask, dim=0)
    return {"input_ids": input_ids, "attention_mask": attention_mask}

# ---------------------------
# Load tokenizer
# ---------------------------
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=False)
if tokenizer.pad_token_id is None:
    tokenizer.add_special_tokens({"pad_token": "<|pad|>"})

# ---------------------------
# Load base model (frozen) and training model (4-bit + LoRA)
# ---------------------------

# 1) Load base model as fp16/float (frozen) to provide distillation targets.
#    Option: load base model on CPU or different device to save VRAM. Here we load in fp16 on device.
print("Loading base model (teacher) in fp16...")
teacher = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",  # auto place; may use multiple devices
    trust_remote_code=True
)
teacher.eval()
for p in teacher.parameters():
    p.requires_grad = False

# 2) Load student model in 4bit and prepare for k-bit training
print("Loading student model in 4-bit (bnb) and applying LoRA (PEFT)...")
if LOAD_4BIT:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        load_in_4bit=True,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=None  # transformers bnb auto may pick defaults; requires bitsandbytes installed
    )
else:
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True)

# Prepare for k-bit training (peft util)
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # for debug

# Move student model to training device (if not already)
# If device_map="auto" placed parts, ensure training uses proper devices (accelerate recommended)
model.train()

# ---------------------------
# Prepare datasets (new_task + rehearsal)
# ---------------------------
# For demo: use small datasets. Replace with real files / HF datasets.
# new_texts: target-domain fine-tuning data (instruction-style preferred)
new_texts = [
    "è³ªå•: æ±äº¬ã‚¿ãƒ¯ãƒ¼ã®é«˜ã•ã¯ï¼Ÿ\nå›ç­”: æ±äº¬ã‚¿ãƒ¯ãƒ¼ã®é«˜ã•ã¯333ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã€‚",
    "è³ªå•: Pythonã§ãƒªã‚¹ãƒˆã‚’åè»¢ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ\nå›ç­”: list[::-1] ã‚’ä½¿ã„ã¾ã™ã€‚"
] * 200  # multiply to get some size

# rehearsal_texts: subset of generic pretraining / instruction data
rehearsal_texts = [
    "ã“ã‚Œã¯ä¸€èˆ¬çš„ãªæ–‡ç« ã®ä¾‹ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚",
    "è‡ªç„¶è¨€èªå‡¦ç†ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†åˆ†é‡ã§ã™ã€‚"
] * 1000

new_ds = TextDataset(new_texts, tokenizer, max_length=MAX_LENGTH)
rehearsal_ds = TextDataset(rehearsal_texts, tokenizer, max_length=MAX_LENGTH)

# Create mixed dataset using ratio
num_new = len(new_ds)
num_re = int((REHEARSAL_RATIO / (1 - REHEARSAL_RATIO)) * num_new)  # ensure proportion
# create a repeated rehearsal dataset of appropriate length
repeats = max(1, math.ceil(num_re / len(rehearsal_ds)))
rehearsal_large = ConcatDataset([rehearsal_ds] * repeats)

# Truncate rehearsal to required num_re
from torch.utils.data import Subset
rehearsal_large = Subset(rehearsal_large, list(range(min(len(rehearsal_large), num_re))))

mixed_ds = ConcatDataset([new_ds, rehearsal_large])

train_loader = DataLoader(
    mixed_ds,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=lambda b: collate_fn(b, pad_token_id=tokenizer.pad_token_id)
)

# ---------------------------
# Optimizer & Scheduler
# ---------------------------
optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)
num_update_steps_per_epoch = math.ceil(len(train_loader) / ACCUMULATION_STEPS)
total_training_steps = EPOCHS * num_update_steps_per_epoch

scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_training_steps)

# Distillation loss: KLDiv between log_softmax(student) and softmax(teacher)
kl_loss_f = nn.KLDivLoss(reduction="batchmean")

# ---------------------------
# Training loop
# ---------------------------
print("Start training loop...")
global_step = 0
scaler = torch.cuda.amp.GradScaler(enabled=True)  # mixed precision

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    for step, batch in enumerate(train_loader):
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)

        # forward teacher (in fp16, on device_map auto) to get distillation targets
        # compute teacher logits with no grad
        with torch.no_grad():
            teacher_out = teacher(input_ids=input_ids, attention_mask=attention_mask)
            teacher_logits = teacher_out.logits.detach()  # (B, T, V)

        # student forward + loss
        with torch.cuda.amp.autocast(enabled=True):
            student_out = model(input_ids=input_ids, attention_mask=attention_mask)
            student_logits = student_out.logits  # (B, T, V)

            # Causal LM loss: shift labels to match
            # For simplicity, use labels = input_ids (teacher-styled) for MLM/casual
            labels = input_ids.clone()
            # compute CE loss (ignore pad)
            loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
            loss_task = loss_fct(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))

            # Distillation: KLDiv between teacher (soft) and student (log probs)
            # reduce across vocab
            student_logprob = nn.functional.log_softmax(student_logits / 1.0, dim=-1)
            teacher_prob = nn.functional.softmax(teacher_logits / 1.0, dim=-1)
            # KLDivLoss expects input = log_prob (student), target = prob (teacher)
            loss_distill = kl_loss_f(student_logprob, teacher_prob)

            loss = loss_task + LAMBDA_DISTILL * loss_distill

            loss = loss / ACCUMULATION_STEPS

        scaler.scale(loss).backward()

        if (step + 1) % ACCUMULATION_STEPS == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            scheduler.step()
            global_step += 1

        running_loss += loss.item() * ACCUMULATION_STEPS

        if global_step % 10 == 0 and global_step > 0:
            print(f"Epoch {epoch+1} step {global_step} avg_loss {running_loss/global_step:.4f}")

    print(f"Epoch {epoch+1} finished. avg loss: {running_loss / (step+1):.4f}")

    # Save LoRA adapters per epoch
    model.save_pretrained(os.path.join(OUTPUT_DIR, f"lora_epoch_{epoch+1}"))

# ---------------------------
# Final save
# ---------------------------
model.save_pretrained(os.path.join(OUTPUT_DIR, "lora_final"))
tokenizer.save_pretrained(OUTPUT_DIR)
print("Training finished. LoRA adapters saved to", OUTPUT_DIR)
```

---

## å®Ÿå‹™ä¸Šã®è£œè¶³ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒã‚¤ãƒ³ãƒˆ

* **TARGET_MODULES** ã¯ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¾å­˜ã§ã™ï¼ˆLLaMAç³»ãªã‚‰ `q_proj`, `v_proj` ç­‰ï¼‰ã€‚models ã® layer åã‚’ `model.named_modules()` ã§ç¢ºèªã—ã¦æœ€é©ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
* **teacher ã®ãƒ­ãƒ¼ãƒ‰**ã¯VRAMã‚’ä½¿ã„ã¾ã™ã€‚ã‚‚ã— VRAM ãŒä¸è¶³ã™ã‚‹å ´åˆã€teacher ã‚’ CPU ã«å‰²ã‚Šå½“ã¦ã€ãƒãƒƒãƒã‚’å°‘ã—ãšã¤ CPUâ†’GPU ã«è»¢é€ã—ã¦è¨ˆç®—ã™ã‚‹è¨­è¨ˆã‚‚å¯èƒ½ï¼ˆãŸã ã—é…ããªã‚‹ï¼‰ã€‚
* **KL ã®æ¸©åº¦ï¼ˆtemperatureï¼‰** ã‚’ä½¿ã†ã¨è’¸ç•™åŠ¹æœãŒå¤‰ã‚ã‚Šã¾ã™ï¼ˆã“ã“ã¯ temp=1.0 ã«å›ºå®šï¼‰ã€‚æ¸©åº¦ã‚’ä¸Šã’ã‚‹ã¨ soft targets ãŒåºƒãŒã‚Šã€ã‚ˆã‚Šæ•™å¸«åˆ†å¸ƒã‚’æ¨¡å€£ã—ã‚„ã™ããªã‚Šã¾ã™ã€‚
* **å­¦ç¿’ç‡ãƒ»LoRA rank** ã¯ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã€‚r=8ã€œ16ã€lr=1e-4ã€œ2e-4 ãŒå®Ÿå‹™ã§ã‚ˆãä½¿ã‚ã‚Œã¾ã™ã€‚
* **ãƒ‡ãƒ¼ã‚¿æ··åˆæ¯”ï¼ˆREHEARSAL_RATIOï¼‰** ã‚’ 0.1ã€œ0.3 ã«ã—ã¦ãŠãã¨å¿˜å´ãŒæŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚
* **accelerate** ã‚’ä½¿ã†ã¨ `device_map="auto"` ã‚„åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã‚ˆã‚Šå®‰å…¨ã«è¡Œãˆã¾ã™ï¼ˆæ¨å¥¨ï¼‰ã€‚

---

## ã•ã‚‰ã«é€²ã‚ãŸã„å ´åˆï¼ˆæ¨å¥¨æ‹¡å¼µï¼‰

1. **QLoRAï¼ˆbitsandbytesï¼‰è©³ç´°è¨­å®š** ï¼šé‡å­åŒ– configï¼ˆnf4 ç­‰ï¼‰ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚
2. **PEFT ã® LoRA ã‚’ä½ç›¸çš„ã« Freeze/Unfreeze** ï¼šæœ€åˆã¯å°ã•ãª rank ã§å­¦ç¿’ã€å¾ŒåŠã§ rank ã‚’å¢—ã‚„ã™ç­‰ã® curriculumã€‚
3. **Better Distillation** ï¼šPerplexity/Calibration å¯¾ç­–ã¨ã—ã¦ label smoothing ã‚„ temperature ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å°å…¥ã€‚
4. **è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ** ï¼šMMLU / Winogrande / æŒ‡ç¤ºè¿½å¾“ãƒ†ã‚¹ãƒˆãªã©ã§æ¯”è¼ƒè©•ä¾¡ã€‚

---

å¿…è¦ãªã‚‰æ¬¡ã¯ã©ã‚Œã‚’è©³ã—ãä½œã‚Šã¾ã™ã‹ï¼Ÿ

* âœ… QLoRA ã® bitsandbytes è¨­å®šï¼ˆnf4, 4-bit è©³ç´°ï¼‰
* âœ… `accelerate` ã‚’ä½¿ã£ãŸåˆ†æ•£å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ¨å¥¨ï¼‰
* âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆinstruction tuning ç”¨ JSONL ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰
* âœ… LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®Ÿé¨“ãƒ­ã‚° & Grid-search ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã©ã‚Œã‚’å…ˆã«å‡ºã—ã¾ã—ã‚‡ã†ï¼Ÿ

## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿæ–½

æº–å‚™ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¾®èª¿æ•´ï¼‰ã‚’è¡Œã„ã¾ã™ã€‚è¨ˆç®—è³‡æºãŒé™ã‚‰ã‚Œã‚‹Colabç’°å¢ƒã§åŠ¹ç‡ã‚ˆãå­¦ç¿’ã‚’è¡Œã†ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå·¥å¤«ãƒ»è¨­å®šã‚’è¡Œã„ã¾ã™ã€‚

* **LoRAã®é©ç”¨** : LLaMAã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ãªã‚ŠãŒã¡ã§ã™ã€‚ãã“ã§**LoRA (Low-Rank Adaptation)** ã¨ã„ã†æŠ€è¡“ã‚’ä½¿ã„ã€ãƒ¢ãƒ‡ãƒ«å†…ã®ä¸€éƒ¨é‡ã¿ï¼ˆè¡Œåˆ—ï¼‰ã ã‘ã‚’å­¦ç¿’å¯èƒ½ã«ã—ã€ãã‚Œä»¥å¤–ã¯å‡çµã—ã¾ã™ã€‚LoRAã§ã¯ã”ãå°ã•ãªè¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ï¼‰ã ã‘ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨è¨ˆç®—é‡ãŒå¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚Unslothã§ã¯LoRAã®é©ç”¨ã‚‚ç°¡å˜ã«è¡Œãˆã¾ã™ã€‚
* **4bité‡å­åŒ–ã®æ´»ç”¨** : æ—¢ã«ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã«4bité‡å­åŒ–ã‚’è¡Œã„ã¾ã—ãŸãŒã€å­¦ç¿’ä¸­ã‚‚å¼•ãç¶šããã®åŠ¹æœã§ãƒ¡ãƒ¢ãƒªç¯€ç´„ãŒã§ãã¾ã™ã€‚**QLoRA**ã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã§ã¯ã€Œ4bité‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« + LoRAã®çµ„ã¿åˆã‚ã›ã€ã«ã‚ˆã‚Šãƒ•ãƒ«ç²¾åº¦ã«è¿‘ã„æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ä½ãƒ¡ãƒ¢ãƒªã§å­¦ç¿’ã§ãã¾ã™ã€‚
* **å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ** : ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚ã®ä¸€æ™‚çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨ã‚’æ¸›ã‚‰ã™ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã¨ã—ã¦Gradient CheckpointingãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯é †ä¼æ¬ã§è¨ˆç®—ã—ãŸä¸­é–“çµæœã‚’ä¿æŒã›ãšéƒ½åº¦å†è¨ˆç®—ã™ã‚‹ã“ã¨ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚
* **ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´** : ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºï¼‰ã¯ãƒ¡ãƒ¢ãƒªã«ç›´æ¥å½±éŸ¿ã—ã¾ã™ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ãã¦ã‚‚å­¦ç¿’è‡ªä½“ã¯å¯èƒ½ã§ã™ãŒã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆiterationæ•°ï¼‰ã‚’å¢—ã‚„ã™å¿…è¦ãŒã‚ã‚‹ç‚¹ã«ç•™æ„ã—ã¦ãã ã•ã„ã€‚

> ä½œã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã¯LoRAã®ãƒ¢ãƒ‡ãƒ«ã€4bité‡å­åŒ–ã€å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´



# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµæœ

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®çµæœãƒ¢ãƒ‡ãƒ«ãŒã€Œè¨€èªãƒ¢ãƒ‡ãƒ«ã€ã‹ã‚‰ã€ŒæŒ‡ç¤ºè¿½å¾“å‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ã¸æŒ¯ã‚‹èˆã„ã‚’æ˜ç¢ºã«å¤‰ãˆãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

ã“ã‚Œã¯ **unsloth/gemma-2b-bnb-4bit Ã— yahma/alpaca-cleaned** ã¨ã„ã†çµ„ã¿åˆã‚ã›ã«ãŠã„ã¦ã€æ¥µã‚ã¦å…¸å‹çš„ã‹ã¤ã€ŒæˆåŠŸã—ã¦ã„ã‚‹ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœã§ã™ã€‚

ä»¥ä¸‹ã€ä½•ãŒã©ã†å¤‰ã‚ã£ãŸã®ã‹ã‚’åˆ†è§£ã—ã¦èª¬æ˜ã—ã¾ã™ã€‚

---

## 1. æœ€ã‚‚å¤§ããªå¤‰åŒ–ï¼šInstruction Following èƒ½åŠ›ã®ç²å¾—

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ï¼ˆæƒ³å®šã•ã‚Œã‚‹æŒ™å‹•ï¼‰

* æ–‡ã®ç¶šãã‚’äºˆæ¸¬ã™ã‚‹ã ã‘
* ã€ŒInstructionã€ã€ŒResponseã€ã¨ã„ã†æ§‹é€ ã‚’ç†è§£ã—ãªã„
* å…¥åŠ›æ–‡ã‚’ãã®ã¾ã¾ç¹°ã‚Šè¿”ã™ã€è„±ç·šã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œï¼ˆã‚ãªãŸã®çµæœï¼‰

* Instruction ã‚’ **å‘½ä»¤ã¨ã—ã¦è§£é‡ˆ**
* ã€Œè³ªå• â†’ å›ç­”ã€ã¨ã„ã†å½¹å‰²åˆ†é›¢ãŒæˆç«‹
* å‡ºåŠ›ãŒæ˜ç¢ºã« **Response ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å‘ã‘**ã«ãªã‚‹

ğŸ‘‰ **Alpacaå½¢å¼ã®å¯¾è©±ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’å­¦ç¿’ã—ãŸ**

---

## 2. å‡ºåŠ›æ§‹é€ ãŒå®‰å®šã—ãŸï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå­¦ç¿’ï¼‰

### è¦³æ¸¬ã§ãã‚‹å¤‰åŒ–

* ç®‡æ¡æ›¸ãï¼ˆ1., 2., 3.ï¼‰ãŒè‡ªç„¶ã«ä½¿ã‚ã‚Œã‚‹
* èª¬æ˜æ–‡ã®æ–‡ä½“ãŒå®‰å®š
* è¡¨å½¢å¼ã‚’è¦æ±‚ã•ã‚Œã‚‹ã¨ã€Œè¡¨ã‚’ä½œã‚ã†ã¨ã™ã‚‹ã€

ã“ã‚Œã¯å†…å®¹ç†è§£ä»¥å‰ã«ã€

> **ã€Œã©ã†ç­”ãˆã‚‹ã®ãŒæ­£ã—ã„ã‹ã€**

ã‚’å­¦ç¿’ã—ãŸè¨¼æ‹ ã§ã™ã€‚

---

## 3. æŠ½è±¡åº¦åˆ¶å¾¡ãŒå¯èƒ½ã«ãªã£ãŸ

ä¾‹ï¼š

```
Instruction: Explain the importance of sleep in simple terms.
```

### è¦³æ¸¬çµæœ

* å°‚é–€ç”¨èªã‚’é¿ã‘ã‚‹
* ä¸€èˆ¬å‘ã‘ã®èªå½™ã‚’é¸æŠ
* é•·ã•ãŒéå‰°ã«ãªã‚‰ãªã„

ğŸ‘‰ **ã€Œsimple termsã€ã¨ã„ã†æ¡ä»¶ã‚’å°Šé‡**

ã“ã‚Œã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å˜ä½“ã§ã¯å¼±ã„èƒ½åŠ›ã§ã™ã€‚

---

## 4. ã‚¿ã‚¹ã‚¯ã‚¹ã‚¤ãƒƒãƒèƒ½åŠ›ãŒç™ºç”Ÿã—ãŸ

åŒä¸€ãƒ¢ãƒ‡ãƒ«ãŒä»¥ä¸‹ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ã„ã¾ã™ã€‚

| æŒ‡ç¤º     | å‡ºåŠ›ã®æ€§è³ª   |
| -------- | ------------ |
| èª¬æ˜     | è‡ªç„¶æ–‡       |
| è³ªå•     | ç®‡æ¡æ›¸ã     |
| æ­´å²     | å› æœåˆ—æŒ™     |
| ã‚³ãƒ¼ãƒ‰   | Pythonã‚³ãƒ¼ãƒ‰ |
| è¦ç´„ï¼‹è¡¨ | æƒ…å ±æ•´ç†     |

ğŸ‘‰ **ã‚¿ã‚¹ã‚¯æ¡ä»¶ä»˜ãç”Ÿæˆ**ãŒæˆç«‹

ã“ã‚Œã¯ Alpaca-cleaned ã®æœ€å¤§ã®åŠ¹æœã§ã™ã€‚

---

## 5. å‡ºåŠ›ã®ã€Œé€”ä¸­åˆ‡ã‚Œã€ã‹ã‚‰è¦‹ãˆã‚‹é™ç•Œã‚‚æ˜ç¢º

ä¸€æ–¹ã§ã€ã„ãã¤ã‹é‡è¦ãªè¦³å¯Ÿç‚¹ãŒã‚ã‚Šã¾ã™ã€‚

### 5.1 é€”ä¸­ã§æ–‡ãŒåˆ‡ã‚Œã¦ã„ã‚‹

ä¾‹ï¼š

```
4. The Great Depression was
```

```
print(f"{number
```

### æƒ³å®šã•ã‚Œã‚‹åŸå› 

1. **max_new_tokens ãŒçŸ­ã„**
2. **EOSãƒˆãƒ¼ã‚¯ãƒ³ã®å­¦ç¿’ãŒä¸å®Œå…¨**
3. **Gemma-2Bã®ç”ŸæˆæŒç¶šåŠ›ã®é™ç•Œ**
4. **QLoRA + 4bit é‡å­åŒ–ã«ã‚ˆã‚‹è¡¨ç¾åˆ¶ç´„**

ğŸ‘‰ **ã€Œèƒ½åŠ›ä¸è¶³ã€ã§ã¯ãªãã€Œç”Ÿæˆåˆ¶å¾¡ã®å•é¡Œã€**

---

## 6. äº‹å®Ÿæ€§ã®å‘ä¸Šã§ã¯ãªãã€Œå½¢å¼çš„å¦¥å½“æ€§ã€ã®å‘ä¸Š

GDPã®ä¾‹ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã§ã™ã€‚

* åŒã˜æ–‡ã‚’ç¹°ã‚Šè¿”ã™
* è¡¨ãŒä½œã‚Œã¦ã„ãªã„
* æ•°å€¤ã®æ¤œè¨¼ã¯è¡Œã‚ã‚Œã¦ã„ãªã„

ã“ã‚Œã¯

> **çŸ¥è­˜ãŒå¢—ãˆãŸã‚ã‘ã§ã¯ãªã„**

ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

### å­¦ç¿’ã•ã‚ŒãŸã®ã¯

* æŒ‡ç¤ºã¸ã®å¾“ã„æ–¹
* å›ç­”ã‚‰ã—ã„æ›¸ãæ–¹
* æƒ…å ±æ•´ç†ã®ã€Œå‹ã€

---

## 7. å†…éƒ¨çš„ã«èµ·ãã¦ã„ã‚‹å¤‰åŒ–ï¼ˆæ¨å®šï¼‰

### é‡ã¿ç©ºé–“ãƒ¬ãƒ™ãƒ«ã§ã¯

* LoRA Adapter ãŒ
  * å‡ºåŠ›åˆæœŸãƒˆãƒ¼ã‚¯ãƒ³åˆ†å¸ƒã‚’å¤‰æ›´
  * ã€ŒResponse:ã€ä»¥é™ã®ç”Ÿæˆç¢ºç‡ã‚’å¤§å¹…ã«å†é…åˆ†

### ç‰¹ã«å¤‰åŒ–ã—ãŸå±¤

* FFNä¸Šä½å±¤
* å‡ºåŠ›å´ attention
* æŒ‡ç¤ºæ–‡ãƒˆãƒ¼ã‚¯ãƒ³å‘¨è¾ºã®è¡¨ç¾

ğŸ‘‰ **åŸºç›¤çŸ¥è­˜ã¯ã»ã¼ãã®ã¾ã¾ã€å‡ºåŠ›åˆ¶å¾¡ã®ã¿å¤‰æ›´**

---

## 8. unsloth Ã— alpaca-cleaned ã®çµ„ã¿åˆã‚ã›ãŒåŠ¹ã„ã¦ã„ã‚‹ç†ç”±

| è¦ç´            | åŠ¹æœ                  |
| -------------- | --------------------- |
| Gemma-2B       | ååˆ†ãªè¨€èªèƒ½åŠ›        |
| 4bité‡å­åŒ–     | è¨ˆç®—è³‡æºå‰Šæ¸›          |
| LoRA           | çŸ¥è­˜ç ´å£Šãªã—          |
| Alpaca-cleaned | é«˜å“è³ªInstructionå½¢å¼ |
| Unsloth        | å­¦ç¿’ã®å®‰å®šæ€§ãƒ»é€Ÿåº¦    |

ğŸ‘‰ **ã€Œæœ€å°ã‚³ã‚¹ãƒˆã§æœ€å¤§ã®è¡Œå‹•å¤‰åŒ–ã€**

---

## 9. ä¸€è¨€ã§ã¾ã¨ã‚ã‚‹ã¨

> **ãƒ¢ãƒ‡ãƒ«ã¯è³¢ããªã£ãŸã®ã§ã¯ãªãã€ã€ŒæŒ‡ç¤ºã«å¾“ã†ã‚ˆã†ã«èª¿æ•™ã•ã‚ŒãŸã€**

ã“ã‚Œã¯LLMãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç†æƒ³å½¢ã®ã²ã¨ã¤ã§ã™ã€‚

---

## æ¬¡ã«æ”¹å–„ã™ã‚‹ã¨ã—ãŸã‚‰

1. `max_new_tokens` å¢—åŠ 
2. EOSãƒˆãƒ¼ã‚¯ãƒ³æ˜ç¤º
3. å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¶ç´„ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ï¼‰
4. Codeå°‚ç”¨ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
5. Repetition penalty èª¿æ•´

ã“ã“ã¾ã§æ¥ã¦ã„ã‚Œã°ã€

* ç¤¾å†…ãƒ„ãƒ¼ãƒ«
* FAQãƒœãƒƒãƒˆ
* æ•™è‚²ç”¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ

ã«ã¯ **å³å®Ÿç”¨ãƒ¬ãƒ™ãƒ«**ã§ã™ã€‚

å¿…è¦ã§ã‚ã‚Œã°
ã€Œã“ã®å‡ºåŠ›ã‹ã‚‰é€†ç®—ã—ãŸæœ€é©ãªå­¦ç¿’è¨­å®šã€
ã€Œé€”ä¸­åˆ‡ã‚Œã‚’é˜²ãå…·ä½“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€
ã¾ã§è½ã¨ã—è¾¼ã¿ã¾ã™ã€‚

![1765686995784](image/perf/1765686995784.png)

Instruction: Explain the importance of sleep in simple terms.
Response:Sleep is an essential part of our daily routine and is vital for our overall health and well-being. It helps us to recover from physical and mental exhaustion, improves our cognitive abilities, and promotes overall health and wellness. It is important to get enough sleep every night, as it allows our bodies to rest and repair themselves, and helps us to function at our best. Without enough sleep, we can experience fatigue, irritability, and difficulty concentrating, which can have a negative impact on our daily lives.

Instruction: What supports the U.S. economy?
Response: 1. The U.S. economy is supported by a diverse range of industries, including agriculture, manufacturing, technology, healthcare, and finance. 2. The government provides support through tax incentives, infrastructure investments, and other programs that encourage businesses to invest and grow. 3. The U.S. has a strong labor force, with a high level of education and training, which allows businesses to hire and retain skilled workers. 4. The U.S. has a strong financial system

Instruction: What were the causes of the American Depression?
Response: 1. The Great Depression was caused by the stock market crash of 1929, which led to a severe economic downturn. 2. The Great Depression was also caused by the failure of the banking system, which led to a loss of confidence in the economy. 3. The Great Depression was also caused by the loss of jobs and the loss of income, which led to a decrease in consumer spending and a decrease in demand for goods and services. 4. The Great Depression was

Instruction: Write code to calculate prime numbers. Input: Response:

`<code>`def is_prime(number):
    if number < 2:
        return False
    for i in range(2, number):
        if number % i == 0:
            return False
    return True

def main():
    number = int(input("Enter a number: "))
    if is_prime(number):
        print(f"{number} is a prime number.")
    else:
        print(f"{number

Instruction: Summarize the GDPs of the United States, Japan, and China with a table. Input: Response:

The GDP of the United States in 2021 was $20.4 trillion, Japan was $5.1 trillion, and China was $14.6 trillion.

The GDP of the United States in 2021 was $20.4 trillion, Japan was $5.1 trillion, and China was $14.6 trillion.

The GDP of the United States in 2021 was $20.4 trillion
