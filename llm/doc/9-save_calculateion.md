結論から言うと、**「保持できる情報の長さ（コンテキストウィンドウ）」は、ここ数年で劇的に進化しました。**

初期のGPT-3が約2,000トークンだったのに対し、現在の最新モデル（GPT-4oやLlama 3.1、Gemini 1.5 Proなど）は、**数万〜数百万トークン**を一度に処理することが可能です。

なぜこれほど長い情報を保持できるようになったのか、その技術的ブレイクスルーを3つの観点で整理します。

---

### 1. 位置エンコーディングの進化（RoPEの登場）

Transformerは本来、単語の「順番」を理解できません。そのため位置情報を付与しますが、以前の手法（絶対位置エンコーディング）では、学習時より長い文章を入力すると精度がガタ落ちしていました。

* **RoPE (Rotary Positional Embedding)**: Llamaなどで採用されている手法です。単語の位置関係を「回転（角度）」で表現します。これにより、学習時よりも遥かに長いシーケンスに対しても、モデルが相対的な位置関係を維持したまま計算できるようになりました。

---

### 2. メモリ効率の劇的な改善（KVキャッシュの共有化）

長い文章を扱う最大の敵は、GPUメモリの不足です。文章が長くなるほど、過去の単語を覚えておくための「KVキャッシュ」が膨大になります。

これを解決したのが、以前お話しした **GQA (Grouped-Query Attention)** です。

* 全てのヘッドでKeyとValueを共有・集約することで、メモリ消費量を数分の1に抑えました。これにより、同じGPUメモリ量でも、以前より4倍〜8倍長い文章を読み込めるようになりました。

---

### 3. 注意機構のアルゴリズム最適化（FlashAttention）

従来のAttention計算は、文章の長さの2乗（）に比例して計算量とメモリが増えるという数学的な弱点がありました。

* **FlashAttention**: GPUの高速なメモリ（SRAM）を最大限活用し、行列計算を細切れにして処理するアルゴリズムです。これにより、計算の「壁」を取り払い、数万トークンの計算も現実的な時間で終わるようになりました。

---

### 4. 具体的な進化の比較

| モデル世代 | コンテキスト長 (トークン数) | 保持できる情報の目安 |
| --- | --- | --- |
| **GPT-3 (2020)** | 約 2,048 | 短いブログ記事1本分 |
| **GPT-4 (初期)** | 8,192 / 32,768 | 短編小説1冊分 |
| **Llama 3.1 (2024)** | 128,000 | 本数冊分 |
| **Gemini 1.5 Pro** | 1,000,000 〜 2,000,000 | 大長編小説、数時間の動画、巨大なソースコード全件 |

---

### 5. それでも残る課題：Lost in the Middle

「読み込める」ことと「完璧に覚えている」ことは別です。
最近の研究では、あまりに長い文章を入力すると、**「最初の方と最後の方は覚えているが、真ん中の方の情報を見落とす（Lost in the Middle）」**という現象が確認されています。

### まとめ

GPTやLlamaは、**物理的（メモリ）・数学的（位置計算）な制約を克服し、長大な情報を保持できるようになりました。** しかし、人間と同様に「あまりに長いと、中ほどの細かい内容を忘れる」という弱点があるため、現在もその検索精度を上げるための改良が続けられています。

**次は、この長いコンテキストをフル活用するために、どうやって「特定の情報を正確に抽出させるか（Needle In A Haystackテスト）」という評価手法について興味はありますか？**