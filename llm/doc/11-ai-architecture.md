近年のAIバブルはLLMの他に、LLMを高速に動作させることが出来るAI専用の半導体の存在があります。
LLMの進化と同様、計算基盤となるAI専用の半導体も進化が進んでいます。
今回はAI専用半導体について説明を行います。

## 概要
AI半導体における「アーキテクチャ」とは、ソフトウェア（AIモデル）が要求する膨大な計算を、　**「いかに効率よく、物理的な電子回路の配置（レイアウト）に落とし込むか」**　という設計の仕組みそのものを指します。
AI半導体のアーキテクチャは、一般的なCPU（中央演算処理装置）とは根本的に異なる思想で設計されています。一言で言えば、 **「巨大なデータの塊を、単純な計算（掛け算と足し算）で、超並列に処理すること」** に特化した構造です。

主要なアーキテクチャの特徴を4つのポイントで解説します。

### 1. 「行列演算」に特化した演算ユニット（Tensor Coreなど）

AI（特にディープラーニング）の計算の大部分は、重みと入力データの　**「積和演算（MAC: Multiply-Accumulate）」**　です。
従来のCPUが1つずつ計算を行うのに対し、AI半導体は「行列（テンソル）」を丸ごと一度に計算できる専用回路を持っています。

* **シストリック・アレイ (Systolic Array):** GoogleのTPUなどで採用されている構造。演算器が網の目のように並び、データが波のように隣の演算器へ伝播しながら計算が進むため、メモリへのアクセス回数を劇的に減らせます。


### 2. 超並列処理（Massive Parallelism）

CPUは数個〜数十個の高性能なコアを持ち、複雑な命令を高速に処理しますが、AI半導体は　**「小さくて単純な演算器」を数千〜数万個**　並べます。

* **GPU (Graphics Processing Unit):** もともと画像処理（ピクセル計算）用に開発された並列構造が、AIの行列演算と相性が良く、現在の主流となっています。
* **NPU (Neural Processing Unit):** AI計算だけに特化し、不要な機能を削ぎ落とすことで電力効率を極限まで高めたものです。


### 3. メモリ帯域の極大化（HBMの採用）

AIの計算では、膨大な「重みデータ」をメモリから演算器へ絶えず送り込む必要があります。計算速度が速すぎるとメモリからの供給が追いつかない「メモリの壁」にぶつかるため、特殊なメモリ接続技術が使われます。

* **HBM (High Bandwidth Memory):** メモリチップを垂直に積み上げ、シリコン貫通電極（TSV）で演算チップと直結することで、従来のDRAMとは比較にならないほどのデータ転送速度を実現します。


### 4. 低精度演算の活用（Quantization）

科学シミュレーションなどでは高い精度（64bit浮動小数点など）が必要ですが、AIの推論や学習では、精度を少し落としても結果に大きな影響が出ないことがわかっています。

* **FP16 / BF16 / INT8:** 精度を16bitや8bitに下げることで、一度に送れるデータ量を増やし、演算速度を数倍〜数十倍に加速させます。最新のチップでは、さらに低いFP4（4bit）などもサポートされています。


### 主要なAIアーキテクチャの例

| アーキテクチャ | 開発元 | 特徴 |
| --- | --- | --- |
| **Hopper (H100/H200)** | NVIDIA | GPUベースの王道。Transformerエンジンを搭載し圧倒的なシェア。 |
| **TPU (Tensor Processing Unit)** | Google | 行列演算に特化したシストリック・アレイの先駆け。 |
| **Cerebras Wafer-Scale Engine** | Cerebras | 巨大なウェハ全体を1つのチップにする超巨大アーキテクチャ。 |
| **NPU (Apple Aシリーズなど)** | Apple等 | スマホ等で写真処理や音声認識を低電力で行う推論特化型。 |


## CPUとの違い

AIアーキテクチャを「回路レベル」で一般的なCPUと比較すると、その設計思想は **「命令実行の柔軟性」を捨てて「データのスループット（処理量）」に全振りしている** という点に集約されます。

具体的には、以下の4つの回路的特徴が顕著です。


### 1. 制御回路（Control Unit）の簡略化

CPUは、次にどの命令を実行するかを判断する「条件分岐」や、実行順序を入れ替えて効率化する「アウトオブオーダー実行」など、非常に複雑な制御回路を持っています。

* **CPU:** 全体の面積の多くを制御回路やキャッシュが占め、演算器（ALU）はわずかです。
* **AI半導体:** 制御回路を最小限にし、その分空いたスペースに数千個の演算器を敷き詰めます。
> **「複雑なことはせず、同じ単純作業を全員で一斉にやる」**という、工場のような構造です。


### 2. 積和演算（MAC）の極大化

AI回路の心臓部は、 **「積和演算（Multiply-Accumulate: MAC）」** 回路です。 という計算を1サイクルで行うユニットが格子状に並んでいます。

* **シストリック・アレイ:** CPUのように「命令」を各演算器に配るのではなく、データを隣の演算器へバケツリレーのように流します。これにより、信号を遠くまで飛ばすための「配線電力」を削減し、高効率な行列演算を実現しています。


### 3. メモリ配置（Near-Memory Computing）

CPUは、低速なメインメモリ（DRAM）からデータを取ってくる際の遅延を隠すために、巨大な「キャッシュメモリ（L1/L2/L3）」を持っています。
対してAI回路では、 **「メモリの壁」** を打破するために、演算器のすぐ隣に小さなメモリを大量に配置します。

* **SRAMの多用:** 演算器のすぐそばに「ウェイト（重み）」を保持するための専用SRAM（スクラッチパッドメモリ）を配置し、外部へのデータ通信を最小化します。
* **HBMとの直結:** チップと同じパッケージ内にメモリを封入し、数千本の配線で直結します。


### 4. 精度（ビット幅）の柔軟な削減

CPUは通常、32bitや64bitの「高い精度」で計算することを前提として回路が組まれています。
AI回路は、精度を落としても結果が変わらないという特性を利用し、 **「低いビット幅」** で動くように最適化されています。

* **INT8やFP4の採用:** 64bitの演算器1つ分の面積があれば、8bitの演算器を8つ以上作れます。これにより、同じチップ面積でも演算能力（TOPS）を劇的に向上させています。


### 比較まとめ

述べたCPUとの違いをまとめるとこんな感じとなります。
データの塊をまとめて処理することに長けているのがGPUの特徴です。
CPUは中央集中制御としての役割が得意ですが、データの塊を処理するという意味ではGPUの方が得意ということになります。

| 特徴 | 一般的なCPU | AI半導体 (NPU/GPU) |
| --- | --- | --- |
| **演算器の数** | 数十個（高性能） | 数千〜数万個（単純） |
| **制御回路** | 巨大（複雑な処理が得意） | 最小（並列処理が得意） |
| **データ移動** | キャッシュ階層で遅延防止 | 演算器の隣にメモリを配置 |
| **得意な計算** | スカラー演算（1つずつ） | テンソル演算（行列まとめて） |

## GPUが抱える課題

現在のAIブームの主役であるGPUですが、もともと「グラフィックス（画像描写）」のために設計されたアーキテクチャであるため、最新の巨大なAIモデル（LLMなど）を動かす上ではいくつかの深刻な限界・課題に直面しています。


### 1. 「メモリの壁（Memory Wall）」と帯域不足

GPUの演算速度は飛躍的に向上しましたが、メモリからデータを読み書きするスピードがそれに追いついていません。

* **課題:** 演算器（演算コア）が計算を終えても、次のデータが届くまで「待ちぼうけ」の状態になります。
* **現状:** これを解決するためにHBM（高帯域幅メモリ）を搭載していますが、HBMは非常に高価で、製造工程も複雑なため、供給不足の主因となっています。


### 2. 膨大な消費電力と熱密度

GPUは数千のコアをフル稼働させるため、1枚で数百ワット（最新のH100などは700W以上）の電力を消費します。

* **課題:**
* **冷却コスト:** データセンターでは、サーバーを冷やすための電気代が計算用の電気代に匹敵するほど膨らんでいます。
* **電力網への負荷:** 巨大なAIクラスタは、小さな町一つ分に相当する電力を消費するため、環境負荷が問題視されています。


### 3. 汎用性のトレードオフ（無駄な回路）

GPUはデータの計算にウェイトを置いているとは言え、「何でもできる（汎用的）」であることが強みです。しかし、AIの計算だけを考えた場合、不要な回路も多く含まれています。

* **課題:** グラフィックス処理用のロジックや、複雑な命令セットを維持するための回路がチップ面積を占有しています。
* **解決策としてのNPU:** そのため、特定のAIモデル（Transformerなど）の計算だけに特化し、不要な機能を削ぎ落とした「NPU（Neural Processing Unit）」や「ASIC（専用IC）」に効率で負け始めています。


### 4. スケーラビリティと通信のオーバーヘッド

一つのGPUに載るメモリ量には限界があるため、巨大なAIは数千枚のGPUを繋いで計算します。

* **課題:** GPU同士を通信させる際、チップ内の計算速度に比べてチップ間の通信速度が格段に遅いため、通信待ちが発生します。
* **現状:** NVIDIAのNVLinkのように、GPU間を超高速で結ぶ専用の「道路（インターコネクト）」の開発に莫大なコストがかかっています。


## 技術動向

GPUが抱える「メモリの壁」「電力」「通信」といった課題を解決するために、現在の半導体業界は新しいアプローチへ大きく舵を切っています。

主な技術動向は、以下の4つのテーマで進んでいます。


### 1. 「データの移動」を極限まで減らす技術（近傍・内蔵演算）

データの移動が最も電力を消費し、時間を奪うため、 **「メモリの近く、あるいはメモリの中で計算する」** という発想です。

* **PIM (Processing-in-Memory):** メモリチップの内部に演算回路を埋め込む技術です。データをチップの外に出さずに計算を完結させるため、電力消費を従来の1/10以下に抑えられる可能性があります。
* **CXL (Compute Express Link):** CPU、GPU、メモリを高速かつ低遅延で「一つの巨大なメモリプール」として繋ぐ新しい規格です。GPUごとのメモリ容量の制限を論理的に取り払おうとしています。


### 2. 「チップの巨大化」の限界を超える技術（チップレット）

一つの巨大なチップを作るのは製造コストが高く、不良品も出やすいため、 **「小さなチップを繋ぎ合わせる」** 手法が主流になっています。

* **チップレット (Chiplet) アーキテクチャ:** 演算、メモリ、通信などの機能を別々の小さなチップとして製造し、パズルのように組み合わせます。AMDやIntel、最新のNVIDIA Blackwellなどで採用されています。
* **2.5D / 3D パッケージング:** チップを平面ではなく「垂直」に積み上げる技術です。配線距離をミリ単位からミクロン単位に短縮し、通信速度を劇的に向上させます。

### 3. 「汎用性」から「専用化」へのシフト（DSA）

GPUは何でもできますが、AIには「無駄」が多い。そこで、特定の計算パターン（例：Transformer）だけに特化した **DSA (Domain-Specific Architecture)** の開発が加速しています。

* **ハイパースケーラーの自社製チップ:** Google (TPU)、Amazon (Trainium)、Meta (MTIA) など、自社のAIサービスに特化した専用チップを開発し、GPUよりも圧倒的な電力効率を実現しています。
* **FPGAの活用:** ソフトウェア側（AIモデル）の進化は非常に速いため、ハードウェア回路を後から書き換えられるFPGAを使って、その時々の最新アルゴリズムに最適化したアクセラレータを作る動きも根強いです。


### 4. 「計算精度」の革命（低精度演算）

32bitや16bitといった従来の計算精度をさらに落とし、 **「賢さを維持しつつ計算量を減らす」** アプローチです。

* **FP4 / MX-format:** 最新のGPUアーキテクチャでは「4bit」での計算をサポートし始めています。ビット数を半分にすれば、同じ回路面積で2倍の計算ができ、メモリ帯域の負荷も半分になります。
* **対数演算やスパース性（間引き）:** ほとんどが「0」であるニューラルネットワークの特性を利用し、0の計算を最初からスキップする回路レベルの最適化が進んでいます。

## 展望
普段の仕事でもGPU使っているときに課題が目に付くようになってきた気がします。
多くの気になる点は、兎に角LLMが乗らない、電力凄すぎて用途が合わないということです。
おおよそ上記で述べた課題に乗ってくるようなことです。

現状は改善策となるようなソリューションが出てくる過渡期という感じがします。
GPUを抱えるNvidiaが一強でしたが、変化は出てくると思います。

そして、今後のAIの普及には半導体の設計技術と、半導体の製造技術の底上げが必要ということは揺るがない事実と思います。


