LLM（大規模言語モデル）の核心であるTransformerには、実は **「単語の順番を理解する能力がデフォルトでは備わっていない」** という弱点があります。

これを解決し、モデルに「どの単語がどこにあるか」を教える仕組みが **ポジショナル・エンコーディング（Positional Encoding：位置エンコーディング）** です。

今日はそんなポジショナル・エンコーディングについての説明を行います。

## なぜ位置の情報が必要なのか？

Transformerのアテンション機構は、全ての単語を一斉に、並列で処理します。
例えば以下の2つの文を考えます。

1. **「犬が人を噛んだ」**
2. **「人が犬を噛んだ」**

アテンションにとっては、どちらも登場する単語（犬、人、噛んだ）のセットは同じです。位置の情報がないと、AIはこれら2つの文を **「全く同じ意味」** として処理してしまいます。
これを防ぐために、単語の意味（埋め込みベクトル）に位置の情報を「足し算」することで、言葉がどういう順番で並んでいるかを理解できるようにします。


## 代表的な3つの方式

ポジショナル・エンコーディングには、技術の進化とともに大きく3つのステージがあります。

### ① 絶対位置エンコーディング（初期Transformer）

各ポジション（1番目、2番目...）に対して、固定の数値ベクトルを割り当てます。

* **仕組み:** サイン（正弦）波とコサイン（余弦）波の関数を使って、位置に応じた独自の波形を生成し、単語ベクトルに加算します。
* **弱点:** 学習時よりも長い文章（未知の長さ）が来ると、対応できなくなることがあります。

### ② 相対位置エンコーディング

「1番目」という絶対的な数字ではなく、 **「自分から見て3つ前の単語」** といった単語同士の距離（相対的な位置関係）に注目します。

* **メリット:** 文章が長くなっても「隣との関係」は変わらないため、長い文への対応力が上がります。

### ③ RoPE (Rotary Positional Embedding) ※現在の主流

Llama 3やQwenなど、最新のLLMのほとんどで採用されている方式です。

* **仕組み:** 単語ベクトルを複素平面上で **「回転」** させます。回転させる角度によって位置を表現します。
* **メリット:** 絶対位置と相対位置の両方のメリットを兼ね備えており、数学的に非常に美しく、長文への「外挿（学習時より長い文を処理すること）」にも強いのが特徴です。

## 原理

ポジショナル・エンコーディング（PE）の原理は、一言で言えば **「意味のベクトルに、位置を表す固有の『署名』を刻み込むこと」** です。

Transformerが「単語の並び」を理解するための数学的な仕組みを、最も標準的な「Sinusoidal（正弦波）ポジショナル・エンコーディング」を例に解説します。

### 1. 基本的な考え方：足し算の魔法

各単語の埋め込みベクトル（Embedding）を $E$、その位置 $pos$ に対応する位置ベクトルを $PE$ とすると、モデルに入力される値は単にこれらを足し合わせたものになります。

$$Input = E + PE$$

なぜ上書きではなく「足し算」なのか。それは、高次元空間において、モデルは**意味を抽出する回路**と**位置を抽出する回路**を独立して学習できるからです。


### 2. 正弦波（サイン・コサイン）を使う理由

初期のTransformerでは、以下の式で  $PE$  を生成しました。

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

ここで、なぜわざわざ複雑な三角関数を使うのでしょうか？主な理由は2つあります。

#### ① 相対的な位置関係の表現

三角関数には「加法定理」があります。これにより、ある位置 $pos + k$ のベクトルは、位置 $pos$ のベクトルの線形変換として表現できます。
つまり、モデルは **「単語Aと単語Bがどれくらい離れているか」を、絶対的な番号ではなく「相対的な距離」として計算しやすくなる** のです。

#### ② 未知の長さへの対応（外挿性）

単に $1, 2, 3...$ と数値を振ると、学習時よりも長い文章が来たときに数値が巨大になりすぎてモデルがパニックを起こします。
周期関数（波）を使えば、値は常に $-1$ から $1$ の間に収まり、かつ波の組み合わせ（周波数の違い）によって、長い距離でも重複しない固有のパターンを作れます。


### 3. アナログ時計に例えるとわかりやすい

ポジショナル・エンコーディングの原理は「時計の針」に似ています。

* **短針（低い周波数）:** ゆっくり動き、大まかな位置（文のどのあたりか）を示す。
* **長針（高い周波数）:** 素早く動き、細かい位置（隣の単語との違い）を示す。

ベクトル内の各次元が異なるスピードで回転する「針」の役割を果たすことで、全ての「時刻（位置）」をユニークな状態として表現しています。


### 4. 進化した原理：RoPE（回転式位置埋め込み）

現在のLLM（Llama 3など）で使われている **RoPE (Rotary Positional Embedding)** は、この「回転」の概念をより直接的にアテンション計算に取り入れたものです。

* **原理:** 単語ベクトルそのものを、位置に応じた角度で「回転」させます。
* **利点:** 2つの単語のアテンション（内積）を計算したときに、**結果が「2つの単語間の相対的な距離」にのみ依存する**ようになります。これにより、位置情報の伝達効率が劇的に向上しました。

## 数理的な挙動

ここが一番面白い部分です。

ポジショナル・エンコーディング（PE）がアテンション計算、特に **「内積（Inner Product）」** の中でどのように作用し、モデルが「距離」を認識するのかを数式で紐解いていきましょう。

最も普及している **RoPE (Rotary Positional Embedding)** を例にするのが、現代のLLMの挙動を理解する上で最も近道です。


### 1. アテンションにおける「位置」の役割

標準的なアテンションでは、クエリ $q$ とキー $k$ の内積で関連度を計算します。

位置情報を付与した後のベクトルを $\tilde{q}_m$（位置 $m$）、$\tilde{k}_n$（位置 $n$）とすると、モデルが知りたいのは以下の値です。

$$\text{Score}(m, n) = \tilde{q}_m^\top \tilde{k}_n$$

ここで、**「位置の原理」**として理想的なのは、このスコアが**相対的な距離  $(m - n)$  だけに依存する**こと（相対的依存性）です。

### 2. RoPEの数式：複素数平面での回転

RoPEでは、2次元のベクトル $(x_1, x_2)$ を複素数 $z = x_1 + ix_2$ と見なします。位置 $m$ における回転操作は、オイラーの公式を用いて次のように表現されます。

$$\tilde{q}_m = q \cdot e^{im\theta}$$

$$\tilde{k}_n = k \cdot e^{in\theta}$$

ここで  $\theta$  は各次元に割り当てられた回転の「歩幅（周波数）」です。


### 3. なぜ「距離」が抽出できるのか（数式のマジック）

ここが最も重要なポイントです。
位置 $m$ のクエリと位置 $n$ のキーの内積を計算してみましょう。複素数の内積は、一方の共役複素数との積の実数部に相当します。

$$\tilde{q}_m^\top \tilde{k}_n = \text{Re}[ (q e^{im\theta}) \cdot (k e^{in\theta})^* ]$$

$$\tilde{q}_m^\top \tilde{k}_n = \text{Re}[ q \cdot k^* \cdot e^{i(m-n)\theta} ]$$

**この数式が意味すること：**

1. **絶対位置が消える**: $m$ 単独や $n$ 単独の情報ではなく、常に $(m - n)$ という「差」の形になっています。
2. **相対距離への依存**: アテンションスコアは、純粋に2つの単語がどれくらい離れているか（角度の差 $(m-n)\theta$）によって決まるようになります。
3. **内積の不変性**: 複素平面上で両方のベクトルを同じ角度だけ回転させても、それらの間の「角度差」は変わらないため、相対的な関係性が保存されます。


### 4. 行列形式での実装

実際のPyTorchなどのコードでは、複素数を使わずに回転行列  $R$ を用いて以下のように計算されます。

$$\begin{pmatrix} \tilde{q}_1 \\ \tilde{q}_2 \end{pmatrix} = 
\begin{pmatrix} \cos m\theta & -\sin m\theta \\ \sin m\theta & \cos m\theta \end{pmatrix}
\begin{pmatrix} q_1 \\ q_2 \end{pmatrix}$$

## 5. まとめ：数式がもたらす恩恵

* **減衰効果**:  距離 $(m-n)$ が大きくなると、異なる周波数の波が干渉し合い、自然とアテンションスコアが小さくなる（遠くの語には注目しにくくなる）という性質が生まれます。
* **外挿性**: 学習時に 2048 トークンまでしか見ていなくても、数式上は $m=4096$ の回転も定義できるため、長い文章への対応がスムーズになります。
ポジショナル・エンコーディングは、単に「番号を振る」作業ではなく、 **「アテンション計算という空間の中に、距離という名の幾何学的な構造を持ち込む」** 数学的工夫なのです。

## まとめ

本日はAIが言葉の並びを理解して処理する上で欠かせないポジショナル・エンコーディング（PE）について説明しました。
単純に加算しただけの位置情報ですが、LLMの中で何度も行われる行列積がされても情報が消去されずに残り続けます。

そして残る情報は各単語間の距離としてLLMに、"どのくらい離れているんだこれ？"という情報を提示します。

今回記事を、Transformerを理解する上でご参考頂ければ幸いです。


