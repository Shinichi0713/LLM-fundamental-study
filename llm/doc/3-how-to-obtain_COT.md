LLM（大規模言語モデル）が、単なる言語パターン復唱機から、論理的な推論や複雑な問題解決を行う「思考力」を獲得するまでの学習の流れは、主に以下の3つの段階を経て進展します。
この「思考力」は、特に**Chain-of-Thought (CoT)** の能力、つまり**段階的な推論プロセス**を生成する能力を指すことが多いです。


## 1. 大規模な事前学習 (Pre-training): 言語能力の基盤構築

この段階で、モデルは言語の基本的な構造と、世界に関する広範な知識を獲得します。
MLMやCLMという手法でLLMを訓練していきます。

| 段階 | 目的 | 仕組み | 思考力への貢献 |
| :--- | :--- | :--- | :--- |
| **事前学習** | 次の単語予測を通して、言語の統計的パターンと世界の知識を学習する。 | **自己教師あり学習:** 大量のテキストデータ（数兆トークン）から、穴埋めや次の単語予測（Causal Language Modeling）を繰り返す。 | 大量の知識を記憶し、単語間の複雑な関係性を理解する基盤を構築します。しかし、この段階ではまだ、**論理的な推論プロセス**を**自発的に**生成する能力は低いです。 |

## 2. 命令チューニング (Instruction Tuning): 人間の指示への適合

モデルに、人間が出す様々な「命令（指示）」を理解し、それに従った形式で応答する能力を学習させます。

| 段階 | 目的 | 仕組み | 思考力への貢献 |
| :--- | :--- | :--- | :--- |
| **SFT (Supervised Fine-Tuning) または命令チューニング** | モデルの出力を「人間が使いやすい形式」に整える。 | **教師あり学習:** 指示（プロンプト）とその回答（理想的な出力）のペアを集めたデータセットを用いて微調整を行う。 | モデルは「質問に答える」「要約する」「リストを作成する」といった**タスクの形式**を学習します。**CoTプロンプト**（例: 「ステップバイステップで考えなさい」）を与えられた場合、それに従った出力形式（推論ステップ）を生成する能力がわずかに向上します。 |

## 3. 推論能力の獲得と洗練 (Alignment & CoT Tuning)

この段階で、LLMは「指示に適切に従う」能力に加え、**複雑な論理的思考を実行する能力**を本格的に獲得し、強化します。
ここが近年のLLMの思考力を向上させた核心と言えるかもしれません。

### A. CoTデータの収集と微調整

思考力を獲得する最も直接的な手法は、**良質な推論データ**を用いた微調整です。

| 手法 | 仕組み | 思考力への貢献 |
| :--- | :--- | :--- |
| **CoT (Chain-of-Thought) データセットの学習** | 複雑な問題と、その**解法に至るまでの段階的な推論ステップ**が記述されたデータセットを用いてSFTを行う。 | モデルは、単なる最終結果だけでなく、**「どのように考えるべきか」**という推論のメカニズムを学習し、これが**Few-shot CoT**や**Zero-shot CoT**といった、より高度な推論能力の基盤となります。 |

### B. 人間のフィードバックによる調整 (Alignment)

最終的に、生成された推論プロセスが**人間の意図や論理**に合致するように調整されます。

| 手法 | 仕組み | 思考力への貢献 |
| :--- | :--- | :--- |
| **RLHF (Reinforcement Learning from Human Feedback)** | 人間がモデルの複数の出力（推論ステップを含む）の優劣を評価し、その評価を元に強化学習を用いてモデルを調整する。 | **人間が良しとする論理的・倫理的な推論**をするようにモデルを誘導します。思考のプロセスがより人間に理解しやすく、エラーが少ない形に洗練されます。 |
| **DPO (Direct Preference Optimization)** | RLHFの報酬モデルを省略し、人間の好みを直接モデルの学習に組み込む。 | 複雑なRL設定を避けつつ、LLMの推論出力の質を人間の基準に合わせて高め、**安定性**と**信頼性**を確保します。 |


### 思考力獲得のための最も重要な要素

「次の単語を予測する」という基本的な仕組み自体は変わりませんが、**「次に予測されるべき単語が、論理的推論の次のステップである」**ように学習を誘導することが、思考力獲得の本質です。

これが成功するためには、**CoTデータセットによるSFT（段階3-A）**と、**RLHF/DPO（段階3-B）**による人間とのアライメントが不可欠です。 


## 2番目命令チューニングの課題

先述の命令チューニングを行う場合、課題があります。
それは、意味が同じ言い回しが異なる文章を、正解とは見なさない点です。
つまり、命令チューニング（SFT）におけるクロスエントロピー損失の大きな限界は、**「正解がデータセットにある単一の言い回しに強く固定されてしまう」**ことです。

理想的な応答 $R$ が一つしか用意されていない場合、モデルは**表現の多様性**を無視し、その特定の単語列を生成するように強くバイアスされます。

言い回しが異なっても意味が同じ、あるいはより良い応答を正解として評価するための工夫は、次の学習ステップである**RLHF（人間のフィードバックによる強化学習）**にその核心があります。

命令チューニングの段階と、それを克服するRLHFの仕組みについて解説します。

### 1. 命令チューニング (SFT) の限界と対策

命令チューニングの段階では、モデルは厳密にデータセットのトークン列 $\{r_1, r_2, \dots, r_n\}$ を予測しようとします。

#### A. 損失の性質による限界

$$\text{Loss} \propto - \sum_{t} \log \mathbf{P}(r_t | \text{context})$$

この損失は、モデルがデータセットに含まれる正解トークン以外のトークンを予測した場合、**その予測の良し悪しに関わらず**高いペナルティ（高い損失）を与えます。

* **例:**
    * **データセットの正解:** 「とても良い提案だと思います。」
    * **モデルの出力:** 「素晴らしいアイデアです。」
    * **結果:** 意味は同じでも、トークンが異なるため、モデルは損失を受け、データセットの正解に近づくよう強制されます。

#### B. SFT段階での工夫（多様性の維持）

命令チューニングの段階でできる工夫は限られていますが、以下の方法で多様な表現に対応しようとします。

* **多様な正解の作成:** データセットの作成段階で、同じ命令 $I$ に対して、意味は同じだが言い回しが異なる複数の応答 $R_1, R_2, R_3, \dots$ を含めます。モデルはこれらの多様な表現を「正解の候補」として学習し、一つの特定の表現に固執しにくくなります。

### 2. 評価の多様性を実現するRLHFの仕組み

言い回しが異なっても「意味が通っている」「より自然である」「より有用である」といった**抽象的な品質**を評価する能力は、命令チューニングの次のステップである **人間のフィードバックからの強化学習 (RLHF)** によってモデルに組み込まれます。

RLHFは、以下の3つの主要なステップで構成されます。

#### ステップ 1: 報酬モデル (Reward Model, RM) の学習

これが、**言い回しが異なっても品質を評価する「評価関数」そのもの**です。

* **仕組み:**
    1.  LLM（SFT済み）に同じ命令を与え、複数の異なる応答（例：A, B, C）を生成させます。
    2.  人間がこれらの応答を比較し、「**最も良い**」「**次に良い**」といった順位付け（Preference）を行います。（例：B > A > C）
    3.  この人間の順位付けデータを用いて、**報酬モデル (RM)** という別の小さなモデルを教師あり学習で訓練します。
    4.  RMは、命令 $I$ と応答 $R$ のペアを入力として受け取り、その応答の**品質スコア $R(I, R)$** を出力するように学習されます。
* **効果:** RMは、特定の単語列の再現性ではなく、**人間が感じる応答の抽象的な品質**（有用性、無害性、論理性、自然な言い回しなど）を評価する「評価関数」となります。 

#### ステップ 2: 強化学習 (RL) によるLLMの調整

学習された報酬モデル $R(I, R)$ を使用して、LLM自体を強化学習で微調整します。

* **仕組み:**
    1.  LLMはランダムに文章を生成します（行動 $a$）。
    2.  生成された文章は、ステップ1で学習したRMによって即座に評価され、**報酬** $r = R(I, R)$ が与えられます。
    3.  LLMは、この報酬を最大化するようにパラメータを更新します（PPOアルゴリズムなどを使用）。
* **効果:** モデルは、**クロスエントロピー損失に縛られず**、報酬モデルが高く評価するような、**多様な言い回しでも品質の高い応答**を生成するように学習されます。

したがって、命令チューニングは「何を答えるべきか」の**知識と形式**を教える基盤ですが、「どのように答えるべきか」の**品質と多様性**を評価し、学習するのは**報酬モデル**を用いたRLHFの役割となります。 



