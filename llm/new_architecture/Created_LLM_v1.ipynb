{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "NTfX3oiZifoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQCVF_gfiduW"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Utilities: unfold_kv (same as earlier)\n",
        "# -----------------------------\n",
        "def unfold_kv(x: torch.Tensor, kernel_size: int, padding: int = 0):\n",
        "    \"\"\"\n",
        "    x: (B, H, T, D)\n",
        "    returns: (B, H, T, kernel_size, D)\n",
        "    \"\"\"\n",
        "    B, H, T, D = x.shape\n",
        "    x_img = x.permute(0, 1, 3, 2).reshape(B * H, D, 1, T)\n",
        "    x_unf = F.unfold(x_img, kernel_size=(1, kernel_size), padding=(0, padding), stride=(1, 1))\n",
        "    x_unf = x_unf.view(B * H, D, kernel_size, T)\n",
        "    x_unf = x_unf.permute(0, 3, 2, 1).reshape(B, H, T, kernel_size, D)\n",
        "    return x_unf\n",
        "\n",
        "# -----------------------------\n",
        "# RoPE helpers\n",
        "# -----------------------------\n",
        "def build_rope_cache(seq_len: int, dim: int, device=None, dtype=torch.float32):\n",
        "    \"\"\"\n",
        "    Build cos and sin caches for RoPE.\n",
        "    Returns:\n",
        "      cos: (seq_len, dim//2)\n",
        "      sin: (seq_len, dim//2)\n",
        "    Note: dim must be even (we treat pairs).\n",
        "    \"\"\"\n",
        "    assert dim % 2 == 0, \"RoPE head dim must be even\"\n",
        "    half = dim // 2\n",
        "    inv_freq = 1.0 / (10000 ** (torch.arange(0, half, dtype=dtype, device=device) / half))\n",
        "    positions = torch.arange(seq_len, dtype=dtype, device=device).unsqueeze(1)  # (seq_len,1)\n",
        "    angles = positions * inv_freq.unsqueeze(0)  # (seq_len, half)\n",
        "    cos = torch.cos(angles)  # (seq_len, half)\n",
        "    sin = torch.sin(angles)\n",
        "    return cos, sin\n",
        "\n",
        "def apply_rope_to_qk(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    \"\"\"\n",
        "    x: (B, H, T, D) where D is even\n",
        "    cos, sin: (T, D//2)\n",
        "    returns rotated x of same shape\n",
        "    \"\"\"\n",
        "    B, H, T, D = x.shape\n",
        "    half = D // 2\n",
        "    # Split interleaved: even/odd positions along last dim\n",
        "    x1 = x[..., :D:2]  # (B,H,T,half)\n",
        "    x2 = x[..., 1:D:2]  # (B,H,T,half)\n",
        "    # cos/sin -> (1,1,T,half) for broadcasting\n",
        "    cos_b = cos.unsqueeze(0).unsqueeze(0)  # (1,1,T,half)\n",
        "    sin_b = sin.unsqueeze(0).unsqueeze(0)\n",
        "    # rotate\n",
        "    x1c = x1 * cos_b - x2 * sin_b\n",
        "    x2c = x1 * sin_b + x2 * cos_b\n",
        "    # interleave back: [x1c0, x2c0, x1c1, x2c1, ...]\n",
        "    x_rot = torch.stack([x1c, x2c], dim=-1).reshape(B, H, T, D)\n",
        "    return x_rot\n",
        "\n",
        "# -----------------------------\n",
        "# Hybrid Sparse Attention with RoPE\n",
        "# -----------------------------\n",
        "class RoPEHybridSparseAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int = 8, window: int = 4, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
        "        head_dim = dim // num_heads\n",
        "        assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.window = window\n",
        "        self.kernel_size = 2 * window + 1\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # caches for RoPE will be created on forward based on seq_len\n",
        "\n",
        "    def forward(self, x: torch.Tensor, global_mask: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        global_mask: (B, T) bool\n",
        "        returns: out (B, T, D), full_attn (B, H, T, T)  # full_attn is for visualization\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        device = x.device\n",
        "        # 1) project\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (B,H,T,dh)\n",
        "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # 1.5) build RoPE cache and apply to q,k\n",
        "        cos, sin = build_rope_cache(T, self.head_dim, device=device, dtype=q.dtype)  # (T, dh/2)\n",
        "        q = apply_rope_to_qk(q, cos, sin)\n",
        "        k = apply_rope_to_qk(k, cos, sin)\n",
        "\n",
        "        # 2) extract local windows\n",
        "        K_windows = unfold_kv(k, kernel_size=self.kernel_size, padding=self.window)  # (B,H,T,win,dh)\n",
        "        V_windows = unfold_kv(v, kernel_size=self.kernel_size, padding=self.window)  # (B,H,T,win,dh)\n",
        "\n",
        "        # 3) local scores\n",
        "        scores_local = torch.einsum(\"bhtd,bhtwd->bhtw\", q, K_windows) / (self.head_dim ** 0.5)  # (B,H,T,win)\n",
        "\n",
        "        # 4) global part\n",
        "        if global_mask is None:\n",
        "            scores_global = None\n",
        "            K_global = None\n",
        "            V_global = None\n",
        "            global_idx_list = [torch.empty(0, dtype=torch.long, device=device) for _ in range(B)]\n",
        "            global_token_mask = None\n",
        "        else:\n",
        "            global_idx_list = []\n",
        "            maxG = 0\n",
        "            for b in range(B):\n",
        "                idx = torch.nonzero(global_mask[b], as_tuple=False).squeeze(-1)\n",
        "                if idx.numel() == 0:\n",
        "                    idx = torch.empty(0, dtype=torch.long, device=device)\n",
        "                global_idx_list.append(idx)\n",
        "                if idx.numel() > maxG:\n",
        "                    maxG = idx.numel()\n",
        "\n",
        "            if maxG == 0:\n",
        "                scores_global = None\n",
        "                K_global = None\n",
        "                V_global = None\n",
        "                global_token_mask = None\n",
        "            else:\n",
        "                # pad to maxG\n",
        "                K_global = torch.zeros(B, self.num_heads, maxG, self.head_dim, device=device, dtype=q.dtype)\n",
        "                V_global = torch.zeros(B, self.num_heads, maxG, self.head_dim, device=device, dtype=q.dtype)\n",
        "                global_token_mask = torch.zeros(B, maxG, dtype=torch.bool, device=device)\n",
        "                for b in range(B):\n",
        "                    idx = global_idx_list[b]\n",
        "                    if idx.numel() == 0:\n",
        "                        continue\n",
        "                    kg = k[b, :, idx, :]  # (H, G_b, dh)\n",
        "                    vg = v[b, :, idx, :]\n",
        "                    G_b = kg.shape[1]\n",
        "                    K_global[b, :, :G_b, :] = kg\n",
        "                    V_global[b, :, :G_b, :] = vg\n",
        "                    global_token_mask[b, :G_b] = True\n",
        "\n",
        "                scores_global = torch.einsum(\"bhtd,bhgd->bhtg\", q, K_global) / (self.head_dim ** 0.5)\n",
        "                # mask padded later\n",
        "\n",
        "        # 5) combine local and global\n",
        "        if scores_global is None:\n",
        "            attn_local = F.softmax(scores_local, dim=-1)\n",
        "            ctx_local = torch.einsum(\"bhtw,bhtwd->bhtd\", attn_local, V_windows)\n",
        "            out_heads = ctx_local  # (B,H,T,dh)\n",
        "            full_attn = torch.zeros(B, self.num_heads, T, T, device=device, dtype=q.dtype)\n",
        "            # fill local-only full_attn\n",
        "            for t in range(T):\n",
        "                left = max(0, t - self.window)\n",
        "                right = min(T, t + self.window + 1)\n",
        "                win_len = right - left\n",
        "                # attn_local[..., t, :win_len] -> place at positions left:right\n",
        "                full_attn[..., t, left:right] = attn_local[..., t, :win_len]\n",
        "        else:\n",
        "            # mask padded global slots\n",
        "            gmask = global_token_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,G)\n",
        "            scores_global = scores_global.masked_fill(~gmask, float(\"-1e9\"))\n",
        "\n",
        "            scores_cat = torch.cat([scores_local, scores_global], dim=-1)  # (B,H,T, win+G)\n",
        "            attn_cat = F.softmax(scores_cat, dim=-1)\n",
        "            attn_cat = self.dropout(attn_cat)\n",
        "\n",
        "            w_local = attn_cat[..., : self.kernel_size]\n",
        "            w_global = attn_cat[..., self.kernel_size :]\n",
        "\n",
        "            ctx_local = torch.einsum(\"bhtw,bhtwd->bhtd\", w_local, V_windows)\n",
        "            ctx_global = torch.einsum(\"bhtg,bhgd->bhtd\", w_global, V_global)\n",
        "            out_heads = ctx_local + ctx_global\n",
        "\n",
        "            # build full_attn for visualization\n",
        "            full_attn = torch.zeros(B, self.num_heads, T, T, device=device, dtype=q.dtype)\n",
        "            for b in range(B):\n",
        "                gidx = global_idx_list[b]\n",
        "                for t in range(T):\n",
        "                    left = max(0, t - self.window)\n",
        "                    right = min(T, t + self.window + 1)\n",
        "                    win_len = right - left\n",
        "                    # local part\n",
        "                    full_attn[b, :, t, left:right] = w_local[b, :, t, :win_len]\n",
        "                    # global part -> assign per actual indices\n",
        "                    if gidx.numel() > 0:\n",
        "                        G_b = gidx.numel()\n",
        "                        full_attn[b, :, t, gidx] += w_global[b, :, t, :G_b]\n",
        "\n",
        "        # 6) merge heads & out proj\n",
        "        out = out_heads.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out, full_attn\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Small Transformer Encoder block using RoPEHybridSparseAttention\n",
        "# -----------------------------\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_dim, window=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = RoPEHybridSparseAttention(dim=dim, num_heads=num_heads, window=window, dropout=dropout)\n",
        "        self.ln1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.ln2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, global_mask=None):\n",
        "        # Attention (RoPE + Local+Global)\n",
        "        residual = x\n",
        "        x_ln = self.ln1(x)\n",
        "        attn_out, _ = self.attn(x_ln, global_mask=global_mask)  # attn returns (out, full_attn)\n",
        "        x = residual + self.dropout(attn_out)\n",
        "\n",
        "        # MLP\n",
        "        residual = x\n",
        "        x_ln2 = self.ln2(x)\n",
        "        x = residual + self.mlp(x_ln2)\n",
        "        return x\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Encoder-style LLM (small)\n",
        "# -----------------------------\n",
        "class SparseRoPEEncoderLM(nn.Module):\n",
        "    def __init__(self, vocab_size, dim=256, num_heads=8, num_layers=4, mlp_dim=512, window=4, dropout=0.1, tie_word_embeddings=True):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(dim=dim, num_heads=num_heads, mlp_dim=mlp_dim, window=window, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_final = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n",
        "        if tie_word_embeddings:\n",
        "            # weight tying\n",
        "            self.lm_head.weight = self.token_emb.weight\n",
        "\n",
        "    def forward(self, input_ids, global_mask=None, return_attn=False):\n",
        "        \"\"\"\n",
        "        input_ids: (B, T) long\n",
        "        global_mask: (B, T) bool\n",
        "        returns: logits (B, T, V) ; optionally optionally return last layer full_attn (B, H, T, T)\n",
        "        \"\"\"\n",
        "        x = self.token_emb(input_ids) * (self.dim ** 0.5)  # scale\n",
        "        full_attn_last = None\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, global_mask=global_mask)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.lm_head(x)  # (B, T, V)\n",
        "        if return_attn:\n",
        "            # get full_attn from the last attention module by running one more call to get full_attn\n",
        "            # (slightly wasteful but useful for visualization)\n",
        "            # To avoid double-computation, we can call last layer's attn directly on ln input:\n",
        "            with torch.no_grad():\n",
        "                # reconstruct last layer's attn input: apply ln on pre-last activation\n",
        "                pre_last = x  # note: in this simple code we don't have easy access to pre-ln input; skip\n",
        "                full_attn_last = None\n",
        "        return logits, full_attn_last\n",
        "\n",
        "# -----------------------------\n",
        "# Quick test snippet\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "    B, T, D = 1, 48, 128\n",
        "    H = 8\n",
        "    window = 4\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = RoPEHybridSparseAttention(dim=D, num_heads=H, window=window).to(device)\n",
        "    x = torch.randn(B, T, D, device=device)\n",
        "\n",
        "    # set a couple of global tokens\n",
        "    global_mask = torch.zeros(B, T, dtype=torch.bool, device=device)\n",
        "    global_mask[0, 0] = True\n",
        "    global_mask[0, 12] = True\n",
        "\n",
        "    out, full_attn = model(x, global_mask)\n",
        "    print(\"out.shape=\", out.shape)            # (B,T,D)\n",
        "    print(\"full_attn.shape=\", full_attn.shape)  # (B,H,T,T)\n",
        "\n",
        "    # visualize head 0\n",
        "    att = full_attn[0, 0].detach().cpu().numpy()  # (T,T)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(att, aspect=\"auto\")\n",
        "    plt.colorbar()\n",
        "    plt.title(\"RoPE + Hybrid Sparse Attention (head 0)\")\n",
        "    plt.show()\n"
      ]
    }
  ]
}