## RNN
まずは時系列情報を扱う初期の頃のニューラルネットワークであるRNNについて説明を行います。

#### RNN の概要

RNN（Recurrent Neural Network）は、**時系列データや文章のように「順番が重要なデータ」を扱うためのニューラルネットワーク**です。

従来のニューラルネットワーク（MLP / CNN）は、

* 入力を **すべて同時に処理**し
* **順序** という概念を持っていません

そのため、以下のような「過去 → 現在 → 未来」という流れを理解するのが苦手でした。

RNNはこれを解決するため、**1単語ずつ（1時刻ずつ）順番に入力を処理し、過去の情報（内部状態）を記憶しながら次の処理に進む構造**を持ちます。

図で表すとこうです：

![1765096384766](image/2-RNN/1765096384766.png)

* x1, x2, x3 …：文章や時系列の各要素
* h1, h2, h3 …：RNNが内部に保持する「記憶」

これにより、

✔ 「文章の前半の意味を踏まえて後半を処理する」
✔ 「1秒前の値と比較して判断する」

などが可能になります。

#### RNN が解決したかった課題

__課題1：順番を無視してしまう問題__

従来のニューラルネットワークは、文章を「順不同の特徴量」として扱っていました。

例えば、

* 「私は**犬が好き**です」
* 「犬は**私が好き**です」

は単語は似ていても意味が全く違います。

**順序（トークンの並び）なしでは意味を正確に理解できない**という問題がありました。

RNNは、

> 「前の単語を踏まえて次の単語を処理する」
> という仕組みを導入することで、この問題を解決しました。

__課題2：時系列データの依存関係を捉えられない__

従来のモデルでは、

* 昨日の気温 → 今日の気温
* 1フレーム前の音声 → 今の音声
  のような **連続した関係** を表すことが困難でした。

RNNは内部状態 h(t) に過去の情報を蓄積することで、

- 機械翻訳
- 音声認識
- 株価予測
- センチメント分析

など、「時間の流れで意味が変化するデータ」の扱いを可能にしました。

__課題3：入力長が可変のデータを扱えない__

従来のニューラルネットワークは、固定長のベクトルしか扱えません。

しかし、実際のデータは：

* 5単語の文もある
* 20単語の文もある
* 音声の長さもバラバラ

RNNは **1ステップずつ処理を繰り返す仕組み**のため、
入力の長さが自由でした。


#### RNN が持ち込んだ革新性（まとめ）

| 課題                | RNNがもたらした解決       |
| ----------------- | ----------------- |
| 単語の順序を扱えなかった      | 過去の情報を保持しながら順番に処理 |
| 時系列の依存関係を捉えられなかった | h(t) に時間に沿った記憶を保持 |
| 可変長データが扱えなかった     | 任意の長さのデータを処理可能に   |
| 文章・音声などの連続データが困難  | 翻訳・音声認識などが初めて高精度に |

---

# ◆ さらに分かりやすい例（直感的理解）

**文脈を覚えていないと意味が変わる例：**

▼文章A
「私は昨日スーパーで**肉を買った**。
その肉を今日の夕飯で食べた。」

▼文章B
「私は昨日スーパーで**肉を買った**。
その**犬を**今日の夕飯で食べた。」

後半だけ見たら
「犬を食べた」？？
意味が通らない。

RNNのように **前の文脈を覚えながら読む** ことが自然な理解につながります。

---

# ◆ まとめ

RNNは、

> **「過去の情報を記憶しながら順番に処理できるネットワークを作る」**

という目的で開発されました。

その結果、

✔ 文章
✔ 音声
✔ 時系列データ

など、人間にとって自然な「流れ・順序」のある情報を扱えるようになり、
現代の LSTM・GRU・Transformer、そして LLM の基盤となっていきました。

---

必要であれば、

* LSTM・GRU が RNN の欠点をどう改善したか
* Transformer が RNN を超えた理由
* 具体的な応用例
  も詳しく説明できます。
