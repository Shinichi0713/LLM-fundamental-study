# VLMを理解するには？
VLM（Vision-Language Model：画像言語モデル）は画像を認識して考えることが出来るAIモデルです。
現実世界でAIを使った機械を動作させる場合欠かせないAI技術となります。

このVLMを理解するためには、単にLLMの知識だけでなく、画像を言語と同じ「シーケンス」として扱うための**画像処理技術（ViT）**と、**異なるモダリティを統合する仕組み（クロスアテンション）**を学ぶことが重要です。

以下の3つのステップで知識を深めることをお勧めします。


## 1. 🌐 基盤となる技術の習得

まず、VLMが依存している核となる技術を固めます。

* ### Transformerアーキテクチャ
    LLMの学習で得た知識を再確認します。特に、**自己注意機構（Self-Attention）**がトークン間の関係をどう処理しているかを理解します。VLMでは、この機構が**画像パッチ間**の関係を理解するために利用されます。

* ### 埋め込み（Embedding）とシーケンス化
    VLMでは、テキストだけでなく画像もシーケンス（トークン列）として扱われます。
    * **テキスト**: 単語/サブワード $\rightarrow$ 埋め込みベクトル。
    * **画像**: 画像 $\rightarrow$ パッチ $\rightarrow$ **パッチ埋め込みベクトル**。


## 2. 🖼️ Vision側のインプット処理：ViTの理解

VLMが画像を扱う上で主流となっているのが **ViT (Vision Transformer)** です。これを理解することがVLMの画像処理の根幹を理解することに直結します。

* ### Vision Transformer (ViT)
    ViTは、画像を$16\times 16$などの小さな**パッチ**に分割し、それらを線形層で埋め込んで**「画像トークン」**として扱います。
    * **パッチ化**: 画像を空間情報を持つトークンのシーケンスに変換する仕組みを学びます。
    * **位置情報**: テキストと同様に、パッチにも**位置エンコーディング**が適用されることを理解します。
* ### 従来型CNNとの違い
    画像認識の主流であったCNN（畳み込みニューラルネットワーク）と比較し、ViTが自己注意機構のみでどのように画像の特徴を抽出しているかを理解します。


## 3. 🧩 モダリティの統合と代表的なアーキテクチャ

VisionとLanguageの情報を単一のモデルで扱うための具体的な手法と、代表的なモデルの構造を学習します。

### 3.1. クロスアテンションの役割
VLMにおいて最も重要な要素の一つが**クロスアテンション（Cross-Attention）**です。

* **処理**: 一方のモダリティ（例：テキスト）のクエリ（Q）を使って、もう一方のモダリティ（例：画像）のキー（K）とバリュー（V）を参照し、情報を統合します。
* **効果**: テキストが画像の中でどの領域が重要かを尋ね、画像側がそれに対応する情報を返す、という相互作用を可能にします。

### 3.2. 主要なVLMアーキテクチャ
VLMの具体的な実装方法を知ることで、技術の応用方法が明確になります。
参考となる代表的なVLMの例を挙げてみます。

| アーキテクチャ | 構造のタイプ | 特徴と学習方法 |
| :--- | :--- | :--- |
| **CLIP** | デュアルエンコーダ型 | 画像とテキストをそれぞれ独立したエンコーダで符号化し、出力されたベクトル間の類似度を対照学習（Contrastive Learning）で一致させます。 |
| **BLIP/ViLT** | 統合型/Fusion型 | 画像トークンとテキストトークンを早い段階で統合し、**クロスアテンション**を使って相互に情報をやり取りさせます。 |
| **Flamingo** | 結合型（Perceiver-style） | 訓練済みのLLMをフリーズしたまま、画像エンコーダからの情報を特定の層でLLMに注入する構造（Gate Attention）を持ちます。 |

### まとめ

LLM、ViT、クロスモーダリティが主要な技術です。
この3つを合わせてできたのがVLMです。

---

著者は暇があればgithubいじってます。 LLM関係でしたら以下が作品です。 宜しければご覧ください。

https://github.com/Shinichi0713/LLM-fundamental-study
