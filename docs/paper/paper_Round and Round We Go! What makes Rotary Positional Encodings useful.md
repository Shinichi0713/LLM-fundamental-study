本稿はLLMの論文紹介です。
RoPEは近年のLLMの位置情報を保持する機構の主流として使われている手法です。
LLMの通例として、中で行われていることは想像で、実際はどうか分からないという課題に対して解析をしたという論文です。

## 論文の概要

「**Round and Round We Go! What makes Rotary Positional Encodings useful?**」は、AI研究団体の **EleutherAI**（GPT-NeoXなどを開発したチーム）によって公開された技術記事（ブログポスト）です。

このタイトルは、**RoPE (Rotary Positional Embeddings)** の有用性を、数式と直感的なイメージを用いて深く解説したもので、現在多くのLLM開発者がRoPEを理解するための「教科書」のような存在になっています。

この論文（記事）の主な内容は、以下の3点に集約されます。

### 1. 既存の手法の「いいとこ取り」であることの解説

この記事では、RoPEがこれまでの2つの主流な手法の課題を解決するものであることを説明しています。

* **絶対位置エンコーディング (Absolute PE)**:
  * BERTやGPT-2で使用。計算は速いが、「相対的な関係（距離）」をモデルが直接理解するのが難しい。
* **相対位置エンコーディング (Relative PE)**:
  * T5などで使用。「距離」を直接教えるので精度は良いが、実装が複雑で計算コストがかかる（推論時のキャッシュ効率が悪い）。

**RoPEの革新性**:
「絶対位置の座標を与えている（計算が速い）」のに、Attentionの計算（内積）をすると「相対位置の情報しか残らない（精度が良い）」という、**魔法のような性質**を持っていることを示しました。

### 2. 「回転」による直感的な理解

タイトルにある "Round and Round We Go"（ぐるぐる回る）の通り、RoPEの核心が「ベクトルの回転」にあることを説明しています。

* 単語ベクトルを2次元平面上の矢印と見なす。
* 位置が進むごとに、その矢印を少しずつ回転させる。
* **重要な発見**: 2つの矢印の内積（＝Attentionスコア）をとると、それぞれの絶対的な角度は相殺され、**「2つの矢印の角度の差（相対距離）」だけが計算結果に残る**。

これにより、「なぜRoPEが相対位置を保存できるのか」を幾何学的に証明しています。

### 3. 長距離依存性の自然な減衰 (Long-term Decay)

この記事で強調されているもう一つの重要な点は、RoPEが**「遠くの単語との関係を自然に弱める」**という特性を持っていることです。

* RoPEでは、高周波（速く回転する）成分と低周波（遅く回転する）成分があります。
* 距離が離れすぎると、回転の位相が激しくずれるため、内積の期待値（Attentionスコア）が小さくなります。
* これにより、明示的にマスクしなくても、モデルは**「近くの情報を重視し、遠くの情報は参考程度にする」**という、自然言語処理にとって望ましいバイアスを自動的に獲得できると説明しています。

## イントロ

この文章は、Transformerモデルで広く採用されている**RoPE (Rotary Positional Encodings)** に関する、現在の理解の課題と、その実際の挙動に関する研究の意義を述べています。

### 要約：RoPEの有用性に関する研究課題

#### 1. 背景と現状

* **位置エンコーディングの多様性**: TransformerのAttention機構に位置情報（絶対位置、相対位置、バイアスなど）を与える方法はいくつかあります。
* **RoPEの普及**: 現在、特に**LLaMA 3**や**Gemma**などの大規模言語モデル（LLM）でRoPEが最も広く採用されています。
* **RoPEのメカニズム**: RoPEは、クエリとキーのベクトルを2次元のチャンクに分割し、それぞれを異なる周波数で回転させるという、効率的かつ幾何学的なアプローチです。

#### 2. 本研究の動機と課題

* **有用性の不明確さ**: RoPEがこれほど広く採用されているにもかかわらず、**なぜこの手法がTransformerモデルにとって有益なのか**、具体的な理由やメカニズムはまだ十分に解明されていません。
* **減衰特性の疑問**: RoPEの提唱者（Su et al.）は、相対距離が離れるにつれてAttention係数が自然に**減衰する（Decay）**ことをRoPEの主要な利点としています。
  * しかし、この主張はクエリとキーが**一定（constant）**であるという前提に基づいています。
  * 本研究では、実際にはこの減衰が**発生しない**状況が多く存在し、**Gemma 7B**のAttentionヘッドがこの非減衰特性を利用している場合があることを見出しました。
* **周波数の使い分けの謎**: RoPEの異なる回転周波数（トークンあたり1ラジアンの高速回転から、1/10,000ラジアンの低速回転まで）が、LLM内で**正確にどのように利用されているのか**も未解明な、興味深い課題です。最高速の周波数は、小さな位置の変動に極めて敏感であるため、情報キャリアとしては不向きに見える、という疑問点も挙げられています。

## RoPEの減衰特性に関する要約と主張

この節が今回論文の主旨です。

### 1. 通説の問題点と本研究の主張

| 項目                 | 通説（Su et al.）                                                                                                                        | 本研究の主張                                                                                                      |
| :------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------- |
| **前提条件**   | クエリ ($\mathbf{q}$) とキー ($\mathbf{k}$) は**一定（constant）**のベクトルである、という単純化された仮定に基づいて計算されている。 | この仮定は**非現実的**な過剰な単純化であり、無視できない。                                                  |
| **減衰の有無** | 既に整列したクエリとキー（aligned）間の内積は、距離と共に減衰する。                                                                      | 元々整列していないクエリとキー（misaligned）の内積は、相対距離の増加と共に**増加する可能性もある**。        |
| **意義**       | この減衰特性は、ベース波長($\theta$)の調整など、RoPEの改良の根拠として利用されてきたため、減衰しないケースを指摘することが重要である。 | RoPEは、減衰しない場合も含め、Transformerに**特定な相対距離に注目するためのロバストな手段**を提供している。 |

### 2. 数学的根拠と実験結果

本研究は、2つの命題と合成実験を通じて、減衰特性が必ずしも成立しないことを示しています。

![1763886802683](image/paper_RoundandRoundWeGo!WhatmakesRotaryPositionalEncodingsuseful/1763886802683.png)

#### 🔹 命題 3.1: 任意距離での最大化が可能

RoPEは、任意のクエリ ($\mathbf{q}$) と任意の相対距離 $r$ が与えられたとき、**その距離 $r$ でSoftmax値が最大になるようなキー ($\mathbf{k}$) を見つけることが可能**であることを示します。

* **意味**: RoPEは、距離が離れるほどスコアが下がるという制約を持たず、モデルが**任意の相対位置に強く注目する**ことを許容しています。実際、**Gemma 7B**は、このメカニズムを利用して特定の位置に注目するヘッドを構成していることが後に示されます。

#### 🔹 命題 3.2: ガウス分布からのサンプリングでは減衰しない

クエリ ($\mathbf{q}$) とキー ($\mathbf{k}$) を標準の多変量ガウス分布 $\mathcal{N}(\mathbf{0}, \mathbf{I})$ から独立にサンプリングした場合、RoPE適用後のアクティベーションの**期待値は相対距離 $r$ にかかわらず $0$ になる**ことを示します。

$$
\mathbb{E}_{\mathbf{q},\mathbf{k} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}[\mathbf{q}^\top \mathbf{R}_r \mathbf{k}] = 0
$$

* **意味**: 現実的なランダムなベクトル（ガウス分布）を用いた場合、理論上、**相対距離によるアクティベーションの減衰傾向は存在しない**ことが示されます。

#### 🔹 実験結果 (図2)

合成実験の結果、以下の事実が確認されました。

* (a) **「全てが1」の一定ベクトル**を用いた場合: 通説通り、アクティベーションはある程度減衰する傾向が見られる。
* (b) **ガウス分布からサンプリングしたランダムベクトル**を用いた場合: 相対距離が増加しても、アクティベーションに**明確な減衰傾向は見られない**。

### 主張

RoPEの構造を正当化するために使われてきた「減衰特性」は、単純な一定ベクトルという**非現実的な前提**の下でしか成立せず、より現実に近いランダムベクトル（ガウス分布）を用いた場合には成立しないことが、理論的・実験的に示されました。RoPEの真の有用性は、減衰ではなく、**特定の相対距離に柔軟かつロバストに注目できる能力**にあると主張されています。

このセクションは、Gemma 7Bモデルを分析し、**RoPEの異なる回転周波数（frequencies）がAttention計算でどのように利用されているか**を探求した結果を述べています。

## RoPEにおける周波数の利用方法の要約

### 1. 研究の目的と前提

* **問題**: RoPEの周波数は、トークンあたりの変化が大きい**高周波**（ノイズのように振る舞う可能性がある）から、変化が非常に小さい**低周波**まで多岐にわたります。LLMがこれらの周波数をどのように活用しているのかは不明です。
* **測定方法**: コーシー＝シュワルツの不等式 $| \langle \mathbf{q}, \mathbf{k} \rangle | \leq \| \mathbf{q} \| \| \mathbf{k} \|$ に基づき、クエリとキーの対応する周波数成分の **L2ノルム（$\|\mathbf{k}\|$)** の平均を測定することで、その周波数成分がAttentionの内積に与える**影響力の上限**を評価しました。

### 2. Gemma 7Bの周波数利用パターンの発見 (図3)

* **主要な発見**: Gemma 7Bの全ての層（Layer）を通じて、学習は平均的に**最も低い周波数**に対して**最も高いノルム（norm）**を割り当てています。
  * これは、LLMがAttentionのアクティベーションに最も大きな影響を与えるために、最も**安定している（変動が少ない）低周波成分**を優先的に使用していることを示唆しています。
* **高周波の利用**: 低周波が優先される一方で、**最初の層と最後の層**では、平均的に**高周波**の利用がいくらか見られました。
* **RoPE固有の現象**: このノルムの分布パターン（低周波への偏り）は、回転されるクエリ/キーベクトルでのみ観測され、回転されない**値ベクトル（Value vectors）**では観測されませんでした。これは、中～高周波の「チャンク」が Attention の内積計算にほとんど寄与しないと学習が判断し、そのノルムを意図的に **$0$ に近づけている**結果であると結論付けられています。

### 3. アテンションヘッドごとの分析 (図4)

* **特異なヘッド**: 第1層の16個のAttentionヘッドを個別に分析したところ、**ヘッド5とヘッド8**が特に**高周波**を利用していることが分かりました。
  * これらのヘッドは、後のセクションで**位置（Positional）に強く注目するヘッド**に対応していることが示されます。
* **疎な利用パターン**: ノルムが高い帯域（Band）と低い帯域が明確に分かれており、周波数利用が**疎（Sparse）な性質**を持つことが強調されています。
  * これは、フィードフォワード層が疎な辞書ルックアップテーブルとして機能するという先行研究（Geva et al., 2021）の観察と一致しており、これらの帯域が何らかの**疎なクエリ/キーの意味的マッチング**に使用されている可能性が指摘されています。

![1763886999225](image/paper_RoundandRoundWeGo!WhatmakesRotaryPositionalEncodingsuseful/1763886999225.png)

## 高周波数の利用：位置特化型アテンションの要約

このセクションは、通常はモデルに敬遠される**RoPEの最高周波数**が、特定のAttentionヘッドでどのように利用されているか、またその意義について分析した結果を述べています。

### 1. 高周波数の役割と位置特化型ヘッドの発見

* **高周波数の特異性**: RoPEの最高周波数は、その変動の激しさから、通常はGemma 7Bによってほとんど使用されない傾向があります。
* **位置特化型ヘッド**: 本研究では、Attentionヘッドの中には、そのアクティベーションが**純粋に相対位置にのみ基づくパターン**を示すものがあり、これらのヘッドは**主に最高周波数**を利用していることを発見しました。
* **構築の可能性**: RoPEは、このような**「極めてシャープな」**位置特化型アテンションパターンを、理論上、**任意に構築できる**ことを証明します（NoPEでは構築不可能）。Gemma 7Bは、実際にこの構造を学習しているように見えます。

### 2. Gemma 7Bにおける具体的なパターン (図5, 6)

* **パターン例**: Gemma 7Bで確認された純粋な位置特化型アテンションパターンには以下の例があります。
  * **「対角線」ヘッド (Diagonal head)**: 最終層などで見られ、トークンが**自分自身**にのみ強く注目するパターン（残差接続のように機能する）。
  * **「直前トークン」ヘッド (Previous-token head)**: 第1層などで見られ、トークンが**直前のトークン**にのみ強く注目するパターン。
* **周波数利用**: 図6が示す通り、これらの位置特化型ヘッド（例：対角線ヘッド）は、他のヘッドが避ける**非常に高い周波数に依存**してAttentionを計算しています。この傾向は、位置特化型ヘッドを通じて一貫して見られるパターンです。
* **図4との関連**: 以前のセクションで高周波の利用が指摘されたヘッド5とヘッド8は、それぞれ純粋な対角線ヘッド（ヘッド5）と直前トークンヘッド（ヘッド8）に対応していました。

![1763887096913](image/paper_RoundandRoundWeGo!WhatmakesRotaryPositionalEncodingsuseful/1763887096913.png)

### 3. 結論と重要性

* **RoPEの重要な能力**: 最高周波数を利用して**純粋な位置関係**のみに注目するヘッドを構築できる能力は、**自己回帰生成の最適化**や、**構造的な一般化**（先行研究でも有用性が示唆されている）を試みる上で重要であると考えられます。
* **学習された振る舞い**: 対角線アテンションのように、セマンティクス（意味）を無視し、純粋に位置のみに基づくパターンをなぜLLMが学習するのかは、今後の興味深い研究課題であると述べています（一部にはトレーニング上の「バグ」である可能性も示唆）。

## 低周波数の利用：セマンティックアテンションの要約

### 1. 低周波数の役割と特徴

* **優先的な利用**: Gemma 7Bでは、クエリとキーの埋め込みに割り当てられるノルムの**大半が低周波数**に向けられています（セクション4で観察された通り）。
* **セマンティクス（意味）検出**: 低周波数は、**トークンの相対距離による影響を最も受けにくいため**、トークンの**意味（セマンティクス）に関連する情報**を検出するのに最も有用であると論じられています。
* **不変性の限界**: ただし、低周波数でも非常に長い相対距離に対しては完全に不変ではなく、最終的にはベクトルのミスアライメント（ずれ）が生じます。

### 2. コンテキスト長とベース波長の問題

* **影響の最小化**: 低周波数が有用なのは、**相対距離による内積への影響が最も少ない**周波数だからです。
* **LLaMA 3の事例**: **LLaMA 3**がベース波長 $\theta$ を従来の10,000から**500,000**に増やしたことの有用性は、この点にあると考えられます。
  * 標準の波長（10,000）では、128kトークンという長大なコンテキスト長では約2.04回転してしまい、トークンが誤ってミスアライメントされ、汎化性能が低下する可能性があります。
  * LLaMA 3の変更は、コンテキスト長が長くなるにつれて、RoPEの$\theta$ベース波長もそれに合わせて増加させる必要があることを示唆しています。

### 3. セマンティック特化型ヘッドの分析（図7）

* **アポストロフィ検出ヘッド**: 図7では、興味深い**セマンティックヘッド**の例が特定されています。このヘッドは、アポストロフィ（`’`）トークンの後に現れるトークンが、そのアポストロフィに注目するように動作します。
  * **例**: トークン列 `[BOS, I, ’, m]` のうち、トークン `m` のアテンションはアポストロフィ `’` に向かいます。それ以外のトークンは `[BOS]` トークンに注目します。
* **周波数利用の混合**: このヘッドは、意味的（アポストロフィ検出）かつ位置的（直前のトークンのみ）な性質を持ちます。
  * **高周波数**: 直前のトークンに現れるアポストロフィという**位置的な近接性**を検出するために、**高周波数**が利用されていることが示されています。
  * **低周波数**: アポストロフィがない場合に**`[BOS]` トークンにロバストに注目する**ために、**低周波数**の濃い帯域が使用されていると考えられています。

![1763887177426](image/paper_RoundandRoundWeGo!WhatmakesRotaryPositionalEncodingsuseful/1763887177426.png)

### 4. 低周波数の限界

* **ロバスト性の限界**: 低周波数がセマンティック情報を検出するのに役立つ一方で、非常に長いコンテキストに対しては、その「セマンティックバンド」がロバストではないことが示されています。
* **理論的証明**: 一つの周波数（2次元ケース）で、**アテンションアクティベーションが誤って大きくなる**ような相対距離が存在することを証明できるとしています。これは、長いコンテキストにおいては、この種のセマンティックバンドがロバストに機能し続けることはできないという懸念を示唆しています。

## 結論の要約：RoPEの挙動に関する新しい理解

この論文の結論は、RoPE（Rotary Positional Encodings）の挙動、特にその利点に関する通説を検証し、RoPEの各周波数帯域がLLM内で果たす役割を明確にした点にあります。

### 1. 通説への異議と理論的発見

* **減衰特性への反論**: RoPEの利点として広く信じられてきた「相対距離が増加するにつれてAttentionウェイトが減衰する」という通説に異議を唱えました。
  * キーとクエリが**ガウスランダムベクトル**であると仮定した場合、この減衰は証明上、発生しません。
  * さらに、RoPEは、距離の大小に関わらず、**任意の相対距離でAttentionウェイトが最大になる**ような構造を構築できることを示しました。これらの観察結果は、RoPEを正当化する従来の直感と矛盾します。

### 2. 周波数帯域ごとの役割の解明

* **高周波数の役割（位置特化型Attention）**:
  * 高周波数帯域は、**純粋な位置情報**に基づくAttentionパターン（位置特化型回路）を構築するために使用されているという証拠を提示しました。
  * この振る舞いを保証する理論的な構造を提供し、Gemma 7BのAttentionヘッド内に**酷似した構造**が存在することを特定しました。
  * **NoPE（位置エンコーディングなし）**では同様の位置特化型Attentionを構築できないことを示し、RoPEのこの能力の重要性を強調しました。
* **低周波数の役割（意味特化型Attention）**:
  * 低周波数帯域は、主に**意味情報（セマンティック）**に基づくAttentionを検出するために使用されていることを示しました。
  * これらの低周波数からRoPEの回転を取り除くことで、**意味的Attentionの能力が向上し**、特に**長いコンテキスト**に対する汎化性能が改善する可能性があると主張しました。これはGemma 2Bを用いたアブレーション研究（一部要素を取り除いた実験）で検証されています。

### 3. 全体的な貢献

本研究は、RoPEの挙動について**より繊細で詳細な理解**を提供しました。この知見が、LLMが位置エンコーディングをどのように利用しているかについての理解を深め、特に**長文コンテキスト**における性能向上につながることを期待しています。
