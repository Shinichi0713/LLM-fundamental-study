# Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

NeurIPS 2025でBest Paper Awardを受賞した「Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free」の論文、とりあえず和訳してみました。

arxivのリンク先: https://arxiv.org/pdf/2505.06708

## ゲーティングメカニズムに関する研究論文の和訳と要約

この文章は、ニューラルネットワーク、特に最新のアテンションメカニズムにおける**ゲーティング（Gating Mechanism）**の役割とその本質的な価値を深く掘り下げた研究の背景と主要な発見を説明しています。

### 概要 (Gating Mechanism: Function and Impact)

ゲーティングメカニズムは、ニューラルネットワークにおいて情報フローを制御し、勾配伝播を改善するための確立された手法です。初期のLSTM、Highway Network、GRUといったアーキテクチャから、State-Space Models (SSM) や最新のアテンションメカニズムに至るまで、広く採用され成功を収めています。しかし、その広範な採用と実証的な成功にもかかわらず、ゲーティングの機能と影響は、初期の直感を超えて**十分に探求されていません**。

### 既存研究の課題

ゲーティングの本質的な貢献を、他のアーキテクチャ的要因（例えば、ルーティングメカニズムやスパースアテンション設計）から切り離して評価することが困難であることが指摘されています。

* **Switch Headsの例:**
  Top-Kエキスパートを選択するためにシグモイドゲーティングが導入されていますが、実験によると、選択肢を単一のエキスパートに減らしても、**性能向上の大部分が持続する**ことが判明しました。これは、ゲーティング自体が**ルーティングメカニズムとは独立した、重要な本質的価値**を提供していることを強く示唆しています。
* **Native Sparse Attention (NSA)の例:**
  全体の性能改善が示されているものの、そのゲーティングメカニズムの貢献が、スパースアテンション設計自体の効果から**分離されていません**。

これらの考察から、ゲーティングの効果を他のアーキテクチャ要素から厳密に分離する必要性が強調されています。

### 本研究のアプローチと主要な発見

本研究では、標準的なソフトマックスアテンション（Vaswani, 2017）におけるゲーティングメカニズムを調査しました。  具体的には、クエリ、キー、バリューの射影後（G4, G3, G2）、Scaled Dot Product Attention (SDPA) の出力後（G1）、および最終的な全結合層の出力後（G5）という**異なる位置**でゲーティングを導入して探索しました。

様々なゲーティングのバリエーション（要素ごとの、ヘッドごとの、加算形式、乗算形式など）を調査した結果、以下の重要な発見が得られました。

1. **SDPA出力後のヘッドごとのゲーティング (G1) が最も顕著な性能向上をもたらす**（例：PPL（Perplexity）が最大0.2減少し、MMLUスコアが2ポイント向上）。
2. SDPA出力ゲーティングは、**学習の安定性を改善**し、**損失のスパイクをほぼ解消**する効果があり、より大きな学習率を可能にし、モデルのスケーラビリティを高めます。

## ゲート付きアテンション層: 準備（マルチヘッド・ソフトマックス・アテンション）

このセクションでは、トランスフォーマーの**標準的なアテンション層**の計算を、以下の3つの主要ステップに分けて説明しています。

1. **線形射影（QKV Projections）**: 入力 $X$ を、学習可能な行列 $W^Q, W^K, W^V$ を使ってクエリ $Q$、キー $K$、バリュー $V$ の行列に変換します。
2. **スケーリングされたドット積アテンション (SDPA)**: $Q$ と $K$ の類似度を計算し、$\sqrt{d_k}$ でスケーリングした後、$\text{softmax}$ で確率的な重み（アテンションウェイト）に変換します。この重みを使って $V$ を加重平均し、出力テンソルを得ます。
3. **マルチヘッド連結**: このSDPAプロセスを $h$ 個の独立した「ヘッド」で並行して実行し、それぞれの結果を最後に**連結 (Concat)** して一つの大きな出力テンソルに戻します。

この全体の手順は、次のセクションで紹介される「**ゲート付きアテンション層**」のベースライン（基礎）となる構造です。

入力 $X \in \mathbb{R}^{n \times d_{model}}$ が与えられたとき、ここで $n$ はシーケンス長、$d_{model}$ はモデル次元です。トランスフォーマーのアテンション層（Vaswani, 2017）の計算は、主に4つの段階に分けられます。

#### 1. QKV線形射影 (QKV Linear Projections)

入力 $X$ は、学習された重み行列 $W^Q, W^K, W^V \in \mathbb{R}^{d_{model} \times d_k}$ を用いて、クエリ $Q$、キー $K$、およびバリュー $V$ に線形変換されます。結果として得られる $Q, K, V$ は $\mathbb{R}^{n \times d_k}$ の次元を持ちます。

$$
Q = X W^Q, \quad K = X W^K, \quad V = X W^V.
$$

#### 2. スケーリングされたドット積アテンション (Scaled Dot-Product Attention, SDPA)

クエリとキーの間のアテンションスコアを計算した後、ソフトマックス正規化を行います。出力はバリューの重み付き和になります。

$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V,
$$

ここで、$\frac{Q K^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}$ はスケーリングされたドット積類似度行列を表し、$\text{softmax}(\cdot)$ は、アテンションの重みが非負であり、各行にわたって合計が $1$ になることを保証します。

#### 3. マルチヘッド連結 (Multi-Head Concatenation)

マルチヘッド・アテンションでは、上記のプロセスが $h$ 個のヘッドに対して並行して繰り返されます。各ヘッドは独自の射影行列 $W_i^q, W_i^k, W_i^v$ を持ちます。全ヘッドの出力は連結されます。

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h),
$$

ここで、$\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$ です。

## 2.2 ゲーティングメカニズムによるアテンション層の拡張

このセクションでは、情報フローを制御する動的フィルターとして機能する**ゲーティングメカニズム**の数学的な定義 $Y' = Y \odot \sigma(X W_\theta)$ を示しています。

本研究の目的は、このゲーティングをアテンション層に適用する際の最適な方法を見つけることであり、以下の5つの軸で包括的にバリエーションを探求します。

1. **位置**: QKV射影後、SDPA出力後 (G1)、最終出力後 (G5) など、層内のどこにゲートを配置するか。
2. **粒度**: アテンションヘッド全体を単一のスカラーで変調するか (ヘッドごと)、次元ごとに変調するか (要素ごと)。
3. **共有**: ゲートのパラメータを全ヘッドで共有するか、各ヘッド固有とするか。
4. **形式**: ゲートスコアを元の入力 $Y$ に**乗算**するか（フィルタリング）、**加算**するか（バイアス/残差接続）。
5. **活性化関数**: 主にシグモイド（乗算用）と SiLU（加算用）を使用する。

これにより、ゲーティングメカニズムの多様な側面を網羅的に評価し、その本質的な貢献を明らかにしようとしています。

### 和訳とゲーティングの形式化

ゲーティングメカニズムは以下のように形式化されます。

$$
Y' = g(Y, X, W_\theta, \sigma) = Y \odot \sigma(X W_\theta) \tag{5}
$$

ここで、$Y$ は変調される入力、$X$ はゲーティングスコアの計算に使用されるもう一つの入力、$W_\theta$ は学習可能なゲートのパラメータ、$\sigma$ は活性化関数（例：シグモイド）であり、$Y'$ はゲート付きの出力です。

ゲーティングスコアである $\sigma(X W_\theta)$ は、効果的に**動的なフィルター**として機能し、$Y$ の特徴を選択的に保持または消去することによって、情報フローを制御します。

### 本研究の包括的な調査項目

本研究では、アテンション層内のゲーティングメカニズムのいくつかのバリエーションを包括的に調査します。私たちの探索は、以下の5つの主要な側面（）に焦点を当てています。

#### (1) 位置 (Positions)

ゲーティングを適用する異なる位置の効果を調査します（図1(左)に示されています）。

* (a) $Q, K, V$ 射影後（式 1 に対応）：図1(左)の**G2, G3, G4**の位置。
* (b) SDPA（式 3）の出力に続く位置：**G1**の位置。
* (c) 最終的なマルチヘッド・アテンションの連結出力後（式 4 に対応）：**G5**の位置。

#### (2) 粒度 (Granularity)

ゲーティングスコアの2つの粒度レベルを考慮します。

* (a) **ヘッドごと (Headwise)**：単一のスカラーゲーティングスコアが、アテンションヘッド全体の出力を変調します。
* (b) **要素ごと (Elementwise)**：ゲーティングスコアが $Y$ と同じ次元を持つベクトルであり、次元ごとのきめ細かな変調を可能にします。

#### (3) ヘッド固有か共有か (Head Specific or Shared)

アテンションのマルチヘッドの性質を考慮し、さらに以下を検討します。

* (a) **ヘッド固有 (Head-Specific)**：各アテンションヘッドが固有のゲーティングスコアを持ち、各ヘッドに対して独立した変調を可能にします。
* (b) **ヘッド共有 (Head-Shared)**：$W_\theta$ とゲーティングスコアが全ヘッド間で共有されます。

#### (4) 乗算形式か加算形式か (Multiplicative or Additive)

ゲーティングスコアを $Y$ に適用する方法として、以下を考慮します。

* (a) **乗算ゲーティング (Multiplicative Gating)**：ゲート付き出力 $Y'$ は $Y' = Y \odot \sigma(X W_\theta)$ として計算されます。
* (b) **加算ゲーティング (Additive Gating)**：ゲート付き出力 $Y'$ は $Y' = Y + \sigma(X W_\theta)$ として計算されます。

#### (5) 活性化関数 (Activation Function)

主に2つの一般的な活性化関数、SiLU（Shazeer, 2020）とシグモイドを考慮します。

* SiLU は出力範囲が非有界であるため、**加算ゲーティング**にのみ使用します。
* シグモイドは $[0, 1]$ のスコアのみを出力するため、主に**乗算ゲーティング**に使用します。

さらに、ゲーティングの有効性の根底にあるメカニズムを詳細に分析するために、恒等写像（Identity Mapping）やRMSNorm（Zhang & Sennrich, 2019）も考慮します（詳細はセクション 4.1）。

#### 💡 デフォルト設定

特に指定がない限り、本研究では**ヘッド固有**の**乗算ゲーティング**を採用し、活性化関数としてシグモイド $\left(\sigma(x) = \frac{1}{1 + e^{-x}}\right)$ を利用します。

## 🔬 3. 実験 (Experiments)

この研究は、MoEモデル（15A2B）と密なモデル（1.7B）を用いて、アテンション層におけるゲーティングメカニズムの効果を検証しました。

**主要な発見は以下の通りです:**

* **最適な位置**: ゲーティングを **SDPAの出力後 (G1)** または **バリュー射影後 (G2)** に適用することが、PPLの削減とベンチマーク性能の向上において最も効果的でした。G1が総合的に最良の結果を示しました。
* **ヘッド依存性の重要性**: ゲートのパラメータやスコアを全アテンションヘッドで共有するよりも、**各ヘッドが独立したゲーティングスコアを持つ（ヘッド固有）**ことが、わずかなパラメータ増加で実質的な性能向上をもたらすために重要です。
* **最適な形式**: 乗算形式のゲーティングが、加算形式よりも優れた性能を示しました。

さらに、実験ではゲーティングによって導入される**ウォールタイムレイテンシは $2\%$ 未満**であり、**SDPA出力ゲーティングが学習の安定性を改善し、より大きな学習率を可能にする**という結果も示されています。

### 3.1 実験設定 (Experimental Setups)

#### モデルアーキテクチャと学習設定

* **実験モデル**: MoEモデル（総パラメータ 15B、アクティブパラメータ 2.54B、**15A2B**）と、密なモデル（総パラメータ 1.7B）の両方で実験を実施。
* **MoE設定**: 15A2B MoEモデルは、合計 128 のエキスパート、Top-8 ソフトマックスゲーティング、ファイングレインエキスパート（Fine-grained experts）、グローバルバッチ LBL (Layer-by-Layer) 学習、および Z-loss を利用。
* **アテンション**: アテンション部分には **Group Query Attention (GQA)** を採用。
* **データセット**: 多言語、数学、および一般知識コンテンツを含む、3.5兆の高品位トークンのサブセットでモデルを学習。
* **コンテキスト長**: シーケンス長は 4096 に設定。
* **オプティマイザ**: AdamWオプティマイザのデフォルト値を使用。
* **レイテンシ**: ゲーティングによって導入されるパラメータと FLOPs（浮動小数点演算回数）が小さいため、**ウォールタイムレイテンシ（実時間遅延）の増加は 2% 未満**。

#### 評価 (Evaluation)

ポピュラーなベンチマークを用いて、Few-shots（少数の例による推論）結果をテスト。

| ベンチマーク  | 分野         |
| :------------ | :----------- |
| Hellaswag     | 英語理解     |
| MMLU          | 一般知識     |
| GSM8k         | 数学推論     |
| HumanEval     | コーディング |
| C-eval, CMMLU | 中国語能力   |

また、言語モデリングの**パープレキシティ (PPL)** を、英語、中国語、コード、数学、法律、文学といった多様な保持テストセットで報告。

### 3.2 主要な結果 (Main Results)

#### 3.2.1 MoEモデル向けゲーテッド・アテンション

効率的な学習が可能な **MoE-15A2B** モデルで、異なるゲーテッド・アテンション層の結果を比較。

* **学習スケジュール**: 1kステップで最大学習率 $2 \times 10^{-3}$ までウォームアップし、コサイン関数を用いて $3 \times 10^{-5}$ まで減衰。
* **バッチサイズ**: グローバルバッチサイズ 1024、合計 100k 最適化ステップ。
* **公正な比較**: ゲーティング機構の貢献を公平に評価するため、バニラMoEベースラインに対し、キー・バリューヘッド数の増加、クエリヘッド数の増加、エキスパート数の増加など、**ゲーティングと同等かそれ以上のパラメータを導入する拡張ベースライン**も補足して比較。

#### 📊 主な実験結果の観察（表1より）

1. **SDPA出力とバリュー出力のゲーティングが効果的**:
   SDPAの出力（**G1**）またはバリューマップ（**G2**）にゲートを挿入することが最も効果的であり、他のバリエーションよりも低い PPL と優れた総合ベンチマーク性能を達成しました。
2. **ヘッド固有のゲーティングが重要**:
   G1とG2でヘッドごとのゲーティングを適用しても、追加されるパラメータは非常にわずか（MoE-15A2Bモデルで2M未満）ですが、それでも実質的な改善をもたらします（行10と11）。ゲーティングスコアを異なるアテンションヘッド間で**共有**した場合（行12 vs. 10、行13 vs. 11）、ベンチマークの改善はヘッドごとのゲーティングよりも小さくなりました。これは、**異なるアテンションヘッドごとに異なるゲーティングスコアを適用することの重要性**を強調しています。
3. **乗算ゲーティングが好ましい**:
   加算形式の SDPA 出力ゲーティングは、ベースラインに対して改善を示しますが、**乗算形式のゲーティングを下回ります**。
4. **シグモイド活性化関数が優れている**:
   最も効果的なゲーティング構成（行5）の活性化関数を SiLU に置き換えると（行15）、改善効果が減少しました。

#### 📈 全体的な結論（MoEモデル）

バリュー層（G2）とSDPA出力（G1）にゲーティングを追加することで、PPL は $0.2$ 以上削減され、様々なパラメータ拡張ベースラインを上回る性能を示しました。ただし、**G1 でのゲーティングがより良い PPL とベンチマーク結果を達成**しています。異なるヘッドが**異なるゲーティングスコア**を受け取る限り、ゲーティングの粒度や活性化関数の選択は比較的小さな影響しか与えません。

## 🎓 3.2.2 密なモデル向けのゲーテッド・アテンション

このセクションでは、主に密なトランスフォーマーモデルにおけるSDPA出力ゲーティング（G1）の有効性を、特に**学習安定性**の観点から検証しています。

**主要な発見と結論:**

* **一貫した有効性**: SDPA出力ゲーティングは、パラメータサイズをFFNの幅を減らすことで維持しても、モデルの構成、データ量、ハイパーパラメータ設定を問わず、**一貫して性能上の利益（PPLの低下、ベンチマークスコアの向上）**をもたらします。
* **安定性向上とスケーリング**: ゲーティングメカニズムは、学習中の**損失スパイクの発生を劇的に減少させる**効果があります。
* **高負荷環境での優位性**: ネットワークの深さを増やしたり、大きな学習率やバッチサイズを使用したりする不安定になりやすい設定（スケーリング設定）において、ベースラインが収束に失敗するか性能向上に乏しいのに対し、**ゲーティングは安定した収束を可能にし、同時に性能を顕著に向上させます**。

結論として、SDPAの要素ごとのゲーティングは、アテンションメカニズムを補強する最も効果的な手法であり、特に**大規模かつ高効率な学習（スケーリング）において、安定性と性能の両方を向上させる**上で不可欠な要素であることが実証されました。

### 📝 和訳

我々は、SDPA出力におけるシグモイドゲーティングの有効性を検証するために、密なモデル（Dense Models）でも実験を実施しました。ゲーティングを使用する際、パラメータサイズを維持するためにFFN（フィードフォワードネットワーク）の幅を縮小しました。

ほとんどの実験では、ベースライン用に最適化されたハイパーパラメータを使用しています。例えば、400Bトークンで学習された1.7Bモデルの場合、最大学習率（LR）は $4 \times 10^{-3}$、バッチサイズ（bsz）は 1024 を使用しました。3.5Tトークンでの学習では、最大学習率を $4.5 \times 10^{-3}$、バッチサイズを 2048 に増加させました。

先行研究により、ネットワークの深さの増加、大きな学習率、および大きなバッチサイズはモデル性能と分散学習効率を大幅に向上させますが、**しばしば学習の不安定性を引き起こす**ことが確立されています。

我々の実験では、ゲーティングメカニズムを適用することで、学習中の**損失スパイクの発生が顕著に減少する**ことが観察されました。これは、学習安定性の向上においてゲーティングが有望な役割を果たすことを示唆しています。

この知見に動機付けられ、我々は層数の増加、より高い最大学習率、より大きなバッチサイズを特徴とする別の実験設定を導入し、ゲーティングの**安定化効果**をさらに検証しました。

### 📊 表2による主要な結果

表2は、以下の事実を明らかにしています。

1. **ゲーティングは様々な設定で有効**:
   様々なモデル構成（行1 vs. 2、行5 vs. 8）、学習データ量（行3 vs. 4）、およびハイパーパラメータ（行11 vs. 13）にわたって、SDPA出力ゲーティングを適用することは**一貫して利益をもたらします**。
2. **ゲーティングは安定性を改善し、スケーリングを促進する**:
   3.5Tトークンの設定では、ゲーティングは学習安定性を改善し、**損失スパイクを大幅に減少させます**（図1、右）。
   最大学習率を増加させた場合、ベースラインは**収束の問題**に直面しました（行6, 12）。SandwichNorm を追加することで収束は回復しますが、改善はごくわずかです。対照的に、ゲーティングを適用したモデルで最大学習率を増加させると、**顕著な性能向上**が得られます（行10 vs. 6, 行14 vs. 12）。

## 4. 分析: 非線形性、スパース性、およびアテンション・シンク・フリー

このセクションでは、シンプルなゲーティングメカニズムが性能と学習安定性を大幅に向上させる理由を探るための一連の実験を行います。分析から得られた**主要な知見**は以下の通りです。

1. **非線形性**: ゲーティング操作が非線形性を高めることで、一貫して性能向上に繋がります（セクション 4.1）。
2. **スパース性**: 最も効果的な SDPA の要素ごと G1 ゲートは、**入力依存の強いスパース性**を SDPA 出力に導入し、これが「アテンション・シンク」現象の解消に役立ちます（セクション 4.2）。

### 4.1 非線形性はアテンションにおける低ランク写像の表現力を向上させる

#### 低ランク写像の問題点

マルチヘッド・アテンションの出力 $\boldsymbol{o}_i^k$ は、最終的に、バリュー射影 $W_V^k$ と出力層 $W_O^k$ の合成 $\left( W_V^k W_O^k \right)$ を通じた線形写像として表現できます。

$$
\boldsymbol{o}_i^k = \sum_{j=0}^{n} S_{ij}^k \cdot \boldsymbol{X}_j \left( W_V^k W_O^k \right)
$$

ここで、バリューの次元 $d_k$ はモデル次元 $d_{\text{model}}$ よりも小さいため、この合成写像 $W_V^k W_O^k$ は**低ランク**になり、アテンション層全体の**表現力が制限される**という問題があります。Group Query Attention (GQA) では $W_V$ がヘッド間で共有されることで、この表現力の制限はさらに悪化します。

#### 非線形性の導入による改善

2つの線形写像の間に非線形性を追加することで表現力を向上できるという先行研究に基づき、本研究では以下の2つの修正を検討しました。

1. **G2の位置での非線形性（式 7）**: バリュー射影 $\boldsymbol{X}_j W_V^k$ の直後に非線形な写像を適用する。
   * これは、**G2** の位置でのゲーティング（表3、行3）に対応します。
2. **G1の位置での非線形性（式 8）**: SDPA出力 $\sum \dots$ の直後に非線形な写像を適用する。
   * これは、**G1** の位置でのゲーティング（表3、行4）や、**RMSNorm**（表3、行5）に対応します。

**結果**: G1またはG2にゲーティングや正規化（RMSNorm）を追加すると性能が向上しますが、出力層 $W_O$ の後である **G5** の位置では改善が見られません（表1、行9）。これは、**$W_V$ と $W_O$ の間に非線形性を挿入する**ことが重要であることを裏付けています。

#### 加算ゲーティングと非線形性

* G1での加算ゲーティングが乗算ゲーティングより劣るものの改善を示すのは、その活性化関数である **SiLU** が非線形性を導入しているためです。
* **追加実験**として、G1でSiLUのみを導入したり、加算ゲーティングから SiLU を除去したりすると、性能向上の効果が減少することが確認されました。

**結論**: **効果的なゲーティングは、$W_V$ と $W_O$ の間に非線形性を導入する**ことに起因して性能向上をもたらす可能性が高いことが示されました。G1とG2で効果に違いが見られることから、次のセクションでさらに分析が進められます。

### 4.2 ゲーティングは入力依存のスパース性を導入する

このセクションでは、特に最も効果的であった G1（SDPA出力）ゲーティングと G2（バリュー）ゲーティングのスコアを分析し、性能向上に繋がるスパース性の性質を特定します。

#### 📊 主要な観察結果

ゲーティングスコアの平均値と分布を分析したところ、以下の重要な知見が得られました（）。

1. **有効なゲーティングスコアはスパースである**:
   * **SDPA出力ゲーティング（G1）**（要素ごと/ヘッドごと）は、**最も低い平均ゲーティングスコア**を示しました。
   * G1ゲーティングスコアの分布は $0$ の近くに集中しており、これは**実質的なスパース性**（多くの値が0に近いこと）を示しています。このスパース性が、G1の優れた性能と一貫しています。
2. **ヘッド固有のスパース性が重要**:
   * ゲーティングスコアをアテンションヘッド間で**共有**すると、全体のゲーティングスコアが増加し、性能向上の効果が減少します。
   * このことから、個々のアテンションヘッドが入力の異なる側面を捉えているという先行研究の通り、**ヘッド固有のゲーティング**の重要性が強調されます。
3. **クエリ依存性が重要**:
   * バリューゲーティング（G2）のスコアは、SDPA出力ゲーティング（G1）のスコアよりも高かったため、性能が劣っていました。
   * G1ゲーティングスコアは**現在のクエリ**（Attention(Q,K,V)の出力）に対応する隠れ状態から導出されるのに対し、G2ゲーティングスコアは**過去のキーやバリュー**に関連する隠れ状態から導出されます。
   * **示唆**: G1ゲーティングのスパース性は**クエリ依存**であるため、クエリにとって**無関係な文脈情報**をフィルターアウトしている可能性が高いです。
   * このクエリ依存性の重要性をさらに検証するために導入された「入力非依存ゲーティング」は、非線形性の導入によりベースラインを改善しましたが、高いゲーティングスコアを示し、効果的なスパース性が**入力依存**であるべきことを裏付けました。
4. **スパース性の低いゲーティングは劣る**:
   * スパース性の重要性を確認するため、ゲーティングの定式化からスパース性を減らす試みを行いました。シグモイド関数を、スコアを $[0.5, 1.0]$ に制約する修正版 **NS-sigmoid** に置き換えました（非線形性は維持しつつスパース性を除去）。
   * 結果（表4、行7）として、NS-sigmoid ゲーティングによる改善効果は、SDPA出力シグモイドゲーティングよりも**劣る**ことが示されました。

### 4.3 SDPA出力ゲーティングは「アテンション・シンク」を軽減する

#### 仮説と検証方法

先行セクション（4.2）の分析に基づき、入力依存のスパース性を導入するゲーティングメカニズムは、現在のクエリトークンに**無関係なコンテキスト**をフィルタリングし、その結果、**アテンション・シンク（Attention Sink）**現象を軽減するという仮説を立てました。

この仮説を検証するため、以下の指標を分析しました。

1. **先頭トークンへのアテンションスコアの割合 (F-Attn)**: アテンション・シンクの典型的な指標。
2. **隠れ状態の最大活性化の平均 (M-Act)**: 「大規模な活性化（Massive Activation）」の度合いを示す指標。

#### 📊 主要な観察結果

1. **G1ゲーティングはアテンション・シンクを大幅に軽減する**:
   * SDPA出力（G1）に適用されたヘッドごとおよび要素ごとのクエリ依存シグモイドゲーティングは、**先頭トークンに割り当てられるアテンションスコアを大幅に減らし (F-Attn の削減)**、同時に**大規模な活性化 (M-Act) も減少**させました。
2. **大規模な活性化はアテンション・シンクの前提ではない**:
   * ヘッド間でゲーティングスコアを共有したり、バリュー射影後（G2）にゲーティングを適用したりした場合、大規模な活性化は減少しましたが、**先頭トークンへのアテンションスコア（F-Attn）は減少しませんでした**。
   * このことから、大規模な活性化はアテンション・シンクの**必須条件ではない**ことが示唆され、**ヘッド固有のゲーティング**の重要性が再確認されました。
3. **スパース性の喪失は現象を悪化させる**:
   * ゲーティングの入力依存性を減らしたり（行6）、NS-sigmoidを使用してスパース性を減らしたり（行7）すると、**大規模な活性化とアテンション・シンクの両方が増大しました**。

#### 💡 総合的な結論と学習安定性への影響

これらの観察結果を総合すると、以下の結論が得られます。

* **ゲーティングの本質的な効果**: SDPA出力における入力依存のヘッド固有ゲーティングは、**有意なスパース性**を導入し、それによって**アテンション・シンクを軽減**します。
* **安定性への影響**: SDPA出力におけるスパース性は、モデル内の**大規模な活性化 (Massive Activation) を減少**させます。スパース性が高いほど、活性化は小さくなります。
* **損失スパイクの解消**: 大規模な活性化が減少することで、モデルはBF16（16ビット浮動小数点）学習中の数値エラーに対して脆弱でなくなり、これがゲーティングによる**学習安定性（損失スパイクの解消）の改善**を説明する可能性が高いです。
* **大規模な活性化の原因**: 大規模な活性化は主に初期層（例：層5）のFFN出力から発生し、プレ・ノルム（Pre-norm）メカニズムを介して後続の層に伝播します。この知見は、FFN出力にLayerNormを適用することで大規模な活性化を抑制し、学習安定性を高める **Sandwich Normalization** の有効性（表2、行7）とも一致します。

## 結論

本研究は、標準的なソフトマックスアテンションにおけるゲーティングメカニズムの役割を体系的に調査し、その性能、学習安定性、およびアテンションのダイナミクスに対する重要な影響を明らかにしました。

### 🔑 主要な発見と貢献最も効果的なゲーティング:

30種類以上のバリエーションについて、15B MoEモデルと1.7B密なモデルで広範な実験を実施した結果、Scaled Dot-Product Attention (SDPA) の出力後にシグモイドゲートを適用する手法が、最も実質的な性能向上をもたらすことを実証しました。

### 改善のメカニズム:

このシンプルなゲーティングメカニズムは、以下の3つの主要な効果を通じて性能を向上させます。

### 非線形性の強化:

バリュー射影 $W_V$ と出力層 $W_O$ の間に非線形性を導入し、アテンション層の表現力を高めます。

### 入力依存のスパース性の導入:

SDPA出力に強い入力依存のスパース性を導入します。

### 非効率性の解消:

スパース性によって、学習の不安定性の原因となる「アテンション・シンク」現象などの非効率性が解消されます。

### スケーリングと安定性の向上:

ゲーティングを導入することで、モデルはより大きな学習率とバッチサイズでの安定した学習が可能になり、スケーラビリティが向上します。コンテキスト長の拡張:ゲーティングはコンテキスト長の拡張も容易にし、再学習なしでモデルがより長いシーケンスに対して効果的に一般化することを可能にします。
