# ニューラルネットワークとは
今回は、ニューラルネットワークの概要や開発の歴史、計算の仕組みについてまとめてみます。


## LLM理解の基礎: ニューラルネットワークの全体像

ニューラルネットワーク（NN）は、人間の脳の神経回路を模倣した計算モデルであり、AI（人工知能）の核となる技術です。その最大の役割は、大量のデータから**自動的にパターンを学習**し、分類や予測を行うことです。


## 1. ニューラルネットワークの基礎

### NNの歴史と発展

NNの研究は、幾度かの「冬の時代」を経て進化してきました。

| 年代 | 出来事 | 意義 |
| :--- | :--- | :--- |
| **1940年代** | **M-Pモデル**の提案 | ニューロン（計算単位）の**数理モデル**を確立。 |
| **1958年** | **パーセプトロン**の登場 | **学習**という概念をNNに導入。単層では線形問題のみ解決可能。 |
| **1969年** | **第1次冬の時代** | XOR問題など、非線形問題が解けないことが判明し、研究が停滞。 |
| **1986年** | **誤差逆伝播法 ($\text{Backpropagation}$) の普及** | 多層NN（DNN）の学習を可能にし、非線形問題を解決。現在のディープラーニングの基礎技術。 |
| **2012年以降** | **深層学習ブーム** | GPUと大規模データセットの登場により性能が飛躍的に向上。 |

### 構造: ノードと層による情報の流れ

NNは、情報を処理する**ノード（ニューロン）**が**重み付きの接続**で結ばれ、以下の3つの層構造を持つことで情報を処理します 

[Image of a simple neural network architecture showing input, hidden, and output layers]
:

| 層の名称 | 役割 | LLMにおける例 |
| :--- | :--- | :--- |
| **入力層 (Input Layer)** | 外部データ（画像やテキスト）を受け取る入り口。 | 文章中の**単語**や**トークン**を数値化したもの。 |
| **隠れ層 (Hidden Layers)** | データの**複雑なパターン**や**特徴**を抽出・解釈する主要な処理部。層が多いほど、より深い特徴を学習できる（深層学習）。 | 文章の**文脈**や**文法構造**を理解する部分。 |
| **出力層 (Output Layer)** | ネットワークの最終的な**予測**や**結果**を出力する出口。 | 次に来る**単語の候補**や、分類結果。 |


## 2. 仕組み：計算と学習の完全な流れ

NNの学習は、「**順方向の予測**」と「**逆方向の修正**」という2つのフェーズを繰り返すことで行われます。

### 順伝播 (Forward Propagation)：推論

**目的:** 入力から予測を生成する。

1.  **積和とバイアス:** 各ニューロンで、前の層からの入力に接続の**重み ($\text{Weight}$)** を掛け合わせて合計し、**バイアス ($\text{Bias}$)** を加算する。
2.  **活性化関数:** この結果を非線形な**活性化関数 $f()$** に通すことで、複雑なパターンを扱えるようにする。
    $$
    \text{Output} = f\left(\sum_{i} (\text{Input}_i \times \text{Weight}_i) + \text{Bias}\right)
    $$

### 逆伝播 (Backpropagation)：学習（重みの調整）

**目的:** 予測誤差（損失）を最小化するように重みを効率的に修正する。

1.  **損失の計算:** 予測値と**正解ラベル**のズレを**損失関数**で数値化する（これがモデルの間違いの大きさ）。
2.  **誤差の逆伝播:** 損失を出発点とし、**出力層から入力層へ逆方向**に、各重みが損失にどれだけ影響したか（**勾配**）を計算し伝達する。
3.  **重みの更新 (勾配降下法):** 計算された**勾配と逆の方向**へ、「**学習率**」分の歩幅で重みを更新し、損失を減らす。

## 3. LLMへの接続：時系列情報の扱いの重要性

LLMを学習する上で時系列情報が重要となるのは、**文章データ自体が本質的に順番を持つ時系列データ**だからです。

### NNの種類と時系列処理

| NNの種類 | 主な応用分野 | LLMとの関係 |
| :--- | :--- | :--- |
| **RNN/LSTM** | **時系列データ**（音声、テキスト） | 過去の情報を保持する**再帰的な接続**を持ち、文章の文脈を理解する初期の基礎となった。 |
| **Transformer** | **LLMの基盤** | **自己注意機構**により、遠く離れた単語間の依存関係（長距離依存性）を効率的に捉え、時系列処理を革新した。 |

LLMは、Transformerという強力なNN構造を用い、文章という**「単語の並び（時系列）」**を扱うことで、文脈を理解し、次の単語を予測する**生成**能力を実現しています。

