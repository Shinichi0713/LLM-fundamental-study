# Instruction Tuning

**Instruction Tuning（指示チューニング）**は、大規模言語モデル（LLM）の**微調整（Fine-tuning）**フェーズで用いられる手法であり、モデルがユーザーからの**多様な指示（Instruction）や質問の意図を正確に理解し、それに応じた応答を生成する能力**を劇的に向上させることを目的としています。


## 💡 Instruction Tuningの仕組みと目的

Instruction Tuningは、簡単に言えば「**モデルに『先生』の役割を与える**」ための学習です。

| 項目 | 詳細 |
| :--- | :--- |
| **目的** | ユーザーの**自然言語による指示**（例：「この文章を要約して」「この質問に答えて」「詩を書いて」）に対して、人間が期待する形式と内容で**忠実（忠実性/Alignment）**に応答できるようにすること。 |
| **学習データ** | **Instruction-Response Pair（指示-応答のペア）**と呼ばれる形式のデータセットを使用します。 |
| **手法** | 事前学習済みのモデルに対し、**教師あり学習（Supervised Learning）**を用いてInstruction-Response Pairを学習させます。 |


## 📝 学習データの構造（Instruction-Response Pair）

Instruction Tuningに用いられるデータは、単なるテキストではなく、以下の3つの要素から構成されることが一般的です。

1.  **指示（Instruction / Prompt）:** ユーザーがモデルに実行を求めるタスクの具体的な説明（例：`次の文章を日本語で要約してください。`）
2.  **入力（Input / Context）:** 処理の対象となるテキストや情報（例：`The cat sat on the mat. The dog was nearby.`）
3.  **応答（Response / Output）:** 指示と入力に対する**理想的な正解出力**（例：`猫はマットの上に座り、犬が近くにいました。`）

### データ収集の例

これらのデータは、人間が手作業で作成するほか、既存のタスクデータセットをInstruction形式に変換したり、より大きなLLM（例：GPT-4）を使用して大量の指示と応答を生成（**Self-Instruct**手法）したりすることで収集されます。


## 📈 Instruction Tuningの効果

Instruction Tuningは、LLMが**チャットボット**や**対話型AI**として機能するために不可欠なプロセスであり、以下のような効果をもたらします。

1.  **ゼロショット学習能力の向上:** 明示的な例なしに、見たことのない新しい指示に対しても、適切な応答を生成する能力（汎化能力）が向上します。
2.  **マルチタスク能力の強化:** 翻訳、要約、Q&A、分類など、多様なタスクを単一のモデルで実行できるようになります。
3.  **Haltucination（ハルシネーション：もっともらしい嘘）の抑制:** モデルが質問の意図をより正確に捉えることで、不正確な情報や無関係な情報を生成する傾向が減少します。

Instruction Tuningの後に、さらに人間の選好（ニュアンス）を学習する**RLHF（人間のフィードバックによる強化学習）**を行うことで、モデルは実用的な性能へと磨き上げられます。

# 学習データ

Instruction Tuning（指示チューニング）の学習データは、モデルが多様なタスクで人間の意図に従って行動できるように設計された、特殊な形式のデータセットです。

そのデータは、単なる文章の羅列ではなく、**「指示」と「それに対する理想的な応答」のペア**を中心に構成されています。

## データの基本構造：Instruction-Response Pair

Instruction Tuningの学習データは、一般的に以下の要素を組み合わせた形式で作成されます。

| 要素名 | 役割 | 具体的な例 |
| :--- | :--- | :--- |
| **Instruction (指示)** | モデルに実行してほしいタスクを記述した自然言語のプロンプト。 | *「以下の文章を50文字以内で要約してください。」* |
| **Input (入力/コンテキスト)** | 指示の実行に必要な情報やデータ（入力文）。タスクによっては省略されることもあります。 | *「富士山は日本で最も高い山であり、その優美な姿から、古くから多くの芸術作品の題材とされてきました。」* |
| **Response (応答/出力)** | 指示と入力に対して、**人間が考える最も適切で質の高い正解の回答**。 | *「富士山は日本一の高さで、古くから芸術の題材となっている。」* |

このペアを大量に学習することで、モデルは「この指示が来たら、このように応答すれば人間が満足する」というパターンを学習します。


## 主な学習データの種類と収集方法

Instruction Tuningに使用されるデータセットは、その**品質**と**多様性**がモデルの性能に直結するため、様々な方法で収集・作成されています。

### 1. 手動でキュレーションされた高品質データ

特定のモデルを開発する際に、コストをかけて人間が作成した最も質の高いデータです。

* **Human-Written Demonstrations (人間による実演例):**
    * 特定のプロンプトに対して、専門のライターやアノテーターが、モデルの理想的な振る舞いを示す応答を手書きで作成します。
    * このデータは、モデルに**正確な事実**と**論理的な推論**を教え込むために重要です。
* **タスクベースのデータセットのInstruction形式への変換:**
    * 既存の自然言語処理（NLP）タスク（例：SQuADなどの質問応答データセット、翻訳コーパス、感情分析データ）を、Instruction Tuningの形式（「指示」＋「入力」＋「応答」）に整形し直して使用します。

### 2. より強力なLLMを用いた自動生成データ

大規模なLLM（例：GPT-4）の強力な能力を利用し、Instruction-Response Pairを**大量に自動生成**する手法です。

* **Self-Instruct (セルフインストラクト) 手法:**
    1.  まず、少数の人間が作成した高品質な「シード指示」を用意します。
    2.  モデル自身が、このシード指示を参考に、**新たな指示**を生成します。
    3.  モデルは、生成した新しい指示に対して**自分で応答を生成**します。
    4.  最終的に、生成された指示と応答のペアをフィルタリングし、Instruction Tuningのデータとして使用します。
    * この手法により、**人件費を抑えつつ、多様な指示を含むデータセット**を効率的に作成できます。（例：Stanford Alpacaデータセットなど）

### 3. 対話とユーザークエリのログ

実際の対話や検索のログから匿名化・整形されたデータも利用されます。

* **Dialogue Data (対話データ):**
    * マルチターンの対話（複数回のやり取り）のログをInstruction形式に変換し、モデルに**自然な会話の流れ**や**文脈の維持**を学習させます。
* **User Queries (ユーザークエリ):**
    * 検索エンジンや既存のAIアシスタントに寄せられた**実際の質問やタスク指示**を利用し、モデルが現実世界で頻繁に出会う多様な要求に対応できるように学習させます。

これらの多様なデータソースと手法を組み合わせることで、LLMは「単なる文章の続きを予測する」能力から、「**人間の意図を解釈し、適切にタスクを実行する**」能力へと進化します。

# 正解不正解の判断

Instruction Tuning（指示チューニング）におけるモデルの「正解・不正解」の判定は、基本的に**教師あり学習（Supervised Learning）**の枠組みに基づいて行われます。

これは、モデルの出力と、学習データに含まれる**「理想的な正解応答」（Ground Truth Response）**を比較することで評価されます。


## 1. 損失関数（Loss Function）による評価

Instruction Tuningでは、Next Token Prediction（次トークン予測）と同じく、モデルが生成するトークンが、教師データ（正解応答）のトークンとどれだけ一致しているかを、**損失関数**を用いて定量的に評価します。

### a. クロスエントロピー損失 (Cross-Entropy Loss)

Instruction Tuningで使用される最も一般的な損失関数です。

* **定義:** モデルが予測した次のトークンの**確率分布**と、正解応答の次のトークンを示す**真の確率分布**（one-hotベクトル）の間の隔たり（差）を計算します。
* **学習の目標:** この損失（エラー）の値を最小化することが学習の目標です。損失が小さいほど、モデルの出力は正解応答に近くなります。
* **判定の仕組み:**
    1.  モデルは、指示と入力に基づいて、次のトークンとして考えられる全ての語彙（トークン）の確率を計算します。
    2.  正解応答の実際のトークン（例: `要約`）が高い確率で予測されていれば、損失は小さくなります。
    3.  モデルが誤ったトークン（例: `翻訳`）を最も高い確率で予測した場合、損失は大きくなり、その間違いを修正するようにモデルの重み（パラメーター）が更新されます。

### b. 正解の範囲

この手法では、**モデルの出力全体**に対して損失を計算し、モデルが生成する**全てのトークン**が正解のシーケンス（一連の流れ）と一致するように学習します。

$$L = - \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{V} y_{i,j} \log(\hat{y}_{i,j})$$

* $L$: 損失
* $N$: トークン数
* $V$: 語彙サイズ
* $y_{i,j}$: $i$番目のトークンが正解の語彙$j$であるか（1か0）
* $\hat{y}_{i,j}$: モデルが予測した $i$番目のトークンが語彙$j$である確率


## 2. 評価指標（Metrics）による判定

学習中や学習後に、モデルの性能を客観的に評価するために、以下の指標が使われます。これらの指標は、モデルの出力が正解と**「完全に一致したか」**、あるいは**「部分的に一致したか」**を測ります。

### a. 精度（Accuracy）

* **トークンレベルの精度:** モデルが予測した個々のトークンが、正解応答の対応するトークンと**完全に一致した割合**です。
* **シーケンスレベルの精度:** モデルが生成した応答の**トークン列全体**が、正解応答のトークン列と**完全に一致した割合**です。Instruction Tuningでは、少しでも言い回しが異なると不正解となるため、シーケンスレベルの精度は非常に厳しくなります。

### b. BLEU/ROUGE スコア

生成される文章の品質を評価するための一般的な指標です。特に正解応答が複数ある場合や、完全一致を求めない場合に有用です。

* **BLEU (Bilingual Evaluation Understudy):** 主に機械翻訳の評価に使われますが、生成タスクでも利用されます。モデルの出力に含まれる**N-gram（連続する単語の並び）**が、正解応答に含まれるN-gramとどれだけ一致するかを測ります。
* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** 主に要約タスクの評価に使われます。モデルの出力に含まれるN-gramが、正解応答によって**どれだけカバーされているか（再現率）**を測ります。


## 3. 「ニュアンスの課題」への対応

Instruction Tuningの段階では、モデルはあくまで**「学習データにある正解応答」を再現すること**を目標とします。そのため、ニュアンスが同じ異なる言い回しを正解とすることはできません。

この「ニュアンスや多様性」の課題に対応するために、Instruction Tuningの**次のステップ**として**RLHF（人間のフィードバックによる強化学習）**が行われます。

* RLHFでは、モデルの出力が**人間にとって有用か、自然か**という**主観的な評価**に基づいて報酬を与えます。これにより、**学習データにはない多様な表現**でも、人間が好むものであればモデルはそれを生成するように最適化され、Instruction Tuningの厳格さを補完します。