# Sparse Attention

## ğŸ§  Sparse Attention Explained

**Sparse Attention** is a technique developed to address the computational and memory limitations of the **Dense Attention** (or **Full Attention**) mechanism, which is the core component of the Transformer architecture.

---

## ğŸ›‘ The Problem with Dense Attention (Full Attention)

In standard Dense Attention, every token in a sequence must calculate its relationship (attention score) with **every other token** in that same sequence.

1. **Computational Complexity**: If the sequence length is $N$, the time and memory complexity required to compute the Attention scores is **quadratic** in relation to the sequence length, denoted as $O(N^2)$.
2. **Scaling Limitation**: This $O(N^2)$ complexity makes it prohibitively expensive and often impossible to process **very long sequences** (e.g., long documents, high-resolution images, long audio tracks) because the computational cost and memory consumption grow too rapidly.

---

## ğŸ¯ The Solution: Sparse Attention

Sparse Attention operates on the premise that not every token needs to interact with *all* other tokens to understand the context. It selectively **restricts (sparsifies)** the connections in the attention matrix to reduce the computational cost.

The goal is to reduce the complexity from $O(N^2)$ to something closer to **linear complexity** with respect to $N$, such as $O(N \cdot W)$ (where $W$ is a fixed window size) or $O(N \cdot \sqrt{N})$.

### Key Principle

Instead of calculating and storing the full $N \times N$ attention matrix, Sparse Attention only computes and focuses on a subset of the most relevant connections.

### Common Sparsification Patterns

Sparse Attention models implement different patterns to decide which connections to keep:

| Pattern                          | Principle                                                                                                                                                        | Complexity Example                          |
| :------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------ |
| **Local Attention**        | Each token only attends to tokens within a**fixed, adjacent window** around itself. This mimics locality bias in CNNs.                                     | $O(N \cdot W)$                            |
| **Global Attention**       | A few designated**"global" tokens** (like the `[CLS]` token) attend to all tokens, while all other tokens only attend locally or to those global tokens. | $O(N \cdot k)$ or $O(N \cdot \sqrt{N})$ |
| **Random Attention**       | Each token randomly selects a small number of other tokens to attend to.                                                                                         | $O(N \cdot k)$ ($k$ is a small number)  |
| **Block Sparse Attention** | The attention matrix is divided into blocks, and only a chosen subset of these blocks is computed, often following a pre-defined or learned structure.           | $O(N \cdot \sqrt{N})$                     |

### Benefits

* **Longer Sequences**: Enables the processing of much longer input sequences that would otherwise crash due to memory limits.
* **Faster Training/Inference**: Reduces the number of necessary computations, leading to faster model training and prediction times.
* **Memory Efficiency**: Significant reduction in GPU memory consumption because the full attention matrix is never calculated or stored.

Sparse Attention is a crucial enabler for models designed to handle extensive data, such as **LongFormers** and other large-scale models in NLP, vision, and audio processing.

There isn't a single, universally accepted classification of "genres" in Sparse Attention, but the implementations and techniques generally fall into categories based on **how the attention connections are restricted (the sparsification pattern)**.

Here are the main categories of Sparse Attention patterns that exist, organized by their approach to limiting the quadratic complexity $O(N^2)$:

## ğŸŒ Main Categories of Sparse Attention

### 1. Fixed or Window-Based Sparsity

This is the simplest and most common form of sparsification, where the pattern of allowed connections is fixed and predetermined, usually based on proximity.

* **Local Attention (or Banded Attention)**:

  * **Principle**: Each token is restricted to attending only to tokens within a **fixed, adjacent window** of size $W$ around itself. Connections beyond this window are blocked.
  * **Benefit**: Reduces complexity to $O(N \cdot W)$, which is linear with respect to sequence length $N$ when $W$ is small.
  * **Use Case**: Effective for tasks where context is highly localized, such as short-range dependency modeling.
* **Dilated Attention**:

  * **Principle**: Similar to local attention, but the tokens within the window are not necessarily adjacent; instead, they are **sampled at a fixed interval** (or dilation rate).
  * **Benefit**: Allows the model to capture information from distant tokens without increasing the overall computational cost dramatically.

### 2. Global + Local Sparsity (Mixed Attention)

These models combine the benefits of local proximity with the need to capture critical long-range dependencies.

* **Global Attention**:
  * **Principle**: Designates a few **"global" or "special" tokens** (like the `[CLS]` token, or tokens at fixed intervals) that attend to *all* tokens, and are attended to by *all* tokens. The remaining tokens only attend locally or to the global set.
  * **Benefit**: The global tokens act as information hubs, effectively summarizing context for the entire sequence, thus bridging long distances.
  * **Examples**: Used in models like **Longformer**.

### 3. Data-Dependent or Adaptive Sparsity

These advanced methods allow the model to dynamically choose which connections are important based on the input data itself, rather than relying on a fixed pattern.

* **Learned Sparsity**:

  * **Principle**: The model learns a sparse mask during training. This might involve using a **gating mechanism** or a **pruning strategy** to dynamically zero out attention scores that are deemed unimportant for a given input.
  * **Benefit**: Highly flexible and potentially more efficient, as attention is only paid to truly relevant context.
* **Query/Key Clustering (e.g., Reformer)**:

  * **Principle**: Uses techniques like **Locality-Sensitive Hashing (LSH)** to group similar queries ($\mathbf{Q}$) and keys ($\mathbf{K}$) together. Attention is then only computed *within* these similar clusters.
  * **Benefit**: This effectively makes attention *sparse* by limiting it to semantically relevant neighbors, dramatically reducing complexity, often to $O(N \log N)$.

### 4. Hierarchical Sparsity

* **Principle**: The attention is structured across multiple levels of granularity. Tokens first attend locally, then the output is pooled to create a representation of a chunk (e.g., paragraph or block), and then these chunk representations attend globally.
* **Benefit**: Ideal for handling structured data like very long documents, allowing the model to focus on both fine-grained details and document-level context.

## Implementation
### Local Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LocalAttention(nn.Module):
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®PyTorchå®Ÿè£…ã€‚
    å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ãã®å‘¨å›²ã® 'window_size' å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã®ã¿æ³¨ç›®ã—ã¾ã™ã€‚
    """
    def __init__(self, d_model, window_size):
        super().__init__()
        self.d_model = d_model
        # window_sizeã¯å¥‡æ•°ã‚’æ¨å¥¨ (ä¾‹: 5 -> è‡ªåˆ†+å‰å¾Œ2)
        self.window_size = window_size 
        
        # Q, K, V ã®ç·šå½¢å¤‰æ›å±¤ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ã¨ã—ã¦ç°¡ç•¥åŒ–ï¼‰
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)

    def forward(self, x):
        # x ã®å½¢çŠ¶: (batch_size, seq_len, d_model)
        batch_size, seq_len, d_model = x.shape

        # 1. Q, K, V ã®è¨ˆç®—
        Q = self.query(x)  
        K = self.key(x)    
        V = self.value(x)  

        # 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆQ * K^Tï¼‰ã®è¨ˆç®—
        # scores ã®å½¢çŠ¶: (batch_size, seq_len, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)

        # 3. ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã®ä½œæˆ
        
        # half_window: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰‡å´ï¼ˆå·¦å³ï¼‰ã«æ³¨ç›®ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        # ä¾‹: window_size=5 ã®å ´åˆ, half_window = 2 (è‡ªåˆ† + å‰2 + å¾Œ2)
        half_window = (self.window_size - 1) // 2
        
        # è·é›¢è¡Œåˆ—ã®ä½œæˆ: (seq_len, seq_len)
        # iè¡Œjåˆ—ã®å€¤ã¯ |i - j| (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ i ã¨ j ã®è·é›¢)
        i = torch.arange(seq_len, device=x.device).unsqueeze(1)
        j = torch.arange(seq_len, device=x.device).unsqueeze(0)
        distance_matrix = torch.abs(i - j)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚¹ã‚¯: è·é›¢ãŒ half_window ä»¥ä¸‹ãªã‚‰ True (æ³¨ç›®è¨±å¯)
        # local_mask ã®å½¢çŠ¶: (seq_len, seq_len)
        local_mask = (distance_matrix <= half_window).to(x.device)

        # 4. ãƒã‚¹ã‚¯ã®é©ç”¨
        # æ³¨ç›®ä¸å¯ãªéƒ¨åˆ† (False) ã®ã‚¹ã‚³ã‚¢ã‚’è² ã®ç„¡é™å¤§ (-torch.inf) ã«è¨­å®š
        # ã“ã‚Œã«ã‚ˆã‚Šã€Softmaxé©ç”¨å¾Œã«ãã®éƒ¨åˆ†ã®é‡ã¿ãŒã‚¼ãƒ­ã«ãªã‚Šã¾ã™ã€‚
        scores = scores.masked_fill(~local_mask, -torch.inf)

        # 5. Softmaxã®é©ç”¨
        attention_weights = F.softmax(scores, dim=-1) # (batch_size, seq_len, seq_len)
        
        # 6. é‡ã¿ã¨Vã®ç©
        output = torch.matmul(attention_weights, V) # (batch_size, seq_len, d_model)

        return output

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
d_model = 64     # ç‰¹å¾´æ¬¡å…ƒ
seq_len = 50     # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
batch_size = 2   # ãƒãƒƒãƒã‚µã‚¤ã‚º
window_size = 7  # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®çª“ã‚µã‚¤ã‚º (ä¾‹: è‡ªåˆ† + å‰å¾Œ3ãƒˆãƒ¼ã‚¯ãƒ³)

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
input_data = torch.randn(batch_size, seq_len, d_model)

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
local_attn_layer = LocalAttention(d_model=d_model, window_size=window_size)

# é †ä¼æ’­ã®å®Ÿè¡Œ
output = local_attn_layer(input_data)

print(f"å…¥åŠ›å½¢çŠ¶: {input_data.shape}")
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")

# ç¢ºèª: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã®ä¸€éƒ¨ã‚’è¡¨ç¤ºã—ã¦ã€ç–çµåˆã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª
# (ã“ã‚Œã¯LocalAttentionã‚¯ãƒ©ã‚¹ã®å†…éƒ¨ã§ã—ã‹ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãŸã‚ã€ç°¡æ˜“çš„ãªç¢ºèªã®ã¿)
# 
# ä»®ã«å†…éƒ¨ã§è¨ˆç®—ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ã®ãƒã‚¹ã‚¯çŠ¶æ…‹ã‚’ç¢ºèªã—ãŸã„å ´åˆ:
# print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚¹ã‚¯ã®å½¢çŠ¶:\n{local_attn_layer.local_mask}")
# å®Ÿéš›ã« True (æ³¨ç›®) ã®æ•°ãŒ seq_len * window_size ç¨‹åº¦ã«ãªã£ã¦ã„ã‚‹ã¯ãšã§ã™ã€‚
```



## Related Work

[Paper Walkthrough - LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://storrs.io/paper-walkthrough-longnet-scaling-transformers-to-1-000-000-000-tokens/)
