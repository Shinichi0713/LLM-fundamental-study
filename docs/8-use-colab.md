# Google Colabを使ったLLMのFT
マイPCではLLMを動作させるにはリソース不足。ファインチューニング/学習なんてさらに難しいということがよくあります。
そんな課題を解決する方法について考えてみました。


## 解決の糸口
使うリソースはGoogle Colabがベストです。
GPUの容量は、無料でもある程度の規模のLLMを動作させるに十分なリソースがあります。
また、必要な環境がデフォルトで整っていて、LLMを動作させるならば非常に良い環境です。
Googleさん、こんな素敵な環境を提供してくださってありがとうございます!!

但し、Google Colab単体ではLLMのファインチューニングはできません。


Google Colabのリソースを使って文章生成可能なLLMをファインチューニングすることは、現在では**PEFT (Parameter-Efficient Fine-Tuning)** という技術の登場により、標準的な手順になっています。

ただし、モデルのサイズとご利用のColabのプランによって、選択すべき手法が異なります。


## 1. 成功の鍵となる技術：LoRAと量子化

Transformerベースの巨大モデルのファインチューニングをColabで行う上で、メモリ不足を回避するための2大必須技術があります。

| **技術**                  | **役割**                                                                                                                                         | **効果**                                                                                     |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------- |
| **LoRA (ローラの採用)**   | **PEFT (Parameter-Efficient Fine-Tuning)**の手法の一つ。モデルの全パラメータを更新せず、ごく一部の小さな追加レイヤー（アダプター）のみを学習させます。 | GPUメモリ使用量を**劇的に削減**し、学習時間を短縮します。                                    |
| **量子化 (Quantization)** | モデルの重み（パラメータ）を通常のFP16/BF16から**4ビット (Int4) や 8ビット (Int8)**に圧縮します。                                                      | モデルのロードに必要なVRAM（GPUメモリ）を大幅に削減します。（例：7Bモデルを16GBのGPUに搭載可能に） |

これらの技術を組み合わせた **QLoRA** を利用すれば、ColabのT4 GPUでも7B（70億パラメータ）クラスのモデルをファインチューニングすることが現実的になります。


## 2. Colabプランごとの実行可能性の目安

| **プラン**     | **推奨されるモデルサイズ** | **使用技術** | **可能な処理**                               |
| -------------------- | -------------------------------- | ------------------ | -------------------------------------------------- |
| **無料版**     | **$\sim$**30億パラメータまで   | LoRA / QLoRA       | 小規模なLLM、または7Bモデルのデモ的な学習          |
| **Colab Pro**  | 70億**$\sim$**130億パラメータ  | QLoRA + 大容量RAM  | 7B/13Bクラスのモデルの本格的なファインチューニング |
| **Colab Pro+** | 130億**$\sim$**340億パラメータ | QLoRA + A100/V100  | より大規模なモデルや、複数エポックでの安定した学習 |

### ⚠️ 注意点

* **T4 GPU の制限** : 無料版で提供される **T4 GPU** は通常VRAMが16GBです。7BモデルをFP16でロードするだけでVRAMの約半分（14GB）を消費するため、ファインチューニングにはLoRA/QLoRAが必須となります。
* **RAM (CPUメモリ)** : データセットのロードや前処理でRAMが不足することがよくあります。データセットを処理する際は、前述の**ストリーミング処理**や**バッチサイズの調整**が重要になります。

### 📌 ファインチューニングの具体的な手順（要約）

1. **モデルロード** : `transformers` ライブラリの `BitsAndBytesConfig` を使って、モデルを4ビット量子化（Int4）しながらGPUにロードします。
2. **LoRA設定** : `peft` ライブラリを使って、ファインチューニング対象のレイヤー（通常はQとKの線形層）を指定し、LoRAアダプターをモデルに追加します。
3. **学習** : Hugging Faceの `Trainer` を利用して学習を実行します。


---

著者は暇があればgithubいじってます。 LLM関係でしたら以下が作品です。 宜しければご覧ください。

https://github.com/Shinichi0713/LLM-fundamental-study
