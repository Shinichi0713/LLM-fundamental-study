# 日本語のトークナイザ

日本語のLLMを開発、あるいはファインチューニングする際に利用するトークナイザとして、現在（2024年〜2025年時点）のトレンドと性能を考慮したおすすめは、以下の3つのアプローチに分かれます。

結論から言うと、**現時点で最も性能バランスが良いのは「Qwen-2.5」のトークナイザ**です。

それぞれの推奨理由と根拠を解説します。

---

### 1. 【イチオシ】Qwen-2.5 Tokenizer

**（Alibaba Cloud開発 / Byte-Level BPE）**

現在、多言語（特に日本語を含むアジア言語）において**最も効率的**だと評価されているトークナイザの一つです。

* **根拠:**
  * **圧縮率（Compression Ratio）が高い** : 日本語の文章をトークン化した際、Llama 3などの他モデルと比較して、**より少ないトークン数**で表現できます。これは、コンテキスト長（一度に入力できる量）の実質的な拡大と、推論・学習速度の向上を意味します。
  * **語彙サイズ (Vocab Size) が適切** : 約152,000語彙を持っています。これはLlama 3 (128k) より大きく、日本語特有の漢字や熟語を「1トークン」として保持している割合が高いです。
  * **Byte-Level BPE** : `tiktoken`ベースの実装であり、未知語（UNK）が発生せず、バイト単位にフォールバックして処理できるため堅牢です。

### 2. 【標準・互換性重視】Llama 3 Tokenizer

**（Meta社開発 / Tiktokenベース）**

世界的なデファクトスタンダードであり、多くの日本語継続事前学習モデル（Swallow, Llama-3-Elyzaなど）がこれをそのまま、あるいは拡張して採用しています。

* **根拠:**
  * **Llama 2からの劇的な改善** : Llama 2 (32k語彙) では日本語がボロボロ（漢字1文字が3バイト＝3トークンになるなど）でしたが、Llama 3 (128k語彙) になり、日本語の効率が大幅に向上しました。
  * **エコシステムの広さ** : 多くのライブラリやツールがLlama 3の形式に最適化されています。ゼロからトークナイザを作るのでなければ、既存のLlama 3ベースのモデルのエコシステムに乗っかるのが最も開発コストが低いです。

### 3. 【国内研究・日本語特化】llm-jp-3 / Fugaku-LLM Tokenizer

**（NIIなどの国内研究機関 / Unigram or BPE）**

日本語の科学技術用語や、日本特有の言い回しを重視する場合の選択肢です。

* **根拠:**
  * **学習データの質** : 日本語のWebコーパス（Common Crawl）だけでなく、高品質な日本語データセットを用いてSentencePiece (Unigram) などで学習されています。
  * **Unigramの採用** : `llm-jp` 系はUnigram言語モデルを採用していることが多いです。BPEと異なり、確率的にトークン分割を行うことができ（Subword Regularization）、少量のデータでの学習において堅牢性が高まるという研究結果があります。

---

### 📊 比較と選定基準

どれを選ぶべきかは、あなたの目的によります。

| **目的**                      | **おすすめ**            | **理由**                                                                          |
| ----------------------------------- | ----------------------------- | --------------------------------------------------------------------------------------- |
| **新規でLLMを事前学習したい** | **Qwen-2.5**            | 日本語の圧縮率が最強クラス。学習効率が良い。                                            |
| **既存モデルを微調整したい**  | **Llama 3**             | ベースモデルの性能が高く、トークナイザも十分実用的。                                    |
| **特殊な日本語ドメイン**      | **SentencePieceで自作** | 専門用語（医療、特許など）が多い場合、専用の語彙でUnigramモデルを学習させるのがベスト。 |

### 🛠️ 実装コード例（Qwen-2.5のロード）

Hugging FaceのTransformersを使えばすぐに試せます。

**Python**

```
from transformers import AutoTokenizer

# Qwen-2.5のトークナイザをロード
model_id = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)

text = "強化学習は非常に奥深く、解決に多くの時間がかかる"

# トークン化
tokens = tokenizer.tokenize(text)
ids = tokenizer.encode(text)

print(f"トークン数: {len(tokens)}")
print(tokens)

# 比較用: Llama 3 なども同様に load して len(ids) を比べると
# Qwenの方がトークン数が少なくなる（＝効率が良い）ことが多いです。
```

**結論として、特に制約がなければ「Qwen-2.5」のトークナイザ（またはそれを利用しているモデル）を出発点にするのが、現在の日本語LLM界隈では「通」な選択とされています。**
