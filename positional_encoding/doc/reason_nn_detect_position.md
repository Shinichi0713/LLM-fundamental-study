素晴らしい質問です。

**「単に足しているだけなのに、なぜ位置がわかるのか？」**

これは Positional Encoding (PE) の本質的な疑問です。

結論から言うと、

> **PE の値が埋め込みに“規則的な位置情報”を刻み込み、
>
> Attention がそれを手掛かりに相対関係を学ぶから**

です。

以下、分解して説明します。

---

## ✅ 1. 加算すると「位置の特徴」が埋め込みに刻まれる

入力語ベクトル (x_i) に対して、Positional Encoding (p_i) を足します:

[

h_i = x_i + p_i

]

もし (x_i) が同じ単語でも、

場所 (i) が違えば **足される (p_i) が違う**ので、

> **同じ単語でも文中の位置ごとに異なるベクトルになる**

例：

| position | word embedding | PE | 結果   |
| -------- | -------------- | -- | ------ |
| 0        | dog            | p0 | dog+p0 |
| 5        | dog            | p5 | dog+p5 |

→ モデルは **“dog”だが、文頭の dog / 文末の dog の違い**を識別できる。

---

## ✅ 2. Sin/Cos により「距離情報」も保持される

Sin/Cos で周期を変えているので

* (\sin(i)) と (\sin(i+1)) の変化 → **近い位置関係**
* (\sin(i)) と (\sin(i+100)) の位相差 → **遠い位置関係**

を数値的に比較できる。

つまり

> **ベクトル空間の幾何学的な距離として、単語の相対位置も含まれる**

Attention する際、Query/Key 内積で距離関係も自然に使われる。

---

## ✅ 3. “加算だけ”で十分な理由

Transformerは線形変換を多層で行います。

[

Attention(Qx, Kx)

]

ここで (x = word + position) なので、

Q,K 生成も

[

Q(word + position) = Q_w(word) + Q_p(position)

]

となる（行列の線形性のおかげ）。

つまりネットワークは

 **語彙情報と位置情報を別々に、かつ同時に扱える** 。

---

## ✅ 4. CNN/RNN がやっていた「順序理解」を数学的に置き換えた

RNN → 時系列処理で自然に“順序”を保持

CNN → kernel の位置バイアス

Transformer → **明示的に位置ベクトルを付与して学習に任せる**

---

## ✅ 結論

> **位置情報を載せた特徴空間にすることで、Attention が自然に順序関係を利用できる。
>
> “加算”で十分に位置が表現できるよう、PE が設計されている。**

---

## ✅ 補足: なぜ「加算」なの？

* 連結(concat)よりパラメータ増えず効率的
* ベクトルを位置方向にずらす “変調” として機能する
* 加算 → 位置が固有ベクトルとして混ざり、学習で分離可能

実験でも

> 加算 > 連結

という結果が一般的です。

---

必要なら以下も解説できます👇

* RoPE と Sinusoidal の違い
* なぜ学習型位置埋め込みよりSin/Cosが強いのか
* PEを抜いたらモデルは何を失うか

どれに興味ありますか？
