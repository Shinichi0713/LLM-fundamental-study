# 相対位置エンコーディング

相対位置を検出する **Relative Positional Encoding（相対位置エンコーディング）** の特徴を、わかりやすくまとめます。


## ✅ Relative Positional Encoding の特徴

### 1. **単語（token）の“相対的な距離”を学習する**

従来の **絶対位置埋め込み (absolute PE)** は

「1番目の単語」「2番目の単語」…のように **固定された順序** を渡しますが、

**相対位置埋め込み** は

「この単語の左に2個目にある単語」

「右隣の単語」

のように **距離関係** をモデルに教えます。

📌 **順番そのものではなく “間隔” を理解する**

> 間隔を理解するための位置関係を提供する

### 2. **文の長さが変わっても性能が安定**

絶対PEは

→ 長さが変わると位置がズレる

相対PEは

→ **距離で判断する**ので、文章長変化に強い

表形式データ、音声、コード理解、長文タスクに有利。

> 相対PEは距離で判断するので文章長変化に強い

### 3. **Attention計算の中に組み込む**

相対PEは、Attentionのスコアに直接足し込みます：



$$
Attention(Q,K) = QK^T + relative_{bias}
$$


📌 **位置を独立した embedding ではなく “Attention の一部” として扱う**

### 4. **順序の移動にロバスト**

「猫が魚を食べた」

「魚を猫が食べた」

のように語順が変わっても、

🍀 **"猫 → 魚"の相対関係** を理解しやすい。

---

### 5. **トランスフォーマーベースで主流になりつつある**

代表例：

| モデル         | 相対PE方式                     |
| -------------- | ------------------------------ |
| Transformer-XL | relative attention             |
| T5             | relative positional bias       |
| DeBERTa        | disentangled relative position |

---

## ✅ Absolute vs Relative の比較

| 観点       | 絶対PE          | 相対PE                |
| ---------- | --------------- | --------------------- |
| 何を学習？ | 位置そのもの    | 距離（相対関係）      |
| 長文耐性   | 弱い            | 強い                  |
| 汎用性     | 中              | 高（順序変化に強い）  |
| 実装場所   | Token embedding | Attention内に埋め込む |

---

## ✅ 直感的な理解例

> 文章を読むとき、
>
> 「この単語は文の3番目」より
>
> 「この単語は主語の近く」
>
> のほうが意味理解に役立つ

相対PEは  **自然な人間の理解方式に近い** 。

---

## ✨ まとめ

| 特徴                   | 内容                                 |
| ---------------------- | ------------------------------------ |
| 位置ではなく距離を学習 | 間隔＝意味手がかり                   |
| 長文に強い             | Transformer弱点の補強                |
| Attentionに統合        | より柔軟で直感的                     |
| 多くの最新モデルで採用 | 特に言語・視覚・マルチモーダルで有効 |

---

## 👇 もし続きの理解を深めたいなら…

👍 コード例

🧠 自作Attentionへの実装

📈 実験（絶対 vs 相対）比較

など、どれを見たいですか？
