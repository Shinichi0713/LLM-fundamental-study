Sinusoidal Positional Encoding（正弦波位置埋め込み）は、

**Transformer の元祖論文（Attention is All You Need）で採用された位置情報付与方式**です。

目的は、**トークンに「順序情報」を数学的に与えること**です。

---

## ✅ 何をしてる仕組み？

各トークンの埋め込みベクトルに

**位置依存の sin/cos 波を足し込んで**

位置による差を与えます。

### 直感的理解

* 位置 1 → 特定の波の値
* 位置 2 → その波が少し進んだ値
* 位置 50 → 周期が進んだ値

つまり、 **波の位相で位置を表す** 。

---

## ✅ 数式

位置 $pos$、次元 $i$ のとき：

$$
PE_{(pos,2i)} = \sin\left(pos / 10000^{2i/d_{model}}\right)
$$

$$
PE_{(pos,2i+1)} = \cos\left(pos / 10000^{2i/d_{model}}\right)
$$

* 偶数次元 → sin
* 奇数次元 → cos
* $10000^{2i/d_{model}}$ で周波数を変化させる

---

## ✅ なぜ sin と cos を使うの？

| 理由                         | 説明                               |
| ---------------------------- | ---------------------------------- |
| **周期構造**           | 繰り返しで長距離まで識別可能       |
| **連続性**             | 学習してない位置にも自然に拡張可能 |
| **相対位置を表現可能** | sin/cos の位相差は相対距離に対応   |
| **逆算性がある**       | 位置の差を数学的に推定可能         |

---

## ✅ なぜ複数の周波数？

> 短周期〜長周期の波を組み合わせて
>
> **近い距離も遠い距離も区別できるようにする**

例：

* 2π周期の波 → 近い位置関係に敏感
* 10,000π周期の波 → 長距離の構造把握

複数の周期の和は、フーリエ変換の考え方に似ています。

---

## ✅ 実際の動作イメージ

位置ごとに波形が少しずつずれる：

```
pos 1:  ~~~~
pos 2:   ~~~~
pos 3:    ~~~~
```

それぞれの次元で波が違うため、

位置ごとに **ユニークなベクトル** になります。

---

## ✅ ベクトルの例（イメージ）

| pos | dim1(sin) | dim2(cos) | dim3(sin long) | … |
| --- | --------- | --------- | -------------- | -- |
| 1   | 0.84      | 0.54      | 0.01           | … |
| 2   | 0.91      | 0.41      | 0.02           | … |
| 3   | 0.14      | –0.98    | 0.03           | … |

👉 どの位置も違うパターンになる

---

## ✅ 長所と短所

### 長所

* パラメータ不要（学習なし）
* 外挿性（見たことない長さにも対応）
* 相対位置情報を自然に表現できる

### 短所

* 超長文で劣化しやすい
* 学習ベースより柔軟性が低い
* 現代モデルでは RoPE や ALiBi が主流

> ※ 近年の LLM（GPT-3/4/5, LLaMA, Qwen等）は
>
> Sinusoidalではなく **RoPE が主流**

---

## ✅ Pythonコード例（簡単）

```python
import numpy as np

def sinusoidal_position_encoding(max_len, d_model):
    pos = np.arange(max_len)$$:, np.newaxis$$
    i = np.arange(d_model)$$np.newaxis, :$$
    angle = pos / np.power(10000, (2*(i//2))/np.float32(d_model))

    pe = np.zeros((max_len, d_model))
    pe$$:, 0::2$$ = np.sin(angle$$:, 0::2$$)
    pe$$:, 1::2$$ = np.cos(angle$$:, 1::2$$)
    return pe
```

---

## ✅ まとめ

| 項目 | 内容                       |
| ---- | -------------------------- |
| 目的 | 位置（順序）の表現         |
| 方法 | トークンに sin/cos波を付加 |
| 特徴 | 複数周期で相対位置も表現   |
| 利点 | 外挿性・パラメータなし     |
| 限界 | 長文に弱い → RoPEが主流   |

上のコードで、**Transformer の Positional Encoding を NumPyで実装し、
ヒートマップと波形で視覚化**しました。

### ✅ 見るポイント

| 図                 | 見るべきこと                                   |
| ------------------ | ---------------------------------------------- |
| ヒートマップ       | 各位置×次元で正弦波が混ざっている様子         |
| 波形（数次元のみ） | 次元ごとに周期が違う → 位置を一意に表現できる |

* 低次元 → 速い波（近い位置の変化に敏感）
* 高次元 → 遅い波（長距離依存も捉える）

これにより Transformer は、**文中の「どこにある単語か」を数学的に理解**できます。



## 数学的根拠

Sinusoidal Positional Encoding（正弦波位置エンコーディング）は、Transformerで用いられる**位置情報を連続的・周期的に表す数学的手法**です。数学的にみると、以下の性質を利用して「位置」をベクトルとして表現しています。

---

## ✅ Sinusoidal PE の式

位置 ( pos )、次元 ( i ) のとき

[

PE_{(pos,,2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)

]

[

PE_{(pos,,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)

]

* ( d )：埋め込み次元
* ( 10000^{2i/d} )：波のスケール（波長）を変える係数

**多周波（multi-frequency）サイン波**で位置を符号化している。

---

## ✅ 数学的解釈ポイント

### ① **フーリエ基底（Fourier basis）**

Sinusoidal PE は**フーリエ級数展開の一種**です。

* 周波数の異なる正弦波・余弦波を重ねている
* → 任意の位置を**一意に**表現できる

つまり、位置を**周波数空間（Fourier space）に写像**している。

> 位置 = 信号，
>
> PE = その信号を多周波正弦波に分解した座標

---

### ② **距離（差分）を線形に計算できる**

[

\sin(a + b) = \sin a \cos b + \cos a \sin b

]

この三角関数の恒等式により、

[

PE(pos+k) \quad \text{を} \quad PE(pos) \text{の線形変換で表せる}

]

つまり、

> 位置差 ( k ) を線形演算で扱える → **相対位置情報の保持**

Transformerは線形演算が中心なので、

相対位置が自然に扱えるようになる。

（自己注意で距離を認識できる理由）

---

### ③ **外挿（extrapolation）できる**

* 学習で見た位置より大きな位置（長い文）
* 未来の時系列データ

などでも、**sin/cos は周期関数なので破綻しない**

→ GPT/Transformerが**未知の長さに対応**できる根拠

---

### ④ **周期＋多スケール**

* 小さい波長 → 近い関係を捉える
* 大きい波長 → 長距離依存も捉える

電波通信の多周波変調や小波変換と似た性質。

---

## ✅ 直感的イメージ

| 機能             | 数学的意味    | 直感                      |
| ---------------- | ------------- | ------------------------- |
| 固有の位置表現   | Fourier basis | 場所を固有の音階で表す    |
| 相対位置が分かる | 加法公式      | 「隣の音」と区別できる    |
| 一般化可能       | 周期性        | 長い文章でも壊れない      |
| 多スケール       | 周波数バンド  | 近距離/長距離を同時に表現 |

---

## ✅ なぜ Attention と相性が良い？

Attentionは

[

QK^T

]

という**内積（線形演算）**で関係を測る仕組み。

Sinusoidal PE は

* 位相差（= 位置差）を内積で推定できる
* 三角関数の和積公式で理論的裏付け

→ 相対位置が自然に抽出される

---

## 🎯 まとめ

| Sinusoidal PE の数学的意味                   |
| -------------------------------------------- |
| フーリエ基底による位置の周波数表現           |
| 内積で相対距離を復元できる（三角関数恒等式） |
| 長さに外挿できる（周期性）                   |
| 多スケールに位置依存性を表現                 |

**位置 = 波（signal）**として捉えているのが本質。

---

## 📌 補足：なぜ learned PE より好まれる場面がある？

| Sinusoidal         | Learned                |
| ------------------ | ---------------------- |
| 一般化力高い       | 訓練データ長に依存     |
| 理論的に相対性保持 | 直接は相対性を学ばない |
| 長文で安定         | 長文弱くなる場合       |
