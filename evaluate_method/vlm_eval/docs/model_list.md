{'AKI': functools.partial(<class 'vlmeval.vlm.aki.AKI'>, name='AKI', ckpt_pth='Sony/AKI-4B-phi-3.5-mini'), 'TransCore_M': functools.partial(<class 'vlmeval.vlm.transcore_m.TransCoreM'>, root=None), 'PandaGPT_13B': functools.partial(<class 'vlmeval.vlm.pandagpt.PandaGPT'>, name='PandaGPT_13B', root=None), 'flamingov2': functools.partial(<class 'vlmeval.vlm.open_flamingo.OpenFlamingo'>, name='v2', mpt_pth='anas-awadalla/mpt-7b', ckpt_pth='openflamingo/OpenFlamingo-9B-vitl-mpt7b'), 'VisualGLM_6b': functools.partial(<class 'vlmeval.vlm.visualglm.VisualGLM'>, model_path='THUDM/visualglm-6b'), 'mPLUG-Owl2': functools.partial(<class 'vlmeval.vlm.mplug_owl2.mPLUG_Owl2'>, model_path='MAGAer13/mplug-owl2-llama2-7b'), 'mPLUG-Owl3': functools.partial(<class 'vlmeval.vlm.mplug_owl3.mPLUG_Owl3'>, model_path='mPLUG/mPLUG-Owl3-7B-240728'), 'OmniLMM_12B': functools.partial(<class 'vlmeval.vlm.omnilmm.OmniLMM12B'>, model_path='openbmb/OmniLMM-12B', root=None), 'MGM_7B': functools.partial(<class 'vlmeval.vlm.mgm.Mini_Gemini'>, model_path='YanweiLi/MGM-7B-HD', root=None), 'Bunny-llama3-8B': functools.partial(<class 'vlmeval.vlm.bunnyllama3.BunnyLLama3'>, model_path='BAAI/Bunny-v1_1-Llama-3-8B-V'), 'VXVERSE': functools.partial(<class 'vlmeval.vlm.vxverse.VXVERSE'>, model_name='XVERSE-V-13B', root=None), '360VL-70B': functools.partial(<class 'vlmeval.vlm.qh_360vl.QH_360VL'>, model_path='qihoo360/360VL-70B'), 'Llama-3-MixSenseV1_1': functools.partial(<class 'vlmeval.vlm.mixsense.LLama3Mixsense'>, model_path='Zero-Vision/Llama-3-MixSenseV1_1'), 'Parrot': functools.partial(<class 'vlmeval.vlm.parrot.Parrot'>, model_path='AIDC-AI/Parrot-7B'), 'OmChat': functools.partial(<class 'vlmeval.vlm.omchat.OmChat'>, model_path='omlab/omchat-v2.0-13B-single-beta_hf'), 'RBDash_72b': functools.partial(<class 'vlmeval.vlm.rbdash.RBDash'>, model_path='RBDash-Team/RBDash-v1.5', root=None), 'Pixtral-12B': functools.partial(<class 'vlmeval.vlm.pixtral.Pixtral'>, model_path='mistralai/Pixtral-12B-2409'), 'Falcon2-VLM-11B': functools.partial(<class 'vlmeval.vlm.falcon_vlm.Falcon2VLM'>, model_path='tiiuae/falcon-11B-vlm'), 'o1': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='o1-2024-12-17', key=None, api_base=None, temperature=0, img_detail='high', retry=3, timeout=1800, max_tokens=16384, verbose=True), 'o3': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='o3-2025-04-16', key=None, api_base=None, temperature=0, img_detail='high', retry=3, timeout=1800, max_tokens=16384, verbose=True), 'o4-mini': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='o4-mini-2025-04-16', key=None, api_base=None, temperature=0, img_detail='high', retry=3, timeout=1800, max_tokens=16384, verbose=True), 'GPT4V': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4-1106-vision-preview', temperature=0, img_size=512, img_detail='low', retry=10, verbose=True), 'GPT4V_HIGH': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4-1106-vision-preview', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'GPT4V_20240409': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4-turbo-2024-04-09', temperature=0, img_size=512, img_detail='low', retry=10, verbose=True), 'GPT4V_20240409_HIGH': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4-turbo-2024-04-09', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'GPT4o': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4o-2024-05-13', temperature=0, img_size=512, img_detail='low', retry=10, verbose=True), 'GPT4o_HIGH': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4o-2024-05-13', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'GPT4o_20240806': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4o-2024-08-06', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'GPT4o_20241120': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4o-2024-11-20', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'ChatGPT4o': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='chatgpt-4o-latest', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'GPT4o_MINI': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4o-mini-2024-07-18', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'GPT4.5': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4.5-preview-2025-02-27', temperature=0, timeout=600, img_size=-1, img_detail='high', retry=10, verbose=True), 'gpt-4.1-2025-04-14': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4.1-2025-04-14', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'gpt-4.1-mini-2025-04-14': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4.1-mini-2025-04-14', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'gpt-4.1-nano-2025-04-14': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-4.1-nano-2025-04-14', temperature=0, img_size=-1, img_detail='high', retry=10, verbose=True), 'gpt-5-2025-08-07': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5-2025-08-07', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'gpt-5-mini-2025-08-07': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5-mini-2025-08-07', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'gpt-5-nano-2025-08-07': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5-nano-2025-08-07', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'gpt-5.1-2025-11-13': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5.1-2025-11-13', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'GeminiPro1-0': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-1.0-pro', temperature=0, retry=10), 'GeminiPro1-5': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-1.5-pro', temperature=0, retry=10), 'GeminiFlash1-5': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-1.5-flash', temperature=0, retry=10), 'GeminiPro1-5-002': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gemini-1.5-pro-002', temperature=0, retry=10), 'GeminiFlash1-5-002': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gemini-1.5-flash-002', temperature=0, retry=10), 'GeminiFlash2-0': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-2.0-flash', temperature=0, retry=10), 'GeminiFlashLite2-0': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-2.0-flash-lite', temperature=0, retry=10), 'GeminiFlash2-5': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-2.5-flash', temperature=0, retry=10), 'GeminiPro2-5': functools.partial(<class 'vlmeval.api.gemini.Gemini'>, model='gemini-2.5-pro', temperature=0, retry=10), 'QwenVLPlus': functools.partial(<class 'vlmeval.api.qwen_vl_api.QwenVLAPI'>, model='qwen-vl-plus', temperature=0, retry=10), 'QwenVLMax': functools.partial(<class 'vlmeval.api.qwen_vl_api.QwenVLAPI'>, model='qwen-vl-max', temperature=0, retry=10), 'QwenVLMax-250408': functools.partial(<class 'vlmeval.api.qwen_vl_api.QwenVLAPI'>, model='qwen-vl-max-2025-04-08', temperature=0, retry=10), 'RekaEdge': functools.partial(<class 'vlmeval.api.reka.Reka'>, model='reka-edge-20240208'), 'RekaFlash': functools.partial(<class 'vlmeval.api.reka.Reka'>, model='reka-flash-20240226'), 'RekaCore': functools.partial(<class 'vlmeval.api.reka.Reka'>, model='reka-core-20240415'), 'Step1V': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='step-1v-32k', api_base='[https://api.stepfun.com/v1/chat/completions](https://api.stepfun.com/v1/chat/completions)', temperature=0, retry=10, img_size=-1, img_detail='high'), 'Step1.5V-mini': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='step-1.5v-mini', api_base='[https://api.stepfun.com/v1/chat/completions](https://api.stepfun.com/v1/chat/completions)', temperature=0, retry=10, img_size=-1, img_detail='high'), 'Step1o': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='step-1o-vision-32k', api_base='[https://api.stepfun.com/v1/chat/completions](https://api.stepfun.com/v1/chat/completions)', temperature=0, retry=10, img_size=-1, img_detail='high'), 'Yi-Vision': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='yi-vision', api_base='[https://api.lingyiwanwu.com/v1/chat/completions](https://api.lingyiwanwu.com/v1/chat/completions)', temperature=0, retry=10), 'Claude3V_Opus': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-3-opus-20240229', temperature=0, retry=10, verbose=True), 'Claude3V_Sonnet': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-3-sonnet-20240229', temperature=0, retry=10, verbose=True), 'Claude3V_Haiku': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-3-haiku-20240307', temperature=0, retry=10, verbose=True), 'Claude3-5V_Sonnet': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-3-5-sonnet-20240620', temperature=0, retry=10, verbose=True), 'Claude3-5V_Sonnet_20241022': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-3-5-sonnet-20241022', temperature=0, retry=10, verbose=True), 'Claude3-7V_Sonnet': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-3-7-sonnet-20250219', temperature=0, retry=10, verbose=True), 'Claude4_Opus': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-4-opus-20250514', temperature=0, retry=10, verbose=True, timeout=1800), 'Claude4_Sonnet': functools.partial(<class 'vlmeval.api.claude.Claude3V'>, model='claude-4-sonnet-20250514', temperature=0, retry=10, verbose=True, timeout=1800), 'GLM4V': functools.partial(<class 'vlmeval.api.glm_vision.GLMVisionAPI'>, model='glm4v-biz-eval', temperature=0, retry=10), 'GLM4V_PLUS': functools.partial(<class 'vlmeval.api.glm_vision.GLMVisionAPI'>, model='glm-4v-plus', temperature=0, retry=10), 'GLM4V_PLUS_20250111': functools.partial(<class 'vlmeval.api.glm_vision.GLMVisionAPI'>, model='glm-4v-plus-0111', temperature=0, retry=10), 'abab6.5s': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='abab6.5s-chat', api_base='[https://api.minimax.chat/v1/chat/completions](https://api.minimax.chat/v1/chat/completions)', temperature=0, retry=10), 'abab7-preview': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='abab7-chat-preview', api_base='[https://api.minimax.chat/v1/chat/completions](https://api.minimax.chat/v1/chat/completions)', temperature=0, retry=10), 'CongRong-v1.5': functools.partial(<class 'vlmeval.api.cloudwalk.CWWrapper'>, model='cw-congrong-v1.5', temperature=0, retry=10), 'CongRong-v2.0': functools.partial(<class 'vlmeval.api.cloudwalk.CWWrapper'>, model='cw-congrong-v2.0', temperature=0, retry=10), 'SenseNova-V6-Pro': functools.partial(<class 'vlmeval.api.sensechat_vision.SenseChatVisionAPI'>, model='SenseNova-V6-Pro', temperature=0, retry=10), 'SenseNova-V6-Reasoner': functools.partial(<class 'vlmeval.api.sensechat_vision.SenseChatVisionAPI'>, model='SenseNova-V6-Reasoner', temperature=0, retry=10), 'SenseNova-V6-5-Pro': functools.partial(<class 'vlmeval.api.sensechat_vision.SenseChatVisionAPI'>, model='SenseNova-V6-5-Pro', retry=10), 'HunYuan-Vision': functools.partial(<class 'vlmeval.api.hunyuan.HunyuanVision'>, model='hunyuan-vision', temperature=0, retry=10), 'HunYuan-Standard-Vision': functools.partial(<class 'vlmeval.api.hunyuan.HunyuanVision'>, model='hunyuan-standard-vision', temperature=0, retry=10), 'HunYuan-Large-Vision': functools.partial(<class 'vlmeval.api.hunyuan.HunyuanVision'>, model='hunyuan-large-vision', temperature=0, retry=10), 'BailingMM-Lite-1203': functools.partial(<class 'vlmeval.api.bailingmm.bailingMMAPI'>, model='BailingMM-Lite-1203', temperature=0, retry=10), 'BailingMM-Pro-0120': functools.partial(<class 'vlmeval.api.bailingmm.bailingMMAPI'>, model='BailingMM-Pro-0120', temperature=0, retry=10), 'BlueLM-2.5-3B': functools.partial(<class 'vlmeval.api.bluelm_api.BlueLM_API'>, model='BlueLM-2.5-3B', temperature=0, retry=3), 'JTVL': functools.partial(<class 'vlmeval.api.jt_vl_chat.JTVLChatAPI'>, model='jt-vl-chat', temperature=0, retry=10), 'JTVL-Mini': functools.partial(<class 'vlmeval.api.jt_vl_chat_mini.JTVLChatAPI_Mini'>, model='jt-vl-chat-mini', temperature=0, retry=10), 'JTVL-2B': functools.partial(<class 'vlmeval.api.jt_vl_chat_mini.JTVLChatAPI_2B'>, model='jt-vl-chat-2b', temperature=0, retry=10), 'VideoChatOnlineV2': functools.partial(<class 'vlmeval.api.video_chat_online_v2.VideoChatOnlineV2API'>, model='videochatonline_v2', temperature=0, retry=10), 'Taiyi': functools.partial(<class 'vlmeval.api.taiyi.TaiyiAPI'>, model='taiyi', temperature=0, retry=10), 'TeleMM': functools.partial(<class 'vlmeval.api.siliconflow.TeleMMAPI'>, model='TeleAI/TeleMM', temperature=0, retry=10), 'TeleMM2.0': functools.partial(<class 'vlmeval.api.telemm.TeleMM2_API'>, model='TeleAI/TeleMM', retry=3, timeout=600), 'TeleMM2.0Thinking': functools.partial(<class 'vlmeval.api.telemm_thinking.TeleMM2Thinking_API'>, model='TeleAI/TeleMM', retry=3, timeout=600), 'Qwen2.5-VL-32B-Instruct-SiliconFlow': functools.partial(<class 'vlmeval.api.siliconflow.SiliconFlowAPI'>, model='Qwen/Qwen2.5-VL-32B-Instruct', temperature=0, retry=10), 'Qwen3-VL-8B--crop--arm_thinker_prompt--sglang': functools.partial(<class 'vlmeval.api.arm_thinker.ARM_thinker'>, mode='agent', agent_repo_root='/path/to/your/ARM-Thinker', model='Qwen/Qwen3-VL-8B-Instruct', retry=10, timeout=300, api_base='[http://100.97.158.184:38888/v1/chat/completions](http://100.97.158.184:38888/v1/chat/completions)', key='EMPTY', temperature=0.0, max_tokens=4096, max_round=16, max_tool_response_length=4096, tool_config_path='/path/to/your/ARM-Thinker/examples/self/multiturn/config/tool_config/image_zoom_in_tool_config.yaml', use_role_tool=False, system_template_type='CommonSystemTemplate', extra_pt='\n\n**Important Requirement:**\nThe given image is `original_image`. You must output your reasoning inside `<think>...</think>`. After reasoning, either output the final answer within `<answer>...</answer>` or call a tool within `<tool_call>...</tool_call>`. You may call tools multiple times across turns to assist with judgment or verification, **but only one tool per turn**. If a tool call fails, you can retry or stop and give your final answer. Once no more tool calls are needed, provide your final answer or judgment within `<answer>...</answer>`.'), 'Qwen3-VL-8B--crop--official_prompt--vllm': functools.partial(<class 'vlmeval.api.arm_thinker.ARM_thinker'>, mode='agent', agent_repo_root='/path/to/your/ARM-Thinker', model='Qwen/Qwen3-VL-8B-Instruct', retry=10, timeout=300, api_base='[http://100.97.203.103:40001/v1/chat/completions](http://100.97.203.103:40001/v1/chat/completions)', key='EMPTY', temperature=0.0, max_tokens=4096, extra_pt='', max_round=16, max_tool_response_length=4096, system_template_type='Qwen3VLSystemTemplateWithTools', tool_config_path='/path/to/your/ARM-Thinker/examples/self/multiturn/config/tool_config/image_zoom_in_tool_qwen3vl_config.yaml', use_role_tool=True), 'lmdeploy': functools.partial(<class 'vlmeval.api.lmdeploy.LMDeployAPI'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=10), 'lmdeploy_internvl_78B_MPO': functools.partial(<class 'vlmeval.api.lmdeploy.LMDeployAPI'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=10, timeout=100), 'lmdeploy_qvq_72B_preview': functools.partial(<class 'vlmeval.api.lmdeploy.LMDeployAPI'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=10, timeout=300), 'Taichu-VLR-3B': functools.partial(<class 'vlmeval.api.taichu.TaichuVLRAPI'>, model='taichu_vlr_3b', url='[https://platform.wair.ac.cn/maas/v1/chat/completions](https://platform.wair.ac.cn/maas/v1/chat/completions)'), 'Taichu-VLR-7B': functools.partial(<class 'vlmeval.api.taichu.TaichuVLRAPI'>, model='taichu_vlr_7b', url='[https://platform.wair.ac.cn/maas/v1/chat/completions](https://platform.wair.ac.cn/maas/v1/chat/completions)'), 'DoubaoVL': functools.partial(<class 'vlmeval.api.doubao_vl_api.DoubaoVL'>, model='Doubao-1.5-vision-pro', temperature=0, retry=3, verbose=True), 'Seed1.5-VL': functools.partial(<class 'vlmeval.api.doubao_vl_api.DoubaoVL'>, model='doubao-1-5-thinking-vision-pro-250428', temperature=0, retry=3, verbose=True, max_tokens=16384), 'Seed1.6': functools.partial(<class 'vlmeval.api.doubao_vl_api.DoubaoVL'>, model='doubao-seed-1.6-250615', temperature=0, retry=3, verbose=True, max_tokens=16384), 'Seed1.6-Flash': functools.partial(<class 'vlmeval.api.doubao_vl_api.DoubaoVL'>, model='doubao-seed-1.6-flash-250615', temperature=0, retry=3, verbose=True, max_tokens=16384), 'Seed1.6-Thinking': functools.partial(<class 'vlmeval.api.doubao_vl_api.DoubaoVL'>, model='doubao-seed-1.6-thinking-250615', temperature=0, retry=3, verbose=True, max_tokens=16384), 'MUG-U-7B': functools.partial(<class 'vlmeval.api.mug_u.MUGUAPI'>, model='MUG-U', temperature=0, retry=10, verbose=True, timeout=300), 'grok-vision-beta': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='grok-vision-beta', api_base='[https://api.x.ai/v1/chat/completions](https://api.x.ai/v1/chat/completions)', temperature=0, retry=10), 'grok-2-vision-1212': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='grok-2-vision', api_base='[https://api.x.ai/v1/chat/completions](https://api.x.ai/v1/chat/completions)', temperature=0, retry=10), 'grok-4-0709': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='grok-4-0709', api_base='[https://api.x.ai/v1/chat/completions](https://api.x.ai/v1/chat/completions)', temperature=0, retry=3, timeout=1200, max_tokens=16384), 'moonshot-v1-8k': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='moonshot-v1-8k-vision-preview', api_base='[https://api.moonshot.cn/v1/chat/completions](https://api.moonshot.cn/v1/chat/completions)', temperature=0, retry=10), 'moonshot-v1-32k': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='moonshot-v1-32k-vision-preview', api_base='[https://api.moonshot.cn/v1/chat/completions](https://api.moonshot.cn/v1/chat/completions)', temperature=0, retry=10), 'moonshot-v1-128k': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='moonshot-v1-128k-vision-preview', api_base='[https://api.moonshot.cn/v1/chat/completions](https://api.moonshot.cn/v1/chat/completions)', temperature=0, retry=10), 'ernie4.5-turbo': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='ernie-4.5-turbo-vl-32k', temperature=0, retry=3, max_tokens=12000), 'ernie4.5-a3b': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='ernie-4.5-vl-28b-a3b', temperature=0, retry=3, max_tokens=8000), 'gpt-5': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5-2025-08-07', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'gpt-5-mini': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5-mini-2025-08-07', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'gpt-5-nano': functools.partial(<class 'vlmeval.api.gpt.GPT4V'>, model='gpt-5-nano-2025-08-07', img_detail='high', retry=3, verbose=True, max_tokens=16384, timeout=300), 'llava-internlm2-7b': functools.partial(<class 'vlmeval.vlm.llava.llava_xtuner.LLaVA_XTuner'>, llm_path='internlm/internlm2-chat-7b', llava_path='xtuner/llava-internlm2-7b', visual_select_layer=-2, prompt_template='internlm2_chat'), 'llava-internlm2-20b': functools.partial(<class 'vlmeval.vlm.llava.llava_xtuner.LLaVA_XTuner'>, llm_path='internlm/internlm2-chat-20b', llava_path='xtuner/llava-internlm2-20b', visual_select_layer=-2, prompt_template='internlm2_chat'), 'llava-internlm-7b': functools.partial(<class 'vlmeval.vlm.llava.llava_xtuner.LLaVA_XTuner'>, llm_path='internlm/internlm-chat-7b', llava_path='xtuner/llava-internlm-7b', visual_select_layer=-2, prompt_template='internlm_chat'), 'llava-v1.5-7b-xtuner': functools.partial(<class 'vlmeval.vlm.llava.llava_xtuner.LLaVA_XTuner'>, llm_path='lmsys/vicuna-7b-v1.5', llava_path='xtuner/llava-v1.5-7b-xtuner', visual_select_layer=-2, prompt_template='vicuna'), 'llava-v1.5-13b-xtuner': functools.partial(<class 'vlmeval.vlm.llava.llava_xtuner.LLaVA_XTuner'>, llm_path='lmsys/vicuna-13b-v1.5', llava_path='xtuner/llava-v1.5-13b-xtuner', visual_select_layer=-2, prompt_template='vicuna'), 'llava-llama-3-8b': functools.partial(<class 'vlmeval.vlm.llava.llava_xtuner.LLaVA_XTuner'>, llm_path='xtuner/llava-llama-3-8b-v1_1', llava_path='xtuner/llava-llama-3-8b-v1_1', visual_select_layer=-2, prompt_template='llama3_chat'), 'qwen_base': functools.partial(<class 'vlmeval.vlm.qwen_vl.QwenVL'>, model_path='Qwen/Qwen-VL'), 'qwen_chat': functools.partial(<class 'vlmeval.vlm.qwen_vl.QwenVLChat'>, model_path='Qwen/Qwen-VL-Chat'), 'monkey': functools.partial(<class 'vlmeval.vlm.monkey.Monkey'>, model_path='echo840/Monkey'), 'monkey-chat': functools.partial(<class 'vlmeval.vlm.monkey.MonkeyChat'>, model_path='echo840/Monkey-Chat'), 'minimonkey': functools.partial(<class 'vlmeval.vlm.minimonkey.MiniMonkey'>, model_path='mx262/MiniMonkey'), 'llava_v1.5_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA'>, model_path='liuhaotian/llava-v1.5-7b'), 'llava_v1.5_13b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA'>, model_path='liuhaotian/llava-v1.5-13b'), 'llava_v1_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA'>, model_path='Please set your local path to LLaVA-7B-v1.1 here, the model weight is obtained by merging LLaVA delta weight based on vicuna-7b-v1.1 in [https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md) with vicuna-7b-v1.1. '), 'sharegpt4v_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA'>, model_path='Lin-Chen/ShareGPT4V-7B'), 'sharegpt4v_13b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA'>, model_path='Lin-Chen/ShareGPT4V-13B'), 'llava_next_vicuna_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-v1.6-vicuna-7b-hf'), 'llava_next_vicuna_13b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-v1.6-vicuna-13b-hf'), 'llava_next_mistral_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-v1.6-mistral-7b-hf'), 'llava_next_yi_34b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-v1.6-34b-hf'), 'llava_next_llama3': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llama3-llava-next-8b-hf'), 'llava_next_72b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-next-72b-hf'), 'llava_next_110b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-next-110b-hf'), 'llava_next_qwen_32b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next2'>, model_path='lmms-lab/llava-next-qwen-32b'), 'llava_next_interleave_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-interleave-qwen-7b-hf'), 'llava_next_interleave_7b_dpo': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_Next'>, model_path='llava-hf/llava-interleave-qwen-7b-dpo-hf'), 'llava-onevision-qwen2-0.5b-ov-hf': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision_HF'>, model_path='llava-hf/llava-onevision-qwen2-0.5b-ov-hf'), 'llava-onevision-qwen2-0.5b-si-hf': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision_HF'>, model_path='llava-hf/llava-onevision-qwen2-0.5b-si-hf'), 'llava-onevision-qwen2-7b-ov-hf': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision_HF'>, model_path='llava-hf/llava-onevision-qwen2-7b-ov-hf'), 'llava-onevision-qwen2-7b-si-hf': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision_HF'>, model_path='llava-hf/llava-onevision-qwen2-7b-si-hf'), 'llava_onevision_qwen2_0.5b_si': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/llava-onevision-qwen2-0.5b-si'), 'llava_onevision_qwen2_7b_si': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/llava-onevision-qwen2-7b-si'), 'llava_onevision_qwen2_72b_si': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/llava-onevision-qwen2-72b-si'), 'llava_onevision_qwen2_0.5b_ov': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/llava-onevision-qwen2-0.5b-ov'), 'llava_onevision_qwen2_7b_ov': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/llava-onevision-qwen2-7b-ov'), 'llava_onevision_qwen2_72b_ov': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/llava-onevision-qwen2-72b-ov-sft'), 'Aquila-VL-2B': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='BAAI/Aquila-VL-2B-llava-qwen'), 'llava_video_qwen2_7b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/LLaVA-Video-7B-Qwen2'), 'llava_video_qwen2_72b': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision'>, model_path='lmms-lab/LLaVA-Video-72B-Qwen2'), 'granite_vision_3.1_2b_preview': functools.partial(<class 'vlmeval.vlm.granite_vision.granite_vision.GraniteVision3'>, model_path='ibm-granite/granite-vision-3.1-2b-preview'), 'granite_vision_3.2_2b': functools.partial(<class 'vlmeval.vlm.granite_vision.granite_vision.GraniteVision3'>, model_path='ibm-granite/granite-vision-3.2-2b'), 'granite_vision_3.3_2b': functools.partial(<class 'vlmeval.vlm.granite_vision.granite_vision.GraniteVision3'>, model_path='ibm-granite/granite-vision-3.3-2b'), 'InternVL-Chat-V1-1': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL-Chat-V1-1', version='V1.1'), 'InternVL-Chat-V1-2': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL-Chat-V1-2', version='V1.2'), 'InternVL-Chat-V1-2-Plus': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL-Chat-V1-2-Plus', version='V1.2'), 'InternVL-Chat-V1-5': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL-Chat-V1-5', version='V1.5'), 'InternVL2-1B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-1B', version='V2.0'), 'InternVL2-2B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-2B', version='V2.0'), 'InternVL2-4B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-4B', version='V2.0'), 'InternVL2-8B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-8B', version='V2.0'), 'InternVL2-26B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-26B', version='V2.0'), 'InternVL2-40B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-40B', version='V2.0'), 'InternVL2-76B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-Llama3-76B', version='V2.0'), 'InternVL2-8B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-8B-MPO', version='V2.0'), 'InternVL2-8B-MPO-CoT': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2-8B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-1B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-1B', version='V2.0'), 'InternVL2_5-2B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-2B', version='V2.0'), 'QTuneVL1-2B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='hanchaow/QTuneVL1-2B', version='V2.0'), 'InternVL2_5-4B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-4B', version='V2.0'), 'InternVL2_5-8B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-8B', version='V2.0'), 'InternVL2_5-26B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-26B', version='V2.0'), 'InternVL2_5-38B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-38B', version='V2.0'), 'InternVL2_5-78B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-78B', version='V2.0'), 'InternVL2_5-8B-BoN-8': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-8B', version='V2.0', best_of_n=8, reward_model_path='OpenGVLab/VisualPRM-8B'), 'Mini-InternVL-Chat-2B-V1-5': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/Mini-InternVL-Chat-2B-V1-5', version='V1.5'), 'Mini-InternVL-Chat-4B-V1-5': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/Mini-InternVL-Chat-4B-V1-5', version='V1.5'), 'InternVL2_5-1B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-1B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-2B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-2B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-4B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-4B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-8B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-8B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-26B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-26B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-38B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-38B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-78B-MPO': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL2_5-78B-MPO', version='V2.0', use_mpo_prompt=True), 'InternVL2_5-8B-GUI': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='/fs-computility/mllm1/shared/zhaoxiangyu/models/internvl2_5_8b_internlm2_5_7b_dynamic_res_stage1', version='V2.0', max_new_tokens=512, screen_parse=False), 'InternVL3-7B-GUI': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='/fs-computility/mllm1/shared/zhaoxiangyu/GUI/checkpoints/internvl3_7b_dynamic_res_stage1_56/', version='V2.0', max_new_tokens=512, screen_parse=False), 'InternVL3-1B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-1B', version='V2.0'), 'InternVL3-2B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-2B', version='V2.0'), 'InternVL3-8B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-8B', version='V2.0'), 'InternVL3-9B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-9B', version='V2.0'), 'InternVL3-14B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-14B', version='V2.0'), 'InternVL3-38B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-38B', version='V2.0'), 'InternVL3-78B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3-78B', version='V2.0'), 'InternVL3_5-1B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-1B', version='V2.0'), 'InternVL3_5-2B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-2B', version='V2.0'), 'InternVL3_5-4B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-4B', version='V2.0'), 'InternVL3_5-8B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-8B', version='V2.0'), 'InternVL3_5-14B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-14B', version='V2.0'), 'InternVL3_5-GPT-OSS-20B-A4B-Preview': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview', version='V2.0'), 'InternVL3_5-30B-A3B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-30B-A3B', version='V2.0'), 'InternVL3_5-38B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-38B', version='V2.0'), 'InternVL3_5-241B-A28B': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-241B-A28B', version='V2.0'), 'InternVL3_5-1B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-1B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-2B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-2B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-4B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-4B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-8B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-8B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-14B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-14B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-GPT-OSS-20B-A4B-Preview-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-30B-A3B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-30B-A3B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-38B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-38B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'InternVL3_5-241B-A28B-Thinking': functools.partial(<class 'vlmeval.vlm.internvl.internvl_chat.InternVLChat'>, model_path='OpenGVLab/InternVL3_5-241B-A28B', use_lmdeploy=True, max_new_tokens=65536, cot_prompt_version='r1', do_sample=True, version='V2.0'), 'Yi_VL_6B': functools.partial(<class 'vlmeval.vlm.yi_vl.Yi_VL'>, model_path='01-ai/Yi-VL-6B', root=None), 'Yi_VL_34B': functools.partial(<class 'vlmeval.vlm.yi_vl.Yi_VL'>, model_path='01-ai/Yi-VL-34B', root=None), 'XComposer': functools.partial(<class 'vlmeval.vlm.xcomposer.xcomposer.XComposer'>, model_path='internlm/internlm-xcomposer-vl-7b'), 'sharecaptioner': functools.partial(<class 'vlmeval.vlm.xcomposer.sharecaptioner.ShareCaptioner'>, model_path='Lin-Chen/ShareCaptioner'), 'XComposer2': functools.partial(<class 'vlmeval.vlm.xcomposer.xcomposer2.XComposer2'>, model_path='internlm/internlm-xcomposer2-vl-7b'), 'XComposer2_1.8b': functools.partial(<class 'vlmeval.vlm.xcomposer.xcomposer2.XComposer2'>, model_path='internlm/internlm-xcomposer2-vl-1_8b'), 'XComposer2_4KHD': functools.partial(<class 'vlmeval.vlm.xcomposer.xcomposer2_4KHD.XComposer2_4KHD'>, model_path='internlm/internlm-xcomposer2-4khd-7b'), 'XComposer2d5': functools.partial(<class 'vlmeval.vlm.xcomposer.xcomposer2d5.XComposer2d5'>, model_path='internlm/internlm-xcomposer2d5-7b'), 'MiniGPT-4-v2': functools.partial(<class 'vlmeval.vlm.minigpt4.MiniGPT4'>, mode='v2', root=None), 'MiniGPT-4-v1-7B': functools.partial(<class 'vlmeval.vlm.minigpt4.MiniGPT4'>, mode='v1_7b', root=None), 'MiniGPT-4-v1-13B': functools.partial(<class 'vlmeval.vlm.minigpt4.MiniGPT4'>, mode='v1_13b', root=None), 'idefics_9b_instruct': functools.partial(<class 'vlmeval.vlm.idefics.IDEFICS'>, model_path='HuggingFaceM4/idefics-9b-instruct'), 'idefics_80b_instruct': functools.partial(<class 'vlmeval.vlm.idefics.IDEFICS'>, model_path='HuggingFaceM4/idefics-80b-instruct'), 'idefics2_8b': functools.partial(<class 'vlmeval.vlm.idefics.IDEFICS2'>, model_path='HuggingFaceM4/idefics2-8b'), 'Idefics3-8B-Llama3': functools.partial(<class 'vlmeval.vlm.idefics.IDEFICS2'>, model_path='HuggingFaceM4/Idefics3-8B-Llama3'), 'instructblip_7b': functools.partial(<class 'vlmeval.vlm.instructblip.InstructBLIP'>, name='instructblip_7b'), 'instructblip_13b': functools.partial(<class 'vlmeval.vlm.instructblip.InstructBLIP'>, name='instructblip_13b'), 'deepseek_vl_7b': functools.partial(<class 'vlmeval.vlm.deepseek_vl.DeepSeekVL'>, model_path='deepseek-ai/deepseek-vl-7b-chat'), 'deepseek_vl_1.3b': functools.partial(<class 'vlmeval.vlm.deepseek_vl.DeepSeekVL'>, model_path='deepseek-ai/deepseek-vl-1.3b-chat'), 'deepseek_vl2_tiny': functools.partial(<class 'vlmeval.vlm.deepseek_vl2.DeepSeekVL2'>, model_path='deepseek-ai/deepseek-vl2-tiny'), 'deepseek_vl2_small': functools.partial(<class 'vlmeval.vlm.deepseek_vl2.DeepSeekVL2'>, model_path='deepseek-ai/deepseek-vl2-small'), 'deepseek_vl2': functools.partial(<class 'vlmeval.vlm.deepseek_vl2.DeepSeekVL2'>, model_path='deepseek-ai/deepseek-vl2'), 'Janus-1.3B': functools.partial(<class 'vlmeval.vlm.janus.Janus'>, model_path='deepseek-ai/Janus-1.3B'), 'Janus-Pro-1B': functools.partial(<class 'vlmeval.vlm.janus.Janus'>, model_path='deepseek-ai/Janus-Pro-1B'), 'Janus-Pro-7B': functools.partial(<class 'vlmeval.vlm.janus.Janus'>, model_path='deepseek-ai/Janus-Pro-7B'), 'MiniCPM-V': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_V'>, model_path='openbmb/MiniCPM-V'), 'MiniCPM-V-2': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_V'>, model_path='openbmb/MiniCPM-V-2'), 'MiniCPM-Llama3-V-2_5': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_Llama3_V'>, model_path='openbmb/MiniCPM-Llama3-V-2_5'), 'MiniCPM-V-2_6': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_V_2_6'>, model_path='openbmb/MiniCPM-V-2_6'), 'MiniCPM-o-2_6': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_o_2_6'>, model_path='openbmb/MiniCPM-o-2_6'), 'MiniCPM-V-4': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_V_4'>, model_path='openbmb/MiniCPM-V-4'), 'MiniCPM-V-4_5': functools.partial(<class 'vlmeval.vlm.minicpm_v.MiniCPM_V_4_5'>, model_path='openbmb/MiniCPM-V-4_5'), 'cogvlm-grounding-generalist': functools.partial(<class 'vlmeval.vlm.cogvlm.CogVlm'>, model_path='THUDM/cogvlm-grounding-generalist-hf', tokenizer_name='lmsys/vicuna-7b-v1.5'), 'cogvlm-chat': functools.partial(<class 'vlmeval.vlm.cogvlm.CogVlm'>, model_path='THUDM/cogvlm-chat-hf', tokenizer_name='lmsys/vicuna-7b-v1.5'), 'cogvlm2-llama3-chat-19B': functools.partial(<class 'vlmeval.vlm.cogvlm.CogVlm'>, model_path='THUDM/cogvlm2-llama3-chat-19B'), 'glm-4v-9b': functools.partial(<class 'vlmeval.vlm.cogvlm.GLM4v'>, model_path='THUDM/glm-4v-9b'), 'GLM4_1VThinking-9b': functools.partial(<class 'vlmeval.vlm.cogvlm.GLMThinking'>, model_path='THUDM/GLM-4.1V-9B-Thinking'), 'GLM4_5V': functools.partial(<class 'vlmeval.vlm.cogvlm.GLMThinking'>, model_path='THUDM/GLM-4.5V'), 'WeMM': functools.partial(<class 'vlmeval.vlm.wemm.WeMM'>, model_path='feipengma/WeMM'), 'cambrian_8b': functools.partial(<class 'vlmeval.vlm.cambrian.Cambrian'>, model_path='nyu-visionx/cambrian-8b'), 'cambrian_13b': functools.partial(<class 'vlmeval.vlm.cambrian.Cambrian'>, model_path='nyu-visionx/cambrian-13b'), 'cambrian_34b': functools.partial(<class 'vlmeval.vlm.cambrian.Cambrian'>, model_path='nyu-visionx/cambrian-34b'), 'chameleon_7b': functools.partial(<class 'vlmeval.vlm.chameleon.Chameleon'>, model_path='facebook/chameleon-7b'), 'chameleon_30b': functools.partial(<class 'vlmeval.vlm.chameleon.Chameleon'>, model_path='facebook/chameleon-30b'), 'Video-LLaVA-7B': functools.partial(<class 'vlmeval.vlm.video_llm.video_llava.VideoLLaVA'>, model_path='LanguageBind/Video-LLaVA-7B'), 'Video-LLaVA-7B-HF': functools.partial(<class 'vlmeval.vlm.video_llm.video_llava.VideoLLaVA_HF'>, model_path='LanguageBind/Video-LLaVA-7B-hf'), 'VideoChat2-HD': functools.partial(<class 'vlmeval.vlm.video_llm.videochat2.VideoChat2_HD'>, model_path='OpenGVLab/VideoChat2_HD_stage4_Mistral_7B', root=None, config_file='./vlmeval/vlm/video_llm/configs/videochat2_hd.json'), 'Chat-UniVi-7B': functools.partial(<class 'vlmeval.vlm.video_llm.chat_uni_vi.Chatunivi'>, model_path='Chat-UniVi/Chat-UniVi'), 'Chat-UniVi-7B-v1.5': functools.partial(<class 'vlmeval.vlm.video_llm.chat_uni_vi.Chatunivi'>, model_path='Chat-UniVi/Chat-UniVi-7B-v1.5'), 'LLaMA-VID-7B': functools.partial(<class 'vlmeval.vlm.video_llm.llama_vid.LLaMAVID'>, model_path='YanweiLi/llama-vid-7b-full-224-video-fps-1'), 'Video-ChatGPT': functools.partial(<class 'vlmeval.vlm.video_llm.video_chatgpt.VideoChatGPT'>, model_path='MBZUAI/Video-ChatGPT-7B', dir_root=None), 'PLLaVA-7B': functools.partial(<class 'vlmeval.vlm.video_llm.pllava.PLLaVA'>, model_path='ermu2001/pllava-7b', dir_root=None), 'PLLaVA-13B': functools.partial(<class 'vlmeval.vlm.video_llm.pllava.PLLaVA'>, model_path='ermu2001/pllava-13b', dir_root=None), 'PLLaVA-34B': functools.partial(<class 'vlmeval.vlm.video_llm.pllava.PLLaVA'>, model_path='ermu2001/pllava-34b', dir_root=None), 'Ovis1.5-Llama3-8B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis'>, model_path='AIDC-AI/Ovis1.5-Llama3-8B'), 'Ovis1.5-Gemma2-9B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis'>, model_path='AIDC-AI/Ovis1.5-Gemma2-9B'), 'Ovis1.6-Gemma2-9B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis1_6'>, model_path='AIDC-AI/Ovis1.6-Gemma2-9B'), 'Ovis1.6-Llama3.2-3B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis1_6'>, model_path='AIDC-AI/Ovis1.6-Llama3.2-3B'), 'Ovis1.6-Gemma2-27B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis1_6_Plus'>, model_path='AIDC-AI/Ovis1.6-Gemma2-27B'), 'Ovis2-1B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2'>, model_path='AIDC-AI/Ovis2-1B'), 'Ovis2-2B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2'>, model_path='AIDC-AI/Ovis2-2B'), 'Ovis2-4B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2'>, model_path='AIDC-AI/Ovis2-4B'), 'Ovis2-8B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2'>, model_path='AIDC-AI/Ovis2-8B'), 'Ovis2-16B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2'>, model_path='AIDC-AI/Ovis2-16B'), 'Ovis2-34B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2'>, model_path='AIDC-AI/Ovis2-34B'), 'Ovis-U1-3B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.OvisU1'>, model_path='AIDC-AI/Ovis-U1-3B'), 'Ovis2.5-2B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2_5'>, model_path='AIDC-AI/Ovis2.5-2B'), 'Ovis2.5-9B': functools.partial(<class 'vlmeval.vlm.ovis.ovis.Ovis2_5'>, model_path='AIDC-AI/Ovis2.5-9B'), 'VILA1.5-3b': functools.partial(<class 'vlmeval.vlm.vila.VILA'>, model_path='Efficient-Large-Model/VILA1.5-3b'), 'Llama-3-VILA1.5-8b': functools.partial(<class 'vlmeval.vlm.vila.VILA'>, model_path='Efficient-Large-Model/Llama-3-VILA1.5-8b'), 'VILA1.5-13b': functools.partial(<class 'vlmeval.vlm.vila.VILA'>, model_path='Efficient-Large-Model/VILA1.5-13b'), 'VILA1.5-40b': functools.partial(<class 'vlmeval.vlm.vila.VILA'>, model_path='Efficient-Large-Model/VILA1.5-40b'), 'NVILA-8B': functools.partial(<class 'vlmeval.vlm.vila.NVILA'>, model_path='Efficient-Large-Model/NVILA-8B'), 'NVILA-15B': functools.partial(<class 'vlmeval.vlm.vila.NVILA'>, model_path='Efficient-Large-Model/NVILA-15B'), 'Mantis-8B-siglip-llama3': functools.partial(<class 'vlmeval.vlm.mantis.Mantis'>, model_path='TIGER-Lab/Mantis-8B-siglip-llama3'), 'Mantis-8B-clip-llama3': functools.partial(<class 'vlmeval.vlm.mantis.Mantis'>, model_path='TIGER-Lab/Mantis-8B-clip-llama3'), 'Mantis-8B-Idefics2': functools.partial(<class 'vlmeval.vlm.mantis.Mantis'>, model_path='TIGER-Lab/Mantis-8B-Idefics2'), 'Mantis-8B-Fuyu': functools.partial(<class 'vlmeval.vlm.mantis.Mantis'>, model_path='TIGER-Lab/Mantis-8B-Fuyu'), 'MMAlaya': functools.partial(<class 'vlmeval.vlm.mmalaya.MMAlaya'>, model_path='DataCanvas/MMAlaya'), 'MMAlaya2': functools.partial(<class 'vlmeval.vlm.mmalaya.MMAlaya2'>, model_path='DataCanvas/MMAlaya2'), 'Phi-3-Vision': functools.partial(<class 'vlmeval.vlm.phi3_vision.Phi3Vision'>, model_path='microsoft/Phi-3-vision-128k-instruct'), 'Phi-3.5-Vision': functools.partial(<class 'vlmeval.vlm.phi3_vision.Phi3_5Vision'>, model_path='microsoft/Phi-3.5-vision-instruct'), 'Phi-4-Vision': functools.partial(<class 'vlmeval.vlm.phi4_multimodal.Phi4Multimodal'>, model_path='microsoft/Phi-4-multimodal-instruct'), 'xgen-mm-phi3-interleave-r-v1.5': functools.partial(<class 'vlmeval.vlm.xgen_mm.XGenMM'>, model_path='Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5'), 'xgen-mm-phi3-dpo-r-v1.5': functools.partial(<class 'vlmeval.vlm.xgen_mm.XGenMM'>, model_path='Salesforce/xgen-mm-phi3-mini-instruct-dpo-r-v1.5'), 'Qwen-VL-Max-20250813': functools.partial(<class 'vlmeval.api.qwen_vl_api.Qwen2VLAPI'>, model='qwen-vl-max-2025-08-13', min_pixels=1003520, max_pixels=12845056, max_length=8192), 'Qwen-VL-Max-0809': functools.partial(<class 'vlmeval.api.qwen_vl_api.Qwen2VLAPI'>, model='qwen-vl-max-0809', min_pixels=1003520, max_pixels=12845056), 'Qwen-VL-Plus-0809': functools.partial(<class 'vlmeval.api.qwen_vl_api.Qwen2VLAPI'>, model='qwen-vl-plus-0809', min_pixels=1003520, max_pixels=12845056), 'QVQ-72B-Preview': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/QVQ-72B-Preview', min_pixels=1003520, max_pixels=12845056, system_prompt='You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.', max_new_tokens=8192, post_process=False), 'Qwen2-VL-72B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-72B-Instruct', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-7B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-7B-Instruct', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-7B-Instruct-AWQ': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-7B-Instruct-AWQ', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-7B-Instruct-GPTQ-Int4': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-7B-Instruct-GPTQ-Int8': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-2B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-2B-Instruct', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-2B-Instruct-AWQ': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-2B-Instruct-AWQ', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-2B-Instruct-GPTQ-Int4': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4', min_pixels=1003520, max_pixels=12845056), 'Qwen2-VL-2B-Instruct-GPTQ-Int8': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8', min_pixels=1003520, max_pixels=12845056), 'XinYuan-VL-2B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Cylingo/Xinyuan-VL-2B', min_pixels=1003520, max_pixels=12845056), 'Qwen2.5-VL-3B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-3B-Instruct', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'Qwen2.5-VL-3B-Instruct-AWQ': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-3B-Instruct-AWQ', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'Qwen2.5-VL-7B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-7B-Instruct', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'Qwen2.5-VL-7B-Instruct-ForVideo': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-7B-Instruct', min_pixels=100352, max_pixels=602112, total_pixels=19267584, use_custom_prompt=False), 'Qwen2.5-VL-7B-Instruct-AWQ': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-7B-Instruct-AWQ', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'Qwen2.5-VL-32B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-32B-Instruct', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'Qwen2.5-VL-72B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-72B-Instruct', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'MiMo-VL-7B-SFT': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='XiaomiMiMo/MiMo-VL-7B-SFT', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False, use_lmdeploy=True), 'MiMo-VL-7B-RL': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='XiaomiMiMo/MiMo-VL-7B-RL', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False, use_lmdeploy=True), 'Qwen2.5-VL-72B-Instruct-ForVideo': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-72B-Instruct', min_pixels=100352, max_pixels=602112, total_pixels=19267584, use_custom_prompt=False), 'Qwen2.5-VL-72B-Instruct-AWQ': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-VL-72B-Instruct-AWQ', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'Qwen2.5-Omni-7B-ForVideo': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-Omni-7B', min_pixels=100352, max_pixels=602112, total_pixels=19267584, use_custom_prompt=False, use_audio_in_video=True), 'Qwen2.5-Omni-7B': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChat'>, model_path='Qwen/Qwen2.5-Omni-7B', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'VLM-R1': functools.partial(<class 'vlmeval.vlm.vlm_r1.VLMR1Chat'>, model_path='omlab/VLM-R1-Qwen2.5VL-3B-Math-0305', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False), 'VLAA-Thinker-Qwen2.5VL-3B': functools.partial(<class 'vlmeval.vlm.vlaa_thinker.VLAAThinkerChat'>, model_path='UCSC-VLAA/VLAA-Thinker-Qwen2.5VL-3B', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False, post_process=True, system_prompt='You are VL-Thinking, a helpful assistant with excellent reasoning ability. A user asks you a question, and you should try to solve it. You should first think about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>` tags, respectively, i.e., `<think>` reasoning process here `</think><answer>` answer here `</answer>`'), 'VLAA-Thinker-Qwen2.5VL-7B': functools.partial(<class 'vlmeval.vlm.vlaa_thinker.VLAAThinkerChat'>, model_path='UCSC-VLAA/VLAA-Thinker-Qwen2.5VL-7B', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False, post_process=True, system_prompt='You are VL-Thinking, a helpful assistant with excellent reasoning ability. A user asks you a question, and you should try to solve it. You should first think about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>` tags, respectively, i.e., `<think>` reasoning process here `</think><answer>` answer here `</answer>`'), 'WeThink-Qwen2.5VL-7B': functools.partial(<class 'vlmeval.vlm.wethink_vl.WeThinkVL'>, model_path='yangjie-cv/WeThink-Qwen2.5VL-7B', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=False, system_prompt='You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\nThe reasoning process MUST BE enclosed within `<think>` `</think>` tags. The final answer MUST BE enclosed within `<answer>` `</answer>` tags.'), 'Qwen3-VL-235B-A22B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-235B-A22B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.7, max_new_tokens=16384, repetition_penalty=1.0, presence_penalty=1.5, top_p=0.8, top_k=20), 'Qwen3-VL-235B-A22B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-235B-A22B-Thinking', use_custom_prompt=False, use_vllm=True, temperature=1.0, max_new_tokens=40960, repetition_penalty=1.0, presence_penalty=0.0, top_p=0.95, top_k=20), 'Qwen3-VL-30B-A3B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-30B-A3B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.7, max_new_tokens=16384, repetition_penalty=1.0, presence_penalty=1.5, top_p=0.8, top_k=20), 'Qwen3-VL-30B-A3B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-30B-A3B-Thinking', use_custom_prompt=False, use_vllm=True, temperature=1.0, max_new_tokens=40960, repetition_penalty=1.0, presence_penalty=0.0, top_p=0.95, top_k=20), 'Qwen3-VL-8B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-8B-Thinking', use_custom_prompt=False, use_vllm=True, temperature=1.0, max_new_tokens=40960, repetition_penalty=1.0, presence_penalty=0.0, top_p=0.95, top_k=20), 'Qwen3-VL-4B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-4B-Thinking', use_custom_prompt=False, use_vllm=True, temperature=1.0, max_new_tokens=40960, repetition_penalty=1.0, presence_penalty=0.0, top_p=0.95, top_k=20), 'Qwen3-VL-8B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-8B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.7, max_new_tokens=16384, repetition_penalty=1.0, presence_penalty=1.5, top_p=0.8, top_k=20), 'Qwen3-VL-4B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-4B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.7, max_new_tokens=16384, repetition_penalty=1.0, presence_penalty=1.5, top_p=0.8, top_k=20), 'Qwen3-VL-2B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-2B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.7, max_new_tokens=16384, repetition_penalty=1.0, presence_penalty=1.5, top_p=0.8, top_k=20), 'Qwen3-VL-32B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-32B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.7, max_new_tokens=16384, repetition_penalty=1.0, presence_penalty=1.5, top_p=0.8, top_k=20), 'Qwen3-VL-2B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-2B-Thinking', use_custom_prompt=False, use_vllm=True, temperature=1.0, max_new_tokens=40960, repetition_penalty=1.0, presence_penalty=0.0, top_p=0.95, top_k=20), 'Qwen3-VL-32B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-VL-32B-Thinking', use_custom_prompt=False, use_vllm=False, temperature=1.0, max_new_tokens=40960, repetition_penalty=1.0, presence_penalty=0.0, top_p=0.95, top_k=20), 'Qwen3-Omni-30B-A3B-Instruct': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-Omni-30B-A3B-Instruct', use_custom_prompt=False, use_vllm=True, temperature=0.6, top_p=0.95, top_k=20, max_new_tokens=16384), 'Qwen3-Omni-30B-A3B-Thinking': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-Omni-30B-A3B-Thinking', use_custom_prompt=False, use_vllm=True, temperature=0.6, top_p=0.95, top_k=20, max_new_tokens=16384), 'Qwen3-Omni-30B-A3B-Captioner': functools.partial(<class 'vlmeval.vlm.qwen3_vl.model.Qwen3VLChat'>, model_path='Qwen/Qwen3-Omni-30B-A3B-Captioner', use_custom_prompt=False, use_vllm=True, temperature=0.6, top_p=0.95, top_k=20, max_new_tokens=16384), 'Slime-7B': functools.partial(<class 'vlmeval.vlm.slime.SliME'>, model_path='yifanzhang114/SliME-vicuna-7B'), 'Slime-8B': functools.partial(<class 'vlmeval.vlm.slime.SliME'>, model_path='yifanzhang114/SliME-Llama3-8B'), 'Slime-13B': functools.partial(<class 'vlmeval.vlm.slime.SliME'>, model_path='yifanzhang114/SliME-vicuna-13B'), 'Eagle-X4-8B-Plus': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X4-8B-Plus'), 'Eagle-X4-13B-Plus': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X4-13B-Plus'), 'Eagle-X5-7B': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X5-7B'), 'Eagle-X5-13B': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X5-13B'), 'Eagle-X5-13B-Chat': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X5-13B-Chat'), 'Eagle-X5-34B-Chat': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X5-34B-Chat'), 'Eagle-X5-34B-Plus': functools.partial(<class 'vlmeval.vlm.eagle_x.Eagle'>, model_path='NVEagle/Eagle-X5-34B-Plus'), 'Moondream1': functools.partial(<class 'vlmeval.vlm.moondream.Moondream1'>, model_path='vikhyatk/moondream1'), 'Moondream2': functools.partial(<class 'vlmeval.vlm.moondream.Moondream2'>, model_path='vikhyatk/moondream2'), 'Llama-3.2-11B-Vision-Instruct': functools.partial(<class 'vlmeval.vlm.llama_vision.llama_vision'>, model_path='meta-llama/Llama-3.2-11B-Vision-Instruct'), 'LLaVA-CoT': functools.partial(<class 'vlmeval.vlm.llama_vision.llama_vision'>, model_path='Xkev/Llama-3.2V-11B-cot'), 'Llama-3.2-90B-Vision-Instruct': functools.partial(<class 'vlmeval.vlm.llama_vision.llama_vision'>, model_path='meta-llama/Llama-3.2-90B-Vision-Instruct'), 'Llama-4-Scout-17B-16E-Instruct': functools.partial(<class 'vlmeval.vlm.llama4.llama4'>, model_path='meta-llama/Llama-4-Scout-17B-16E-Instruct', use_vllm=True), 'molmoE-1B-0924': functools.partial(<class 'vlmeval.vlm.molmo.molmo'>, model_path='allenai/MolmoE-1B-0924'), 'molmo-7B-D-0924': functools.partial(<class 'vlmeval.vlm.molmo.molmo'>, model_path='allenai/Molmo-7B-D-0924'), 'molmo-7B-O-0924': functools.partial(<class 'vlmeval.vlm.molmo.molmo'>, model_path='allenai/Molmo-7B-O-0924'), 'molmo-72B-0924': functools.partial(<class 'vlmeval.vlm.molmo.molmo'>, model_path='allenai/Molmo-72B-0924'), 'Kosmos2': functools.partial(<class 'vlmeval.vlm.kosmos.Kosmos2'>, model_path='microsoft/kosmos-2-patch14-224'), 'POINTS-Yi-1.5-9B-Chat': functools.partial(<class 'vlmeval.vlm.points.POINTS'>, model_path='WePOINTS/POINTS-Yi-1-5-9B-Chat'), 'POINTS-Qwen-2.5-7B-Chat': functools.partial(<class 'vlmeval.vlm.points.POINTS'>, model_path='WePOINTS/POINTS-Qwen-2-5-7B-Chat'), 'POINTSV15-Qwen-2.5-7B-Chat': functools.partial(<class 'vlmeval.vlm.points.POINTSV15'>, model_path='WePOINTS/POINTS-1-5-Qwen-2-5-7B-Chat'), 'NVLM': functools.partial(<class 'vlmeval.vlm.nvlm.NVLM'>, model_path='nvidia/NVLM-D-72B'), 'Vintern-3B-beta': functools.partial(<class 'vlmeval.vlm.vintern_chat.VinternChat'>, model_path='5CD-AI/Vintern-3B-beta'), 'Vintern-1B-v2': functools.partial(<class 'vlmeval.vlm.vintern_chat.VinternChat'>, model_path='5CD-AI/Vintern-1B-v2'), 'h2ovl-mississippi-2b': functools.partial(<class 'vlmeval.vlm.h2ovl_mississippi.H2OVLChat'>, model_path='h2oai/h2ovl-mississippi-2b'), 'h2ovl-mississippi-1b': functools.partial(<class 'vlmeval.vlm.h2ovl_mississippi.H2OVLChat'>, model_path='h2oai/h2ovl-mississippi-800m'), 'Aria': functools.partial(<class 'vlmeval.vlm.aria.Aria'>, model_path='rhymes-ai/Aria'), 'SmolVLM-256M': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM'>, model_path='HuggingFaceTB/SmolVLM-256M-Instruct'), 'SmolVLM-500M': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM'>, model_path='HuggingFaceTB/SmolVLM-500M-Instruct'), 'SmolVLM': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM'>, model_path='HuggingFaceTB/SmolVLM-Instruct'), 'SmolVLM-DPO': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM'>, model_path='HuggingFaceTB/SmolVLM-Instruct-DPO'), 'SmolVLM-Synthetic': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM'>, model_path='HuggingFaceTB/SmolVLM-Synthetic'), 'SmolVLM2-256M': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM2'>, model_path='HuggingFaceTB/SmolVLM2-256M-Video-Instruct'), 'SmolVLM2-500M': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM2'>, model_path='HuggingFaceTB/SmolVLM2-500M-Video-Instruct'), 'SmolVLM2': functools.partial(<class 'vlmeval.vlm.smolvlm.SmolVLM2'>, model_path='HuggingFaceTB/SmolVLM2-2.2B-Instruct'), 'SAIL-VL-2B': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL-2B'), 'SAIL-VL-1.5-2B': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL-1d5-2B', use_msac=True), 'SAIL-VL-1.5-8B': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL-1d5-8B', use_msac=True), 'SAIL-VL-1.6-8B': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL-1d6-8B', use_msac=True), 'SAIL-VL-1.7-Thinking-2B-2507': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL-1d7-Thinking-2B-2507', use_msac=True, use_cot=True, max_new_tokens=4096), 'SAIL-VL-1.7-Thinking-8B-2507': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL-1d7-Thinking-8B-2507', use_msac=True, use_cot=True, max_new_tokens=4096), 'SAIL-VL2-2B': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL2-2B', use_msac=True), 'SAIL-VL2-8B': functools.partial(<class 'vlmeval.vlm.sail_vl.SailVL'>, model_path='BytedanceDouyinContent/SAIL-VL2-8B', use_msac=True), 'valley2': functools.partial(<class 'vlmeval.vlm.valley.valley2.Valley2Chat'>, model_path='bytedance-research/Valley-Eagle-7B'), 'valley2_dpo': functools.partial(<class 'vlmeval.vlm.valley.valley2.Valley2Chat'>, model_path='bytedance-research/Valley2-DPO'), 'valley3': functools.partial(<class 'vlmeval.vlm.valley.valley3.Valley3Chat'>, use_gthinker_thinking=True, model_path='bytedance-research/Valley3'), 'vita': functools.partial(<class 'vlmeval.vlm.vita.VITA'>, model_path='VITA-MLLM/VITA', root=None), 'vita_qwen2': functools.partial(<class 'vlmeval.vlm.vita.VITAQwen2'>, model_path='VITA-MLLM/VITA-1.5', root=None), 'ross-qwen2-7b': functools.partial(<class 'vlmeval.vlm.ross.Ross'>, model_path='HaochenWang/ross-qwen2-7b'), 'emu2_chat': functools.partial(<class 'vlmeval.vlm.emu.Emu'>, model_path='BAAI/Emu2-Chat'), 'emu3_chat': functools.partial(<class 'vlmeval.vlm.emu.Emu3_chat'>, model_path='BAAI/Emu3-Chat'), 'emu3_gen': functools.partial(<class 'vlmeval.vlm.emu.Emu3_gen'>, model_path='BAAI/Emu3-Gen'), 'ola': functools.partial(<class 'vlmeval.vlm.ola.ola_model.Ola'>, model_path='THUdyh/Ola-7b'), 'URSA-8B': functools.partial(<class 'vlmeval.vlm.ursa.ursa_chat.UrsaChat'>, model_path='URSA-MATH/URSA-8B'), 'URSA-8B-PS-GRPO': functools.partial(<class 'vlmeval.vlm.ursa.ursa_chat.UrsaChat'>, model_path='URSA-MATH/URSA-8B-PS-GRPO'), 'paligemma-3b-mix-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma-3b-mix-448'), 'paligemma2-3b-pt-224': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-3b-pt-224'), 'paligemma2-3b-pt-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-3b-pt-448'), 'paligemma2-3b-mix-224': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-3b-mix-224'), 'paligemma2-3b-mix-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-3b-mix-448'), 'paligemma2-10b-pt-224': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-10b-pt-224'), 'paligemma2-10b-pt-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-10b-pt-448'), 'paligemma2-10b-mix-224': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-10b-mix-224'), 'paligemma2-10b-mix-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-10b-mix-448'), 'paligemma2-28b-pt-224': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-28b-pt-224'), 'paligemma2-28b-pt-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-28b-pt-448'), 'paligemma2-28b-mix-224': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-28b-mix-224'), 'paligemma2-28b-mix-448': functools.partial(<class 'vlmeval.vlm.gemma.PaliGemma'>, model_path='google/paligemma2-28b-mix-448'), 'Gemma3-4B': functools.partial(<class 'vlmeval.vlm.gemma.Gemma3'>, model_path='google/gemma-3-4b-it'), 'Gemma3-12B': functools.partial(<class 'vlmeval.vlm.gemma.Gemma3'>, model_path='google/gemma-3-12b-it'), 'Gemma3-27B': functools.partial(<class 'vlmeval.vlm.gemma.Gemma3'>, model_path='google/gemma-3-27b-it'), 'Long-VITA-16K': functools.partial(<class 'vlmeval.vlm.long_vita.LongVITA'>, model_path='VITA-MLLM/Long-VITA-16K_HF', max_num_frame=128), 'Long-VITA-128K': functools.partial(<class 'vlmeval.vlm.long_vita.LongVITA'>, model_path='VITA-MLLM/Long-VITA-128K_HF', max_num_frame=256), 'Long-VITA-1M': functools.partial(<class 'vlmeval.vlm.long_vita.LongVITA'>, model_path='VITA-MLLM/Long-VITA-1M_HF', max_num_frame=256), 'Ristretto-3B': functools.partial(<class 'vlmeval.vlm.ristretto.Ristretto'>, model_path='LiAutoAD/Ristretto-3B'), 'Kimi-VL-A3B-Thinking': functools.partial(<class 'vlmeval.vlm.kimi_vl.KimiVL'>, model_path='moonshotai/Kimi-VL-A3B-Thinking'), 'Kimi-VL-A3B-Instruct': functools.partial(<class 'vlmeval.vlm.kimi_vl.KimiVL'>, model_path='moonshotai/Kimi-VL-A3B-Instruct'), 'Kimi-VL-A3B-Thinking-2506': functools.partial(<class 'vlmeval.vlm.kimi_vl.KimiVL'>, model_path='moonshotai/Kimi-VL-A3B-Thinking-2506', temperature=0.8, max_tokens=32768, extract_summary=True), 'aguvis_7b': functools.partial(<class 'vlmeval.vlm.qwen2_vl.model.Qwen2VLChatAguvis'>, model_path='xlangai/Aguvis-7B-720P', min_pixels=200704, max_pixels=937664, use_custom_prompt=False, mode='grounding'), 'HawkVL-2B': functools.partial(<class 'vlmeval.vlm.hawk_vl.model.HawkVL'>, model_path='xjtupanda/HawkVL-2B', min_pixels=3136, max_pixels=5331200, use_custom_prompt=True), 'Flash-VL-2B-Dynamic-ISS': functools.partial(<class 'vlmeval.vlm.flash_vl.FlashVL'>, model_path='FlashVL/FlashVL-2B-Dynamic-ISS'), 'api-kimi-vl-thinking-2506': functools.partial(<class 'vlmeval.api.kimivl_api.KimiVLAPI'>, model='api-kimi-vl-thinking-2506'), 'api-kimi-vl-thinking': functools.partial(<class 'vlmeval.api.kimivl_api.KimiVLAPI'>, model='api-kimi-vl-thinking'), 'api-kimi-vl': functools.partial(<class 'vlmeval.api.kimivl_api.KimiVLAPI'>, model='api-kimi-vl', max_new_tokens=2048, temperature=0), 'oryx': functools.partial(<class 'vlmeval.vlm.oryx.Oryx'>, model_path='THUdyh/Oryx-1.5-7B'), 'TreeVGR-7B': functools.partial(<class 'vlmeval.vlm.treevgr.TreeVGR'>, model_path='HaochenWang/TreeVGR-7B', min_pixels=1003520, max_pixels=12845056), 'varco-vision-hf': functools.partial(<class 'vlmeval.vlm.llava.llava.LLaVA_OneVision_HF'>, model_path='NCSOFT/VARCO-VISION-14B-HF'), 'varco-vision-2-1.7b': functools.partial(<class 'vlmeval.vlm.varco_vision.VarcoVision'>, model_path='NCSOFT/VARCO-VISION-2.0-1.7B'), 'varco-vision-2-14b': functools.partial(<class 'vlmeval.vlm.varco_vision.VarcoVision'>, model_path='NCSOFT/VARCO-VISION-2.0-14B'), 'QTuneVL1_5-2B': functools.partial(<class 'vlmeval.vlm.qtunevl.qtune_vl_chat.QTuneVLChat'>, model_path='hanchaow/QTuneVL1_5-2B', version='V1.5'), 'QTuneVL1_5-3B': functools.partial(<class 'vlmeval.vlm.qtunevl.qtune_vl.QTuneVL'>, model_path='hanchaow/QTuneVL1_5-3B', min_pixels=1003520, max_pixels=12845056, use_custom_prompt=True, post_process=True), 'X-VL-4B': functools.partial(<class 'vlmeval.vlm.x_vl.X_VL_HF'>, model_path='YannQi/X-VL-4B', temperature=0, retry=10), 'Thyme-7B': functools.partial(<class 'vlmeval.vlm.thyme.model.Thyme'>, model_path='Kwai-Keye/Thyme-RL'), 'Logics-Thinking-8B': functools.partial(<class 'vlmeval.vlm.logics.Logics_Thinking'>, model_path='Logics-MLLM/Logics-Thinking-8B'), 'Logics-Thinking-32B': functools.partial(<class 'vlmeval.vlm.logics.Logics_Thinking'>, model_path='Logics-MLLM/Logics-Thinking-32B'), 'Cosmos-Reason1-7B': functools.partial(<class 'vlmeval.vlm.cosmos.Cosmos'>, model_path='nvidia/Cosmos-Reason1-7B', use_vllm=True), 'Keye-VL-1.5-8B-auto': functools.partial(<class 'vlmeval.vlm.keye_vlm.model.KeyeChat'>, model_path='Kwai-Keye/Keye-VL-1_5-8B'), 'Keye-VL-1.5-8B-think': functools.partial(<class 'vlmeval.vlm.keye_vlm.model.KeyeChat'>, model_path='Kwai-Keye/Keye-VL-1_5-8B', think=True), 'Keye-VL-1.5-8B-nothink': functools.partial(<class 'vlmeval.vlm.keye_vlm.model.KeyeChat'>, model_path='Kwai-Keye/Keye-VL-1_5-8B', no_think=True), 'Keye-VL-8B-Preview-think': functools.partial(<class 'vlmeval.vlm.keye_vlm.model.KeyeChat'>, model_path='Kwai-Keye/Keye-VL-8B-Preview', think=True), 'Qianfan-VL-3B': functools.partial(<class 'vlmeval.vlm.qianfan_vl.Qianfan_VL'>, model_path='baidu/Qianfan-VL-3B'), 'Qianfan-VL-8B': functools.partial(<class 'vlmeval.vlm.qianfan_vl.Qianfan_VL'>, model_path='baidu/Qianfan-VL-8B'), 'Qianfan-VL-70B': functools.partial(<class 'vlmeval.vlm.qianfan_vl.Qianfan_VL'>, model_path='baidu/Qianfan-VL-70B'), 'LFM2-VL-450M': functools.partial(<class 'vlmeval.vlm.liquid.LFM2VL'>, model_path='LiquidAI/LFM2-VL-450M'), 'LFM2-VL-1.6B': functools.partial(<class 'vlmeval.vlm.liquid.LFM2VL'>, model_path='LiquidAI/LFM2-VL-1.6B'), 'LFM2-VL-3B': functools.partial(<class 'vlmeval.vlm.liquid.LFM2VL'>, model_path='LiquidAI/LFM2-VL-3B'), 'rbdashmm3_DPO_38B_api': functools.partial(<class 'vlmeval.api.rbdashmm_chat3_api.RBdashMMChat3_API'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=3, timeout=600), 'rbdashmm3_5_DPO_38B_api': functools.partial(<class 'vlmeval.api.rbdashmm_chat3_api.RBdashChat3_5_API'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=3, timeout=600), 'rbdashmm3_5_38B_api': functools.partial(<class 'vlmeval.api.rbdashmm_chat3_5_api.RBdashMMChat3_5_38B_API'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=3, timeout=600), 'rbdashmm3_78B_api': functools.partial(<class 'vlmeval.api.rbdashmm_chat3_5_api.RBdashMMChat3_78B_API'>, api_base='[http://0.0.0.0:23333/v1/chat/completions](http://0.0.0.0:23333/v1/chat/completions)', temperature=0, retry=3, timeout=600), 'Intern-S1-mini': functools.partial(<class 'vlmeval.vlm.interns1.interns1_chat.InternS1Chat'>, model_path='/mnt/shared-storage-user/mllm/lijinsong/models/Intern-S1-mini/'), 'insightv': functools.partial(<class 'vlmeval.vlm.insight_v.InsightV'>, pretrained_reason='THUdyh/Insight-V-Reason-LLaMA3', pretrained_summary='THUdyh/Insight-V-Summary-LLaMA3')}
