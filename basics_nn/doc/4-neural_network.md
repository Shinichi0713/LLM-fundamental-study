ューラルネットワーク（NN）をグラフ理論の視点から理解するための、最も効果的な実装例題は、NNの**順伝播（Forward Propagation）を有向非巡回グラフ（DAG）の探索**としてコーディングすることです。

ここでは、NNの基本形である「多層パーセプトロン（MLP）」を例に、グラフ理論とNNを結びつける実装例を提案します。

## DAGとは？

のっけからDAG?となると思います。

DAGは、一般的に「有向非巡回グラフ」（Directed Acyclic Graph）の略称です。
グラフ理論における特定のデータ構造を指し、計算機科学やデータ処理の分野でよく使用されています。

### もう少し詳しく

DAGは、以下の2つの主要な特性を持つグラフです。

- 有向 (Directed): グラフ内のノード（頂点）間の接続（エッジ）には方向があります。データや処理は、あるノードから別のノードへ一方向に流れます。
- 非巡回 (Acyclic): グラフ内にサイクル（循環）が存在しません。つまり、あるノードから出発して、その経路をたどって再び元のノードに戻ってくることは不可能です。

__構造のイメージ__

- ノード (Nodes/Vertices): 処理のステップ、データセット、またはタスクの単位を表します。
- エッジ (Edges): ノード間の依存関係、つまり「どの処理の後にどの処理を行うべきか」という流れを表します。

## 例題のテーマ：DAGとしての順伝播実装

### グラフ理論の概念とNNの対応

| **グラフ理論の用語**                | **NNの要素**                               |
| ----------------------------------------- | ------------------------------------------------ |
| **頂点（ノード**$V$**）**   | **ニューロン**(入力値、活性化値、バイアス) |
| **有向辺（エッジ**$E$**）** | **接続** （信号の方向）                    |
| **重み付きグラフ**                  | **重み**$W$****(エッジの属性)            |
| **トポロジカルソート**              | **順伝播の実行順序** （層ごとの計算順序）  |

### 例題の概要

2つの入力層、1つの隠れ層（2つのニューロン）、1つの出力層を持つ**非常にシンプルなニューラルネットワーク**を構築し、その順伝播を「グラフ」として実装します。

#### 1. グラフ構造の定義（データ構造）

NNの構造と重みを、グラフのエッジリストとノード属性として定義します。

**【グラフ理論の視点】**

* **ノードリスト (V):** `[Input_1, Input_2, Hidden_1, Hidden_2, Output_1]`
* **エッジリスト (E):** 重みを属性として持つ有向辺の集合。

| **始点ノード** | **終点ノード** | **重み (W)**   |
| -------------------- | -------------------- | -------------------- |
| `Input_1`          | `Hidden_1`         | **$W_{11}$** |
| `Input_1`          | `Hidden_2`         | **$W_{12}$** |
| `Input_2`          | `Hidden_1`         | **$W_{21}$** |
| `Input_2`          | `Hidden_2`         | **$W_{22}$** |
| `Hidden_1`         | `Output_1`         | **$W_{31}$** |
| `Hidden_2`         | `Output_1`         | **$W_{41}$** |

**【実装イメージ（Python/辞書）】**

**Python**

```
# グラフ（隣接リスト形式）と重みの定義
graph_weights = {
    'Input_1': [('Hidden_1', W11), ('Hidden_2', W12)],
    'Input_2': [('Hidden_1', W21), ('Hidden_2', W22)],
    'Hidden_1': [('Output_1', W31)],
    'Hidden_2': [('Output_1', W41)],
    'Output_1': [] # 終点
}
```

---

#### 2. 順伝播（Forward Propagation）の実装

順伝播は、グラフ理論における**トポロジカルソート**された順序でノード（ニューロン）を処理するプロセスです。

**【グラフ理論の視点】**

1. **ノード処理順序の決定:** 入力層 → 隠れ層 → 出力層 の順に処理（DAGの性質を利用したトポロジカルソート）。
2. **ノードの計算:** 各ノードは、自分につながる**入力エッジ（前の層の出力 **$\times$** 重み）の総和**を計算します（これが順伝播の基本演算）。

**【実装イメージ（順伝播関数）】**

**Python**

```
def forward_pass(input_data, graph_weights):
    # ノードごとの活性化値を格納する辞書
    activations = {}
 
    # 1. 入力層の値をセット
    activations['Input_1'] = input_data[0]
    activations['Input_2'] = input_data[1]
 
    # 2. 処理順序（トポロジカルソート）に従って計算を実行
    # トポロジカルソート: 計算の依存関係に基づいたノードの処理順序
    sorted_nodes = ['Hidden_1', 'Hidden_2', 'Output_1']

    for node in sorted_nodes:
        input_sum = 0
  
        # 3. ノードへの入力の計算（入力エッジの総和）
        for source_node, connections in graph_weights.items():
            for target_node, weight in connections:
                if target_node == node:
                    # 前のノードの活性化値 × エッジの重み
                    input_sum += activations[source_node] * weight

        # 4. 活性化関数（例：ReLU）を適用
        # (ここでは簡略化のため、活性化関数とバイアスを省略し、積和をそのまま出力とする)
        activations[node] = input_sum
 
    return activations['Output_1']
```

上記コードを使って任意の入力で計算させてみると以下のように動作します。

### この例題から得られること

この実装を通じて、以下の重要な概念がグラフ理論とNNの接点であることが理解できます。

1. NNの構造はなぜDAGでなければならないのか？
   順伝播では、情報が入力から出力へ一方向に流れ、前の層の計算結果が後の層で使われます。もしグラフが巡回（サイクル）を持っていた場合、計算の順序を一意に決定できず、トポロジカルソートが不可能となり、順伝播を停止させることができません。この実装は、NNの計算可能性がDAGというグラフの特性に依存していることを示します。
2. 逆伝播の仕組み（発展）
   逆伝播は、出力層から入力層へ向けてエッジを逆向きに辿りながら、各ノードの勾配（誤差への影響度）を計算し、その結果を使ってエッジの重みを更新する操作です。これは、グラフ理論における逆方向探索であり、NNの学習の本質がグラフ上の最適化問題であることを示しています。

---

## 🚀 発展例題：グラフニューラルネットワーク（GNN）

より進んだ接続を理解するために、**グラフニューラルネットワーク (GNN)** の概念を考えると、グラフ理論とNNの関係がさらに深まります。

| **グラフ理論の概念** | **GNNでの意味合い**                                                                                                                                                                                |
| -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **一般グラフ**       | ネットワークの入力データそのもの。ノードやエッジが不規則に接続されたデータ（分子構造、SNSの友人関係など）。                                                                                              |
| **近傍ノード**       | そのノードに直接つながっているノード。                                                                                                                                                                   |
| **GNNの計算**        | **メッセージパッシング** 。各ノードが近傍ノードからの情報（メッセージ）を集約（Aggregate）し、自身の状態を更新（Update）する。これは**グラフ探索**と**特徴抽出**を同時に行う処理です。 |

GNNを実装することで、グラフ構造を「NNの構造」としてではなく、「NNが処理する **入力データそのもの** 」として扱うことが理解できます。

![Graph Neural Network architectureの画像](https://encrypted-tbn1.gstatic.com/licensed-image?q=tbn:ANd9GcSH8Fivg-l4K2hEptl-lq8pQ-UU_HVNJyzhBRjOvnDzYEc5DZ2rRShmapWifP6ZwzvcxIvBjNmjC7crgPElLfsGSRNv63azkntEGYOTitv6GJScjgs)**Shutterstock**

**詳しく見る**
