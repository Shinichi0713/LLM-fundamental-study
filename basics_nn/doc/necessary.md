LLM（大規模言語モデル）を理解する上で、核となるニューラルネットワークの知識は主に以下の3つの構成要素に集約されます。

## 🧠 基礎となるニューラルネットワークの知識

LLMは、基本的に**フィードフォワードネットワーク**と**リカレントネットワーク**の概念を発展させたものです。

### 1. 全結合層 (Fully Connected Layer / Linear Layer)

* 役割: ニューラルネットワークの最も基本的な構成要素です。入力されたベクトルに対して、線形変換（重み行列 $W$ とバイアス $b$ の適用）を行い、出力を計算します。
  $$
  y = Wx + b
  $$
* **LLMでの役割** : トークン埋め込み、Attention機構におけるQ/K/Vの射影、およびTransformerブロック内の**フィードフォワードネットワーク（FFN）**の主要な構成要素として機能します。

---

### 2. 活性化関数 (Activation Function)

* **役割** : ニューラルネットワークに**非線形性**を導入し、複雑なパターンを学習できるようにします。非線形性がなければ、何層重ねても単なる線形モデルと同じになってしまいます。
* **LLMでよく使われる関数** :
* **ReLU (Rectified Linear Unit)** : **$\max(0, x)$**。計算が速いですが、負の値で勾配がゼロになる問題があります。
* **GELU (Gaussian Error Linear Unit)** : ReLUを平滑化したもので、TransformerやBERT以降のモデルで標準的に使われています。

---

### 3. 最適化と正規化の技術

* **勾配降下法とAdam** : モデルの重みを更新し、損失を最小化するための基本的な最適化アルゴリズムです。LLMの学習では、効率の良い**AdamW** (Adam with Weight Decay) が広く使われます。
* **正規化 (Normalization)** : 安定した学習のために必須です。
* **バッチ正規化 (Batch Normalization)** : 訓練バッチ全体で統計量を計算します（Transformer以前のモデルで主流）。
* **層正規化 (Layer Normalization)** : LLMが採用する主要な正規化手法です。バッチではなく、**個々の入力シーケンス内**で統計量（平均と分散）を計算し正規化します。これにより、シーケンス長の変動があっても安定して動作します。

## 💡 LLMの核心技術につながる発展知識

上記の基礎の上に、LLMの成功の鍵である2つの概念が成り立っています。

### 1. 埋め込み (Embedding)

* **役割** : LLMは生のテキストではなく、数値ベクトル（トークン埋め込み）を処理します。埋め込み層は、離散的なトークンIDを、意味的な情報を含む密な連続ベクトル空間にマッピングします。
* **特徴** : 意味が近い単語（例: "犬"と"子犬"）は、この埋め込み空間内で近い位置に配置されます。

### 2. 自己回帰 (Autoregression)

* **役割** : デコーダ型LLM（GPTなど）が文章を生成する基本的なメカニズムです。
* **概念** :  **過去に生成された単語** （コンテキスト）を入力として、**次の単語を一つずつ**順番に予測・生成していきます。この一方向の生成プロセスが、LLMによる自然な文章生成を可能にしています。

これらの基礎知識を理解することで、なぜTransformerが従来のニューラルネットワークよりも強力で、どのように機能しているかを深く理解できます。
