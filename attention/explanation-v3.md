## Attentionの機能

Attention（特に Transformer で使われる  **Scaled Dot-Product Attention** ）では、

入力データを3つの異なる空間に写像した  **Q (Query)** ,  **K (Key)** , **V (Value)** が中心的な役割を果たします。

それぞれの意味と数式を丁寧に説明します。

---

## 🔷 1. Attention の目的

入力系列中の「どの単語（位置）」が「他の単語にどれくらい関係しているか」を数値的に求める仕組み。

その「関連度」を重みとして使い、重要な情報を強調して出力を作ります。

---

## 🔷 2. 数式による定義

入力ベクトル列を

$$
X = [x_1, x_2, \dots, x_n] \in \mathbb{R}^{n \times d_{\text{model}}}
$$

とします。

これに対して、3つの重み行列 ( W_Q, W_K, W_V ) を使って以下のように変換します：

$$
Q = X W_Q,\quad K = X W_K,\quad V = X W_V
$$

ここで：

$$
* ( W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} )
* ( Q, K, V \in \mathbb{R}^{n \times d_k} )
$$

## 🔷 3. 各ベクトルの役割

| 名称                | 意味           | 数式的役割                             | 直感的な理解                                             |
| ------------------- | -------------- | -------------------------------------- | -------------------------------------------------------- |
| **Q (Query)** | 「問い合わせ」 | どの情報を探しているかを表す           | いま注目している単語が、他のどの単語の情報を欲しているか |
| **K (Key)**   | 「検索キー」   | 各単語がどんな内容を持っているかを表す | 辞書の見出し語のようなもの                               |
| **V (Value)** | 「値」         | 実際に参照・集約する情報本体           | 実際の意味内容・埋め込み情報                             |

---

## 🔷 4. Attention の計算手順

### (1) 類似度の計算

各 Query と Key の内積を取ることで、関連度を求めます：

$$
\text{score}(Q,K) = Q K^\top
$$

→ これにより「どの単語がどの単語にどれだけ注目するか」のスコア行列が得られます。

（形状： (n \times n)）

---

### (2) スケーリングとソフトマックス正規化

スコアを次のように正規化して確率分布に変換します：

$$
\text{Attention Weights} = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)
$$

$\sqrt{d_k}$ で割るのは、内積の値が大きくなりすぎて勾配が不安定になるのを防ぐため（安定化目的）。

---

### (3) Value との加重平均

得られた重みを Value にかけて、加重平均を取ります：

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$
---

## 🔷 5. 直感的イメージ

* Q（今注目している単語）

  → 「私は **何を知りたいか？** 」
* K（他のすべての単語）

  → 「私は **どんな情報を持っているか？** 」
* V（他の単語の実際の意味）

  → 「私の**中身（意味）**はこれです」

すると、Attention の出力は：

> 「すべての単語の意味（V）」を、「どれくらい関連があるか（QK）」の重みで平均したもの。

---

## 🔷 6. まとめ

| 要素          | 記号                                                       | 役割                       | 数式 |
| ------------- | ---------------------------------------------------------- | -------------------------- | ---- |
| Query         | ( Q = XW_Q )                                               | どの情報を取りたいか       | –   |
| Key           | ( K = XW_K )                                               | どの情報を持っているか     | –   |
| Value         | ( V = XW_V )                                               | 実際の情報内容             | –   |
| Attention出力 | ( \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V ) | 関係性に基づいた重み付き和 | ✅   |
