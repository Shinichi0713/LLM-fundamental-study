`head_dim` ã¯ **Multi-Head Attention ã«ãŠã‘ã‚‹ 1 ã¤ã®ãƒ˜ãƒƒãƒ‰ãŒæŒã¤å†…éƒ¨ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ** ã‚’æ„å‘³ã—ã¾ã™ã€‚

å°‘ã—è©³ã—ãæ•´ç†ã™ã‚‹ã¨ï¼š

---

## 1. Transformer ã® Multi-Head Attention ã®ä»•çµ„ã¿

1. å…¥åŠ›ã®éš ã‚ŒçŠ¶æ…‹ `hidden_size`ï¼ˆä¾‹: 768æ¬¡å…ƒï¼‰ãŒã‚ã‚‹ã¨ã—ã¾ã™ã€‚
2. Attention ã‚’è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã§ä¸¦åˆ—ã«è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€ã¾ãšã“ã® `hidden_size` ã‚’ **ãƒ˜ãƒƒãƒ‰æ•°ã§åˆ†å‰²**ã—ã¾ã™ã€‚

* ä¾‹ï¼š
  * hidden_size = 768
  * num_heads = 12
  * 1ãƒ˜ãƒƒãƒ‰ã‚ãŸã‚Šã®æ¬¡å…ƒ = head_dim = 768 / 12 = 64

3. å„ãƒ˜ãƒƒãƒ‰ã¯ **64æ¬¡å…ƒã® Query / Key / Value** ã‚’ä½¿ã£ã¦ç‹¬ç«‹ã« Attention è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚

---

## 2. ãªãœåˆ†å‰²ã™ã‚‹ã®ã‹ï¼Ÿ

* 1ã¤ã®ãƒ˜ãƒƒãƒ‰ã ã‘ã§ Attention ã‚’è¨ˆç®—ã™ã‚‹ã¨ã€ã™ã¹ã¦ã®æƒ…å ±ã‚’åŒã˜æ¬¡å…ƒã§å‡¦ç†ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
* è¤‡æ•°ãƒ˜ãƒƒãƒ‰ã«åˆ†ã‘ã‚‹ã“ã¨ã§ã€**å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹é–¢ä¿‚ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æ³¨ç›®ã§ãã‚‹**ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
  * ä¾‹ï¼šã‚ã‚‹ãƒ˜ãƒƒãƒ‰ã¯æ–‡æ³•é–¢ä¿‚ã«æ³¨ç›®ã€åˆ¥ã®ãƒ˜ãƒƒãƒ‰ã¯æ„å‘³é–¢ä¿‚ã«æ³¨ç›®ã€ãªã©

---

## 3. ã¾ã¨ã‚

| ç”¨èª            | èª¬æ˜                                                        |
| --------------- | ----------------------------------------------------------- |
| `hidden_size` | Transformerã®å„å±¤ã®éš ã‚ŒçŠ¶æ…‹ã®å…¨ä½“æ¬¡å…ƒï¼ˆä¾‹ 768ï¼‰             |
| `num_heads`   | Attentionãƒ˜ãƒƒãƒ‰ã®æ•°ï¼ˆä¾‹ 12ï¼‰                                |
| `head_dim`    | 1ã¤ã®ãƒ˜ãƒƒãƒ‰ãŒæŒã¤ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ (`hidden_size / num_heads`) |

* PyTorchã§ã¯ã€Q/K/Vã®ãƒ†ãƒ³ã‚½ãƒ«ã¯é€šå¸¸ `(batch, num_heads, seq_len, head_dim)` ã®å½¢ã§ä¿æŒã•ã‚Œã¾ã™ã€‚
* ã“ã‚Œã‚’ä½¿ã£ã¦ **å„ãƒ˜ãƒƒãƒ‰ã®è¦–ç‚¹ã§ã®ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾** ã‚’è¦³å¯Ÿã§ãã‚‹ã‚ã‘ã§ã™ã€‚


## **Multi-Head Attention (MHA)** ã®ä»•çµ„ã¿
ã‚‚ã¡ã‚ã‚“ã§ã™ï¼
ã“ã“ã§ã¯ **Multi-Head Attention (MHA)** ã®ä»•çµ„ã¿ã¨ã€**PyTorchã«ã‚ˆã‚‹å®Ÿè£…ä¾‹** ã‚’ã‚ã‹ã‚Šã‚„ã™ãç´¹ä»‹ã—ã¾ã™ã€‚

---

## ğŸ¯ ç›®çš„ï¼šMulti-Head Attentionã¨ã¯ï¼Ÿ

å˜ä¸€ã®Attentionã§ã¯ã€æ–‡ã®ã‚ã‚‹éƒ¨åˆ†ï¼ˆä¾‹ãˆã°ã€Œitã€ï¼‰ãŒä»–ã®éƒ¨åˆ†ï¼ˆã€Œthe catã€ãªã©ï¼‰ã«ã©ã®ç¨‹åº¦æ³¨ç›®ã™ã¹ãã‹ã‚’ä¸€ç¨®é¡ã®å°ºåº¦ã§ã—ã‹è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã€‚
Multi-Head Attentionã¯ã€**è¤‡æ•°ã®è¦–ç‚¹ï¼ˆheadï¼‰ã§æ–‡è„ˆã‚’æ‰ãˆã‚‹** ä»•çµ„ã¿ã§ã™ã€‚

---

## ğŸ§® æ•°å¼ã§ã®å®šç¾©

ã¾ãšã€1ã¤ã®ãƒ˜ãƒƒãƒ‰ã®Attentionã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã¾ã™ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
$$

Multi-HeadåŒ–ã§ã¯ã€ã“ã‚Œã‚’è¤‡æ•°ã®ãƒ˜ãƒƒãƒ‰ã§ä¸¦åˆ—å®Ÿè¡Œã—ã¾ã™ï¼š

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$
$$
\text{where } \text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
$$

---

## ğŸ’» PyTorchå®Ÿè£…ä¾‹

ä»¥ä¸‹ã¯ **ã‚·ãƒ³ãƒ—ãƒ«ãªMultiHeadAttentionã®å®Ÿè£…** ã§ã™ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Q, K, V ã®ç·šå½¢å¤‰æ›
        self.W_Q = nn.Linear(embed_dim, embed_dim)
        self.W_K = nn.Linear(embed_dim, embed_dim)
        self.W_V = nn.Linear(embed_dim, embed_dim)

        # å‡ºåŠ›ã®ç·šå½¢å¤‰æ›
        self.W_O = nn.Linear(embed_dim, embed_dim)

    def forward(self, Q, K, V, mask=None):
        batch_size, seq_len, _ = Q.size()

        # Q, K, Vã‚’å„headã«åˆ†å‰²
        Q = self.W_Q(Q).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.W_K(K).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.W_V(V).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‰ãƒ»ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
        scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn_weights = F.softmax(scores, dim=-1)
        attn_output = attn_weights @ V  # [B, heads, seq, head_dim]

        # å„headã‚’çµåˆã—ã¦ç·šå½¢å¤‰æ›
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        output = self.W_O(attn_output)

        return output, attn_weights
```

---

## âœ… ãƒ†ã‚¹ãƒˆä¾‹

```python
batch_size = 2
seq_len = 5
embed_dim = 64
num_heads = 8

mha = MultiHeadAttention(embed_dim, num_heads)
x = torch.randn(batch_size, seq_len, embed_dim)

output, attn = mha(x, x, x)
print("Output shape:", output.shape)
print("Attention weights shape:", attn.shape)
```

å‡ºåŠ›çµæœï¼š

```
Output shape: torch.Size([2, 5, 64])
Attention weights shape: torch.Size([2, 8, 5, 5])
```

---

## ğŸ’¡ è£œè¶³

* `num_heads` ãŒå¤šã„ã»ã©å¤šæ§˜ãªé–¢ä¿‚ã‚’å­¦ç¿’ã§ãã¾ã™ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä¸ŠãŒã‚Šã¾ã™ã€‚
* å®Ÿéš›ã®Transformerã§ã¯ã€Self-Attentionã‚„Cross-Attentionã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚
* RoPEï¼ˆRotary Position Embeddingï¼‰ãªã©ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€ä½ç½®æƒ…å ±ã‚’è‡ªç„¶ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

---

ã”å¸Œæœ›ãŒã‚ã‚Œã°ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã« **RoPEå¯¾å¿œç‰ˆ** ã‚„ **å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ä»˜ããƒãƒ¼ã‚¸ãƒ§ãƒ³** ã‚‚è¿½åŠ ã§ãã¾ã™ã€‚
ã©ã¡ã‚‰ã‚’ã”è¦§ã«ãªã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ

