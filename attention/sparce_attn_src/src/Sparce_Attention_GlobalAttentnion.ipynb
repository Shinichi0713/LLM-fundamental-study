{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from typing import Optional, List"
      ],
      "metadata": {
        "id": "s0g2wxSF6nVb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# ユーティリティ: 1D unfold (K/V のウィンドウ抽出)\n",
        "# -----------------------------\n",
        "def unfold_kv(x: torch.Tensor, kernel_size: int, padding: int = 0):\n",
        "    \"\"\"\n",
        "    x: (B, H, T, D)\n",
        "    returns: (B, H, T, window_len, D)\n",
        "    kernel_size: window length (2*window + 1)\n",
        "    padding: left/right padding for edges (usually window)\n",
        "    \"\"\"\n",
        "    B, H, T, D = x.shape\n",
        "    # reshape to image-like: (B*H, D, 1, T)\n",
        "    x_img = x.permute(0, 1, 3, 2).reshape(B * H, D, 1, T)\n",
        "    # use unfold on width dimension (kernel_size)\n",
        "    # unfold returns (B*H, D * kernel_size, L_out) where L_out = T if stride=1 and padding applied\n",
        "    x_unf = F.unfold(x_img, kernel_size=(1, kernel_size), padding=(0, padding), stride=(1, 1))\n",
        "    # reshape to (B*H, D, kernel_size, T)\n",
        "    x_unf = x_unf.view(B * H, D, kernel_size, T)\n",
        "    # permute to (B, H, T, kernel_size, D)\n",
        "    x_unf = x_unf.permute(0, 3, 2, 1).reshape(B, H, T, kernel_size, D)\n",
        "    return x_unf\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Hybrid Sparse Attention (Local + Global)\n",
        "# -----------------------------\n",
        "class HybridSparseAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int = 8, window: int = 4, dropout: float = 0.0):\n",
        "        \"\"\"\n",
        "        dim: model embed dim\n",
        "        num_heads: # heads\n",
        "        window: half-window size (each token attends to 2*window+1 neighbors centered)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.window = window\n",
        "        self.kernel_size = 2 * window + 1\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,                    # (B, T, D)\n",
        "        global_mask: Optional[torch.Tensor] # (B, T) bool: True -> this position is global token\n",
        "    ):\n",
        "        B, T, D = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # 1) Project and shape into multi-head: (B, H, T, head_dim)\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # q,k,v: (B, H, T, D_head)\n",
        "\n",
        "        # 2) Extract local windows for K and V via unfold (vectorized)\n",
        "        # pad = window to allow edges to have full kernel_size with padding positions\n",
        "        K_windows = unfold_kv(k, kernel_size=self.kernel_size, padding=self.window)  # (B,H,T,win,Dh)\n",
        "        V_windows = unfold_kv(v, kernel_size=self.kernel_size, padding=self.window)  # (B,H,T,win,Dh)\n",
        "\n",
        "        # 3) Local scores: Q (B,H,T,dh) @ K_windows.transpose(-2,-1) -> (B,H,T,win)\n",
        "        # Use einsum for clarity and speed\n",
        "        # scores_local shape: (B, H, T, win)\n",
        "        scores_local = torch.einsum(\"bhtd,bhtwd->bhtw\", q, K_windows) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # 4) Global part: gather global keys/values\n",
        "        if global_mask is None:\n",
        "            # no global, just compute local attentions\n",
        "            scores_global = None\n",
        "            V_global = None\n",
        "            K_global = None\n",
        "        else:\n",
        "            # indices of global per batch may vary, so gather per batch\n",
        "            # build K_global: (B, H, G, Dh)\n",
        "            # build V_global: (B, H, G, Dh)\n",
        "            global_idx_list: List[torch.Tensor] = []\n",
        "            maxG = 0\n",
        "            for b in range(B):\n",
        "                idx = torch.nonzero(global_mask[b].to(device), as_tuple=False).squeeze(-1)\n",
        "                global_idx_list.append(idx)\n",
        "                if idx.numel() > maxG:\n",
        "                    maxG = idx.numel()\n",
        "            if maxG == 0:\n",
        "                scores_global = None\n",
        "                K_global = None\n",
        "                V_global = None\n",
        "            else:\n",
        "                # We'll pad global lists to same length maxG with zeros and use a mask\n",
        "                K_global = torch.zeros(B, self.num_heads, maxG, self.head_dim, device=device)\n",
        "                V_global = torch.zeros(B, self.num_heads, maxG, self.head_dim, device=device)\n",
        "                global_token_mask = torch.zeros(B, maxG, dtype=torch.bool, device=device)\n",
        "                for b in range(B):\n",
        "                    idx = global_idx_list[b]\n",
        "                    if idx.numel() == 0:\n",
        "                        continue\n",
        "                    kg = k[b, :, idx, :]  # (H, G_b, Dh)\n",
        "                    vg = v[b, :, idx, :]\n",
        "                    G_b = kg.shape[1]\n",
        "                    K_global[b, :, :G_b, :] = kg\n",
        "                    V_global[b, :, :G_b, :] = vg\n",
        "                    global_token_mask[b, :G_b] = True  # valid positions\n",
        "\n",
        "                # Now compute scores_global: Q (B,H,T,dh) @ K_global.transpose(-2,-1) -> (B,H,T,G)\n",
        "                # Note: broadcasting across G (maxG)\n",
        "                # We'll produce scores_global with padded positions (where global_token_mask=False), their scores will be -inf later\n",
        "                scores_global = torch.einsum(\"bhtd,bhgd->bhtg\", q, K_global) / (self.head_dim ** 0.5)\n",
        "                # mask out padded global slots later\n",
        "\n",
        "        # 5) Combine local and global scores: concat along last dim and softmax\n",
        "        if scores_global is None:\n",
        "            # only local\n",
        "            attn_weights = F.softmax(scores_local, dim=-1)  # shape (B,H,T,win)\n",
        "            # Weighted sum over V_windows\n",
        "            context_local = torch.einsum(\"bhtw,bhtwd->bhtd\", attn_weights, V_windows)  # (B,H,T,dh)\n",
        "            out = context_local\n",
        "        else:\n",
        "            # mask invalid global slots by large negative\n",
        "            # scores_global: (B,H,T,G)\n",
        "            # create mask for G: (B, G)\n",
        "            gmask = global_token_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,G)\n",
        "            # set invalid global positions to -1e9\n",
        "            scores_global = scores_global.masked_fill(~gmask, float(\"-1e9\"))\n",
        "\n",
        "            # concat local + global -> (B,H,T, win + G)\n",
        "            scores_cat = torch.cat([scores_local, scores_global], dim=-1)  # (B,H,T, Ktot)\n",
        "\n",
        "            attn_weights_cat = F.softmax(scores_cat, dim=-1)  # (B,H,T,Ktot)\n",
        "            attn_weights_cat = self.dropout(attn_weights_cat)\n",
        "\n",
        "            # split weights\n",
        "            w_local = attn_weights_cat[..., : self.kernel_size]  # (B,H,T,win)\n",
        "            w_global = attn_weights_cat[..., self.kernel_size :]  # (B,H,T,G)\n",
        "\n",
        "            # compute contexts\n",
        "            ctx_local = torch.einsum(\"bhtw,bhtwd->bhtd\", w_local, V_windows)  # (B,H,T,dh)\n",
        "            ctx_global = torch.einsum(\"bhtg,bhgd->bhtd\", w_global, V_global)  # (B,H,T,dh)\n",
        "\n",
        "            out = ctx_local + ctx_global\n",
        "\n",
        "        # 6) merge heads & project out\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, D)\n",
        "        out = self.out_proj(out)  # (B,T,D)\n",
        "\n",
        "        # Also optionally return a full-attention-like map for visualization:\n",
        "        # Build full_attn: (B, T, T) with zeros outside local windows and global slots indicated\n",
        "        # For memory reasons we build per-batch if requested (here we always build for simplicity)\n",
        "        full_attn = torch.zeros(B, self.num_heads, T, T, device=device)\n",
        "        # fill local parts\n",
        "        # we'll vectorize: for each offset in kernel_size, compute contribution positions\n",
        "        offsets = torch.arange(-self.window, self.window + 1, device=device)\n",
        "        # positions matrix: (T, kernel_size) -> indices in [0,T)\n",
        "        pos_idx = (torch.arange(T, device=device).unsqueeze(1) + offsets.unsqueeze(0)).clamp(0, T - 1)\n",
        "        # w_local shape (B,H,T,win) -> assign into full_attn[..., pos_idx]\n",
        "        # We can scatter\n",
        "        for i in range(self.kernel_size):\n",
        "            idx = pos_idx[:, i]  # (T,)\n",
        "            full_attn[..., :, idx] += 0  # no-op to ensure size; below we fill per-batch\n",
        "        # We'll fill per-batch-head with loop over batch/head (small overhead for visualization)\n",
        "        for b in range(B):\n",
        "            for h in range(self.num_heads):\n",
        "                # local\n",
        "                for t in range(T):\n",
        "                    left = max(0, t - self.window)\n",
        "                    right = min(T, t + self.window + 1)\n",
        "                    win_len = right - left\n",
        "                    if scores_global is None:\n",
        "                        wloc = F.softmax(scores_local[b, h, t, :win_len], dim=-1)\n",
        "                        full_attn[b, h, t, left:right] = wloc\n",
        "                    else:\n",
        "                        # recompute combined weights for this (b,h,t) to be consistent\n",
        "                        # slice\n",
        "                        s_local = scores_local[b, h, t, :win_len]\n",
        "                        s_glob = scores_global[b, h, t, : scores_global.shape[-1]]\n",
        "                        s_cat = torch.cat([s_local, s_glob], dim=-1)\n",
        "                        wcat = F.softmax(s_cat, dim=-1)\n",
        "                        wloc = wcat[:win_len]\n",
        "                        wglob = wcat[win_len:]\n",
        "                        full_attn[b, h, t, left:right] = wloc\n",
        "                        if global_token_mask[b].any():\n",
        "                            # distribute global weights to actual positions\n",
        "                            glob_idx = global_idx_list[b]  # indices tensor\n",
        "                            # if padded, wglob length = maxG; but only first len(glob_idx) are valid\n",
        "                            G_b = glob_idx.numel()\n",
        "                            if G_b > 0:\n",
        "                                full_attn[b, h, t, glob_idx] += wglob[:G_b]\n",
        "\n",
        "        return out, full_attn  # out: (B,T,D), full_attn: (B,H,T,T)\n"
      ],
      "metadata": {
        "id": "Upe3oS1E6riR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MS1o5DisHhF",
        "outputId": "240a0160-a6ac-4508-ecea-4f0a9cb882a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time hybrid: 1.9833s  | Time full: 0.0376s  | speedup: 0.02x\n",
            "Output norm diff (L2): 40.52375793457031\n",
            "hybrid out shape: torch.Size([2, 512, 256])\n",
            "full out shape: torch.Size([2, 512, 256])\n",
            "attn_h shape: torch.Size([2, 8, 512, 512])\n",
            "attn_f shape: torch.Size([2, 8, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# ベンチマーク＆簡単な精度チェック\n",
        "# -----------------------------\n",
        "def benchmark_and_check():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    B = 2\n",
        "    T = 512\n",
        "    D = 256\n",
        "    H = 8\n",
        "    window = 8\n",
        "    # random data\n",
        "    x = torch.randn(B, T, D, device=device)\n",
        "\n",
        "    # mark a few global tokens per batch\n",
        "    global_mask = torch.zeros(B, T, dtype=torch.bool, device=device)\n",
        "    # choose 3 global tokens per batch\n",
        "    import random\n",
        "    random.seed(0)\n",
        "    for b in range(B):\n",
        "        choices = random.sample(range(T), 3)\n",
        "        global_mask[b, choices] = True\n",
        "\n",
        "    # modules\n",
        "    hybrid = HybridSparseAttention(dim=D, num_heads=H, window=window).to(device)\n",
        "    # baseline full multihead attention via simple implementation\n",
        "    class FullMHA(nn.Module):\n",
        "        def __init__(self, D, H):\n",
        "            super().__init__()\n",
        "            self.D = D\n",
        "            self.H = H\n",
        "            self.head = HybridSparseAttention(D, H, window=window)  # reuse projections\n",
        "        def forward(self, x):\n",
        "            # naive: compute full attention by using same projections\n",
        "            B,T,D = x.shape\n",
        "            q = self.head.q_proj(x).view(B, T, self.head.num_heads, self.head.head_dim).permute(0,2,1,3)\n",
        "            k = self.head.k_proj(x).view(B, T, self.head.num_heads, self.head.head_dim).permute(0,2,1,3)\n",
        "            v = self.head.v_proj(x).view(B, T, self.head.num_heads, self.head.head_dim).permute(0,2,1,3)\n",
        "            scores = torch.einsum(\"bhqd,bhkd->bhqk\", q, k) / (self.head.head_dim ** 0.5)\n",
        "            attn = F.softmax(scores, dim=-1)\n",
        "            ctx = torch.einsum(\"bhqk,bhkd->bhqd\", attn, v)\n",
        "            out = ctx.permute(0,2,1,3).reshape(B,T,D)\n",
        "            out = self.head.out_proj(out)\n",
        "            return out, attn\n",
        "\n",
        "    fullmha = FullMHA(D, H).to(device)\n",
        "\n",
        "    # warmup\n",
        "    for _ in range(3):\n",
        "        _ = hybrid(x, global_mask)\n",
        "        _ = fullmha(x)\n",
        "\n",
        "    # time hybrid\n",
        "    torch.cuda.synchronize() if device==\"cuda\" else None\n",
        "    t0 = time.time()\n",
        "    out_h, attn_h = hybrid(x, global_mask)\n",
        "    torch.cuda.synchronize() if device==\"cuda\" else None\n",
        "    t1 = time.time()\n",
        "\n",
        "    t_hybrid = t1 - t0\n",
        "\n",
        "    # time full\n",
        "    torch.cuda.synchronize() if device==\"cuda\" else None\n",
        "    t0 = time.time()\n",
        "    out_f, attn_f = fullmha(x)\n",
        "    torch.cuda.synchronize() if device==\"cuda\" else None\n",
        "    t1 = time.time()\n",
        "    t_full = t1 - t0\n",
        "\n",
        "    print(f\"Time hybrid: {t_hybrid:.4f}s  | Time full: {t_full:.4f}s  | speedup: {t_full / max(t_hybrid,1e-12):.2f}x\")\n",
        "    # basic relative difference check (not exact since hybrid is sparse)\n",
        "    print(\"Output norm diff (L2):\", torch.norm(out_h - out_f).item())\n",
        "    print(\"hybrid out shape:\", out_h.shape)\n",
        "    print(\"full out shape:\", out_f.shape)\n",
        "    print(\"attn_h shape:\", attn_h.shape)\n",
        "    print(\"attn_f shape:\", attn_f.shape)\n",
        "\n",
        "    return hybrid, fullmha, x, global_mask, out_h, out_f, attn_h, attn_f\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 実行ベンチマーク\n",
        "# ============================\n",
        "if __name__ == \"__main__\":\n",
        "    hybrid, fullmha, x, gmask, out_h, out_f, attn_h, attn_f = benchmark_and_check()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CUaF-vd6vNr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}