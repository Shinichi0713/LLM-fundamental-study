# **Global Attention**

**グローバルアテンション（Global Attention）** は、スパースアテンションの一種として、長距離の依存関係を効率的に捕捉するために非常に重要な役割を果たします。

その特徴、メリット、そして採用している主要なAIモデルについて解説します。

---

## 🌟 グローバルアテンションの特徴

グローバルアテンションは、ローカルアテンション（近隣トークンのみに注目）の弱点である「長距離の文脈の欠落」を補うために、**特定のトークンを優遇する**仕組みを採用しています。

### 1. 特徴：情報ハブの設置

グローバルアテンションの核となるのは、シーケンス内のごく少数のトークンを**「グローバルトークン」**として指定することです。

* **グローバルトークン** ：
* **全てのトークンに注目する（Attends to all tokens）** 。これにより、シーケンス全体の情報を収集し、文脈の要約を行います。
* **全てのトークンから注目される（Is attended by all tokens）** 。これにより、他の全てのトークンがこのグローバルトークンを通じて、間接的に長距離の情報を得ることができます。
* **ローカルトークン（残りの大多数のトークン）** ：
* 通常は、自身を中心とした**ローカルな窓**内のトークンに注目します。
* さらに、このローカルな窓に加えて、**すべてのグローバルトークン**にも注目します。

### 2. スパース化のパターン

この混合型のパターンにより、Attentionマップは以下のような構造になります。

* **帯状（バンド）の接続** ：ローカルトークン同士の接続（Local Attention）。
* **行・列の接続** ：グローバルトークンと、残りの全てのトークンとの接続（Global Attention）。

これにより、計算量は **$O(N^2)$** ではなく、例えば **$O(N \cdot W + N \cdot G)$** （**$W$** は窓サイズ、**$G$** はグローバルトークン数）のように、シーケンス長 **$N$** に対して線形に近い形で抑えられます。

---

## 📈 グローバルアテンションのメリット

グローバルアテンションを導入するメリットは、主に以下の3点です。

1. 長距離依存関係の捕捉:
   ローカルアテンションでは失われる、遠く離れたトークン間の重要な依存関係を、グローバルトークンが情報の中継役となることで効率的に捕捉できます。これは、文書要約や質問応答など、文書全体を理解する必要があるタスクで特に重要です。
2. 計算効率の維持:
   全てのトークン間の接続を維持するわけではないため、Transformerのボトルネックである $O(N^2)$ の計算コストを回避し、長大な入力シーケンスの処理を可能にします。
3. 柔軟な文脈要約:
   グローバルトークンは、シーケンスの全体的な主題や最も重要な情報を集約する「サマリーノード」として機能するため、モデルがタスクに必要な文脈を柔軟に構築できます。

---

## 🤖 採用されているAIモデルの例

グローバルアテンション、またはこの概念を組み込んだ混合型のスパースアテンションは、特に長文処理に特化したモデルで採用されています。

### 1. Longformer

* **概要** : Facebook AIによって開発されたTransformerモデルで、非常に長い文書（最大4096トークン以上）を処理できるように設計されています。
* **採用パターン** : **ローカルアテンション**を基本としつつ、特定の事前定義されたトークン（例：`[CLS]`トークンや、特定のタスクに特化したトークン）に**グローバルアテンション**を適用することで、長距離の文脈を効果的に保持しています。

### 2. BigBird

* **概要** : Googleによって開発されたTransformerモデルで、Longformerと同様に長いシーケンスを処理できます。
* **採用パターン** : **ローカルアテンション**に加えて、 **ランダムなアテンション** 、そして**グローバルアテンション**の3種類のスパースアテンションを組み合わせることで、理論上、線形の計算量 **$O(N)$** を実現しています。

このように、グローバルアテンションは、限られた計算資源の中でTransformerモデルの「長距離記憶」能力を向上させるための、実用的で効果的な解決策となっています。
