# **Global Attention**

**グローバルアテンション（Global Attention）** は、スパースアテンションの一種として、長距離の依存関係を効率的に捕捉するために非常に重要な役割を果たします。

その特徴、メリット、そして採用している主要なAIモデルについて解説します。

---

## 🌟 グローバルアテンションの特徴

グローバルアテンションは、ローカルアテンション（近隣トークンのみに注目）の弱点である「長距離の文脈の欠落」を補うために、**特定のトークンを優遇する**仕組みを採用しています。

### 1. 特徴：情報ハブの設置

グローバルアテンションの核となるのは、シーケンス内のごく少数のトークンを**「グローバルトークン」**として指定することです。

* **グローバルトークン** ：
* **全てのトークンに注目する（Attends to all tokens）** 。これにより、シーケンス全体の情報を収集し、文脈の要約を行います。
* **全てのトークンから注目される（Is attended by all tokens）** 。これにより、他の全てのトークンがこのグローバルトークンを通じて、間接的に長距離の情報を得ることができます。
* **ローカルトークン（残りの大多数のトークン）** ：
* 通常は、自身を中心とした**ローカルな窓**内のトークンに注目します。
* さらに、このローカルな窓に加えて、**すべてのグローバルトークン**にも注目します。

### 2. スパース化のパターン

この混合型のパターンにより、Attentionマップは以下のような構造になります。

* **帯状（バンド）の接続** ：ローカルトークン同士の接続（Local Attention）。
* **行・列の接続** ：グローバルトークンと、残りの全てのトークンとの接続（Global Attention）。

これにより、計算量は **$O(N^2)$** ではなく、例えば **$O(N \cdot W + N \cdot G)$** （**$W$** は窓サイズ、**$G$** はグローバルトークン数）のように、シーケンス長 **$N$** に対して線形に近い形で抑えられます。

---

## 📈 グローバルアテンションのメリット

グローバルアテンションを導入するメリットは、主に以下の3点です。

1. 長距離依存関係の捕捉:
   ローカルアテンションでは失われる、遠く離れたトークン間の重要な依存関係を、グローバルトークンが情報の中継役となることで効率的に捕捉できます。これは、文書要約や質問応答など、文書全体を理解する必要があるタスクで特に重要です。
2. 計算効率の維持:
   全てのトークン間の接続を維持するわけではないため、Transformerのボトルネックである $O(N^2)$ の計算コストを回避し、長大な入力シーケンスの処理を可能にします。
3. 柔軟な文脈要約:
   グローバルトークンは、シーケンスの全体的な主題や最も重要な情報を集約する「サマリーノード」として機能するため、モデルがタスクに必要な文脈を柔軟に構築できます。

---

## 🤖 採用されているAIモデルの例

グローバルアテンション、またはこの概念を組み込んだ混合型のスパースアテンションは、特に長文処理に特化したモデルで採用されています。

### 1. Longformer

* **概要** : Facebook AIによって開発されたTransformerモデルで、非常に長い文書（最大4096トークン以上）を処理できるように設計されています。
* **採用パターン** : **ローカルアテンション**を基本としつつ、特定の事前定義されたトークン（例：`[CLS]`トークンや、特定のタスクに特化したトークン）に**グローバルアテンション**を適用することで、長距離の文脈を効果的に保持しています。

### 2. BigBird

* **概要** : Googleによって開発されたTransformerモデルで、Longformerと同様に長いシーケンスを処理できます。
* **採用パターン** : **ローカルアテンション**に加えて、 **ランダムなアテンション** 、そして**グローバルアテンション**の3種類のスパースアテンションを組み合わせることで、理論上、線形の計算量 **$O(N)$** を実現しています。

このように、グローバルアテンションは、限られた計算資源の中でTransformerモデルの「長距離記憶」能力を向上させるための、実用的で効果的な解決策となっています。


# **ハイブリッド疎結合アテンション（Hybrid Sparse Attention）**メカニズム

このメカニズムは、通常のTransformerが抱える$O(T^2)$（シーケンス長の2乗）の計算負荷を、$O(T)$（線形）に抑えつつ、長距離の依存関係を捉える能力を維持することを目的としています。

---

## コードの主要な機能と構造

このコードは、主に2つの部分から構成されています。

### 1. `unfold_kv` 関数 (ローカルウィンドウ抽出)

これは、ローカルアテンション（近傍トークンへの注意）に必要な**K（Key）**と**V（Value）**を効率的に抽出するためのユーティリティ関数です。

* **機能:** 入力シーケンス `x`（KまたはV）の各トークンに対し、そのトークンを中心とした固定長の**ウィンドウ（近傍）**に含まれるトークンを抜き出します。
* **技術:** PyTorchの`F.unfold`関数（通常は畳み込みネットワークのプーリングなどで使われる）を1次元シーケンスに応用することで、ループを使わずに高速にウィンドウ抽出を行っています。
* **出力形状:** `(B, H, T, window_len, D)`。これは、「バッチサイズ、ヘッド数、トークン数、**ウィンドウ内のトークン数**、ヘッドの次元」を意味します。

### 2. `HybridSparseAttention` クラス (ハイブリッドアテンションの実装)

このクラスは、**ローカルアテンション**と**グローバルアテンション**を組み合わせて最終的な出力を計算します。

#### A. ローカルアテンションの計算

1.  `unfold_kv` を使用して、KとVのローカルウィンドウ `K_windows` と `V_windows` を抽出します。
2.  各クエリ `Q` とローカルウィンドウ内のキー `K_windows` との内積を計算し、**ローカルスコア**を算出します。

#### B. グローバルアテンションの計算

1.  入力された `global_mask`（グローバルトークンを示すマスク）に基づき、シーケンス全体からグローバルトークンに対応するKとV（`K_global` および `V_global`）を収集します。
2.  このとき、バッチごとにグローバルトークンの数が異なる可能性があるため、最大長 `maxG` に合わせてパディングを行い、**マスク**を適用します。
3.  各クエリ `Q` とグローバルキー `K_global` との内積を計算し、**グローバルスコア**を算出します。

#### C. スコアの結合と出力

1.  ローカルスコアとグローバルスコアを最後の次元で**結合（`torch.cat`）**します。
2.  結合されたスコア全体に対して **Softmax** を適用することで、各トークンが「ローカルの近傍」と「すべてのグローバルトークン」のどちらにどれだけ注意を払うかを決定します。
3.  この重み（`w_local` と `w_global`）を使って、対応するV（`V_windows` と `V_global`）を重み付けし、**ローカルコンテキスト**と**グローバルコンテキスト**を計算し、足し合わせます。
4.  ヘッドを統合（`merge heads`）し、最終的な出力射影（`out_proj`）を通します。

#### D. 可視化のためのフルアテンションマップ

* コードの最後では、計算された疎結合な重みを、全結合アテンションと同様の形状 `(B, H, T, T)` の**`full_attn`**テンソルにマッピングし直す処理が含まれています。
* これは、実際の計算には使用されませんが、**どのトークンがどのトークンに注意を払っているか**を可視化（デバッグや解析）するために役立ちます。

### Big Birdにおける疎結合アテンションの仕組み

Big Birdの疎結合アテンションは、このコードで実装されている「ローカルアテンション」と「グローバルアテンション」に加え、**「ランダムアテンション」**の3つを組み合わせることで、**全結合アテンションの特性を維持**しながら、計算負荷を線形に抑えます。



この実装では、ローカルとグローバルのハイブリッド構造に焦点を当てており、疎結合アテンションの中核となる仕組みを効果的に再現しています。
