# **Global Attention**

**ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆGlobal Attentionï¼‰** ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä¸€ç¨®ã¨ã—ã¦ã€é•·è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«æ•æ‰ã™ã‚‹ãŸã‚ã«éå¸¸ã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

ãã®ç‰¹å¾´ã€ãƒ¡ãƒªãƒƒãƒˆã€ãã—ã¦æ¡ç”¨ã—ã¦ã„ã‚‹ä¸»è¦ãªAIãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚

---

## ğŸŒŸ ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ç‰¹å¾´

ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆè¿‘éš£ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã«æ³¨ç›®ï¼‰ã®å¼±ç‚¹ã§ã‚ã‚‹ã€Œé•·è·é›¢ã®æ–‡è„ˆã®æ¬ è½ã€ã‚’è£œã†ãŸã‚ã«ã€**ç‰¹å®šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„ªé‡ã™ã‚‹**ä»•çµ„ã¿ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

### 1. ç‰¹å¾´ï¼šæƒ…å ±ãƒãƒ–ã®è¨­ç½®

ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æ ¸ã¨ãªã‚‹ã®ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ã”ãå°‘æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’**ã€Œã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã€**ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã“ã¨ã§ã™ã€‚

* **ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³** ï¼š
* **å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®ã™ã‚‹ï¼ˆAttends to all tokensï¼‰** ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã®æƒ…å ±ã‚’åé›†ã—ã€æ–‡è„ˆã®è¦ç´„ã‚’è¡Œã„ã¾ã™ã€‚
* **å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰æ³¨ç›®ã•ã‚Œã‚‹ï¼ˆIs attended by all tokensï¼‰** ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä»–ã®å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã“ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€šã˜ã¦ã€é–“æ¥çš„ã«é•·è·é›¢ã®æƒ…å ±ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
* **ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ®‹ã‚Šã®å¤§å¤šæ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰** ï¼š
* é€šå¸¸ã¯ã€è‡ªèº«ã‚’ä¸­å¿ƒã¨ã—ãŸ**ãƒ­ãƒ¼ã‚«ãƒ«ãªçª“**å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®ã—ã¾ã™ã€‚
* ã•ã‚‰ã«ã€ã“ã®ãƒ­ãƒ¼ã‚«ãƒ«ãªçª“ã«åŠ ãˆã¦ã€**ã™ã¹ã¦ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³**ã«ã‚‚æ³¨ç›®ã—ã¾ã™ã€‚

### 2. ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³

ã“ã®æ··åˆå‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€Attentionãƒãƒƒãƒ—ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã‚Šã¾ã™ã€‚

* **å¸¯çŠ¶ï¼ˆãƒãƒ³ãƒ‰ï¼‰ã®æ¥ç¶š** ï¼šãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³åŒå£«ã®æ¥ç¶šï¼ˆLocal Attentionï¼‰ã€‚
* **è¡Œãƒ»åˆ—ã®æ¥ç¶š** ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã€æ®‹ã‚Šã®å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã®æ¥ç¶šï¼ˆGlobal Attentionï¼‰ã€‚

ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã¯ **$O(N^2)$** ã§ã¯ãªãã€ä¾‹ãˆã° **$O(N \cdot W + N \cdot G)$** ï¼ˆ**$W$** ã¯çª“ã‚µã‚¤ã‚ºã€**$G$** ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ã®ã‚ˆã†ã«ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· **$N$** ã«å¯¾ã—ã¦ç·šå½¢ã«è¿‘ã„å½¢ã§æŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚

---

## ğŸ“ˆ ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒ¡ãƒªãƒƒãƒˆ

ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å°å…¥ã™ã‚‹ãƒ¡ãƒªãƒƒãƒˆã¯ã€ä¸»ã«ä»¥ä¸‹ã®3ç‚¹ã§ã™ã€‚

1. é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®æ•æ‰:
   ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã§ã¯å¤±ã‚ã‚Œã‚‹ã€é ãé›¢ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é‡è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ãŒæƒ…å ±ã®ä¸­ç¶™å½¹ã¨ãªã‚‹ã“ã¨ã§åŠ¹ç‡çš„ã«æ•æ‰ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€æ–‡æ›¸è¦ç´„ã‚„è³ªå•å¿œç­”ãªã©ã€æ–‡æ›¸å…¨ä½“ã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ã§ç‰¹ã«é‡è¦ã§ã™ã€‚
2. è¨ˆç®—åŠ¹ç‡ã®ç¶­æŒ:
   å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®æ¥ç¶šã‚’ç¶­æŒã™ã‚‹ã‚ã‘ã§ã¯ãªã„ãŸã‚ã€Transformerã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ã‚ã‚‹ $O(N^2)$ ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å›é¿ã—ã€é•·å¤§ãªå…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
3. æŸ”è»Ÿãªæ–‡è„ˆè¦ç´„:
   ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å…¨ä½“çš„ãªä¸»é¡Œã‚„æœ€ã‚‚é‡è¦ãªæƒ…å ±ã‚’é›†ç´„ã™ã‚‹ã€Œã‚µãƒãƒªãƒ¼ãƒãƒ¼ãƒ‰ã€ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ã‚¹ã‚¯ã«å¿…è¦ãªæ–‡è„ˆã‚’æŸ”è»Ÿã«æ§‹ç¯‰ã§ãã¾ã™ã€‚

---

## ğŸ¤– æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹AIãƒ¢ãƒ‡ãƒ«ã®ä¾‹

ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€ã¾ãŸã¯ã“ã®æ¦‚å¿µã‚’çµ„ã¿è¾¼ã‚“ã æ··åˆå‹ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã€ç‰¹ã«é•·æ–‡å‡¦ç†ã«ç‰¹åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

### 1. Longformer

* **æ¦‚è¦** : Facebook AIã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸTransformerãƒ¢ãƒ‡ãƒ«ã§ã€éå¸¸ã«é•·ã„æ–‡æ›¸ï¼ˆæœ€å¤§4096ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šï¼‰ã‚’å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
* **æ¡ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³** : **ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ã‚’åŸºæœ¬ã¨ã—ã¤ã¤ã€ç‰¹å®šã®äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹ï¼š`[CLS]`ãƒˆãƒ¼ã‚¯ãƒ³ã‚„ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«**ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€é•·è·é›¢ã®æ–‡è„ˆã‚’åŠ¹æœçš„ã«ä¿æŒã—ã¦ã„ã¾ã™ã€‚

### 2. BigBird

* **æ¦‚è¦** : Googleã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸTransformerãƒ¢ãƒ‡ãƒ«ã§ã€Longformerã¨åŒæ§˜ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã§ãã¾ã™ã€‚
* **æ¡ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³** : **ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ã«åŠ ãˆã¦ã€ **ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³** ã€ãã—ã¦**ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ã®3ç¨®é¡ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ç†è«–ä¸Šã€ç·šå½¢ã®è¨ˆç®—é‡ **$O(N)$** ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

ã“ã®ã‚ˆã†ã«ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã€é™ã‚‰ã‚ŒãŸè¨ˆç®—è³‡æºã®ä¸­ã§Transformerãƒ¢ãƒ‡ãƒ«ã®ã€Œé•·è·é›¢è¨˜æ†¶ã€èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã€å®Ÿç”¨çš„ã§åŠ¹æœçš„ãªè§£æ±ºç­–ã¨ãªã£ã¦ã„ã¾ã™ã€‚


# **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç–çµåˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆHybrid Sparse Attentionï¼‰**ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

ã“ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€é€šå¸¸ã®TransformerãŒæŠ±ãˆã‚‹$O(T^2)$ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®2ä¹—ï¼‰ã®è¨ˆç®—è² è·ã‚’ã€$O(T)$ï¼ˆç·šå½¢ï¼‰ã«æŠ‘ãˆã¤ã¤ã€é•·è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹èƒ½åŠ›ã‚’ç¶­æŒã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚

---

## ã‚³ãƒ¼ãƒ‰ã®ä¸»è¦ãªæ©Ÿèƒ½ã¨æ§‹é€ 

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ä¸»ã«2ã¤ã®éƒ¨åˆ†ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

### 1. `unfold_kv` é–¢æ•° (ãƒ­ãƒ¼ã‚«ãƒ«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æŠ½å‡º)

ã“ã‚Œã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆè¿‘å‚ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨æ„ï¼‰ã«å¿…è¦ãª**Kï¼ˆKeyï¼‰**ã¨**Vï¼ˆValueï¼‰**ã‚’åŠ¹ç‡çš„ã«æŠ½å‡ºã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã§ã™ã€‚

* **æ©Ÿèƒ½:** å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ `x`ï¼ˆKã¾ãŸã¯Vï¼‰ã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã€ãã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸­å¿ƒã¨ã—ãŸå›ºå®šé•·ã®**ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆè¿‘å‚ï¼‰**ã«å«ã¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠœãå‡ºã—ã¾ã™ã€‚
* **æŠ€è¡“:** PyTorchã®`F.unfold`é–¢æ•°ï¼ˆé€šå¸¸ã¯ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ—ãƒ¼ãƒªãƒ³ã‚°ãªã©ã§ä½¿ã‚ã‚Œã‚‹ï¼‰ã‚’1æ¬¡å…ƒã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¿œç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ã‚ãšã«é«˜é€Ÿã«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æŠ½å‡ºã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
* **å‡ºåŠ›å½¢çŠ¶:** `(B, H, T, window_len, D)`ã€‚ã“ã‚Œã¯ã€ã€Œãƒãƒƒãƒã‚µã‚¤ã‚ºã€ãƒ˜ãƒƒãƒ‰æ•°ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€**ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°**ã€ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒã€ã‚’æ„å‘³ã—ã¾ã™ã€‚

### 2. `HybridSparseAttention` ã‚¯ãƒ©ã‚¹ (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…)

ã“ã®ã‚¯ãƒ©ã‚¹ã¯ã€**ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ã¨**ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ã‚’çµ„ã¿åˆã‚ã›ã¦æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

#### A. ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—

1.  `unfold_kv` ã‚’ä½¿ç”¨ã—ã¦ã€Kã¨Vã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ `K_windows` ã¨ `V_windows` ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
2.  å„ã‚¯ã‚¨ãƒª `Q` ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ã‚­ãƒ¼ `K_windows` ã¨ã®å†…ç©ã‚’è¨ˆç®—ã—ã€**ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ã‚³ã‚¢**ã‚’ç®—å‡ºã—ã¾ã™ã€‚

#### B. ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—

1.  å…¥åŠ›ã•ã‚ŒãŸ `global_mask`ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¤ºã™ãƒã‚¹ã‚¯ï¼‰ã«åŸºã¥ãã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã‹ã‚‰ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾å¿œã™ã‚‹Kã¨Vï¼ˆ`K_global` ãŠã‚ˆã³ `V_global`ï¼‰ã‚’åé›†ã—ã¾ã™ã€‚
2.  ã“ã®ã¨ãã€ãƒãƒƒãƒã”ã¨ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æœ€å¤§é•· `maxG` ã«åˆã‚ã›ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€**ãƒã‚¹ã‚¯**ã‚’é©ç”¨ã—ã¾ã™ã€‚
3.  å„ã‚¯ã‚¨ãƒª `Q` ã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ¼ `K_global` ã¨ã®å†…ç©ã‚’è¨ˆç®—ã—ã€**ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ã‚¢**ã‚’ç®—å‡ºã—ã¾ã™ã€‚

#### C. ã‚¹ã‚³ã‚¢ã®çµåˆã¨å‡ºåŠ›

1.  ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ã‚³ã‚¢ã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ã‚³ã‚¢ã‚’æœ€å¾Œã®æ¬¡å…ƒã§**çµåˆï¼ˆ`torch.cat`ï¼‰**ã—ã¾ã™ã€‚
2.  çµåˆã•ã‚ŒãŸã‚¹ã‚³ã‚¢å…¨ä½“ã«å¯¾ã—ã¦ **Softmax** ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒã€Œãƒ­ãƒ¼ã‚«ãƒ«ã®è¿‘å‚ã€ã¨ã€Œã™ã¹ã¦ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã€ã®ã©ã¡ã‚‰ã«ã©ã‚Œã ã‘æ³¨æ„ã‚’æ‰•ã†ã‹ã‚’æ±ºå®šã—ã¾ã™ã€‚
3.  ã“ã®é‡ã¿ï¼ˆ`w_local` ã¨ `w_global`ï¼‰ã‚’ä½¿ã£ã¦ã€å¯¾å¿œã™ã‚‹Vï¼ˆ`V_windows` ã¨ `V_global`ï¼‰ã‚’é‡ã¿ä»˜ã‘ã—ã€**ãƒ­ãƒ¼ã‚«ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**ã¨**ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**ã‚’è¨ˆç®—ã—ã€è¶³ã—åˆã‚ã›ã¾ã™ã€‚
4.  ãƒ˜ãƒƒãƒ‰ã‚’çµ±åˆï¼ˆ`merge heads`ï¼‰ã—ã€æœ€çµ‚çš„ãªå‡ºåŠ›å°„å½±ï¼ˆ`out_proj`ï¼‰ã‚’é€šã—ã¾ã™ã€‚

#### D. å¯è¦–åŒ–ã®ãŸã‚ã®ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—

* ã‚³ãƒ¼ãƒ‰ã®æœ€å¾Œã§ã¯ã€è¨ˆç®—ã•ã‚ŒãŸç–çµåˆãªé‡ã¿ã‚’ã€å…¨çµåˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨åŒæ§˜ã®å½¢çŠ¶ `(B, H, T, T)` ã®**`full_attn`**ãƒ†ãƒ³ã‚½ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ç›´ã™å‡¦ç†ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
* ã“ã‚Œã¯ã€å®Ÿéš›ã®è¨ˆç®—ã«ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ãŒã€**ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨æ„ã‚’æ‰•ã£ã¦ã„ã‚‹ã‹**ã‚’å¯è¦–åŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°ã‚„è§£æï¼‰ã™ã‚‹ãŸã‚ã«å½¹ç«‹ã¡ã¾ã™ã€‚

### Big Birdã«ãŠã‘ã‚‹ç–çµåˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä»•çµ„ã¿

Big Birdã®ç–çµåˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã€Œãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€ã¨ã€Œã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€ã«åŠ ãˆã€**ã€Œãƒ©ãƒ³ãƒ€ãƒ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€**ã®3ã¤ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€**å…¨çµåˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ç‰¹æ€§ã‚’ç¶­æŒ**ã—ãªãŒã‚‰ã€è¨ˆç®—è² è·ã‚’ç·šå½¢ã«æŠ‘ãˆã¾ã™ã€‚



ã“ã®å®Ÿè£…ã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã¨ã‚°ãƒ­ãƒ¼ãƒãƒ«ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹é€ ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãŠã‚Šã€ç–çµåˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä¸­æ ¸ã¨ãªã‚‹ä»•çµ„ã¿ã‚’åŠ¹æœçš„ã«å†ç¾ã—ã¦ã„ã¾ã™ã€‚



ä»¥ä¸‹ã§ã¯ **Sparse Attentionï¼ˆç‰¹ã« BigBird / Longformer ç³»ï¼‰ã® Global Attention ã®æœ€å°å®Ÿè£…ä¾‹** ã‚’ç¤ºã—ã¾ã™ã€‚
âœ” **ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆglobal tokensï¼‰ã‚’ä½¿ã†**
âœ” **global token ã¯å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã¨ç›¸äº’æ³¨æ„ã§ãã‚‹**
âœ” **ãã®ä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ local / sliding-window attention ã®ã¿**

ã¨ã„ã†ä»•çµ„ã¿ã§ã™ã€‚

---

# âœ… **Sparse Attention ã® Global Attention ã®æœ€å°å®Ÿè£…ï¼ˆPyTorchï¼‰**

ä»¥ä¸‹ã¯ **local + global attention** ã‚’å‚™ãˆãŸç°¡æ˜“ç‰ˆã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SparseGlobalAttention(nn.Module):
    def __init__(self, dim, num_heads=8, window=4):
        super().__init__()
        assert dim % num_heads == 0
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.window = window
        self.kernel_size = 2*window + 1

        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)

    def forward(self, x, global_mask):
        """
        x: (B,T,D)
        global_mask: (B,T) bool â€” True ã®ä½ç½®ãŒã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³
        """
        B,T,D = x.shape

        # ---- 1) Project to multi-head ----
        q = self.q_proj(x).view(B,T,self.num_heads,self.head_dim).transpose(1,2)  # (B,H,T,Dh)
        k = self.k_proj(x).view(B,T,self.num_heads,self.head_dim).transpose(1,2)
        v = self.v_proj(x).view(B,T,self.num_heads,self.head_dim).transpose(1,2)

        # ============================================================
        # 2) Local Attention: sliding window (å„ã‚¯ã‚¨ãƒªã¯è¿‘å‚ã®ã¿å‚ç…§)
        # ============================================================
        local_scores = torch.zeros(B,self.num_heads,T,self.kernel_size, device=x.device)

        for t in range(T):
            L = max(0, t-self.window)
            R = min(T, t+self.window+1)
            k_local = k[:,:,L:R,:]          # (B,H,win,Dh)
            q_t = q[:,:,t:t+1,:]            # (B,H,1,Dh)
            score = torch.einsum("bhid,bhjd->bhij", q_t, k_local) / (self.head_dim**0.5)
            local_scores[:,:,t,:R-L] = score.squeeze(2)

        # ============================================================
        # 3) Global Attention éƒ¨åˆ†
        # ============================================================
        # global_mask = True ã®ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘å…¨éƒ¨ã«å¯¾ã—ã¦ attentionï¼
        global_scores = []

        for b in range(B):
            idx = torch.nonzero(global_mask[b], as_tuple=False).squeeze(-1)
            if idx.numel() == 0:
                global_scores.append(None)
                continue

            k_g = k[b,:,idx,:]  # (H, G, Dh)
            v_g = v[b,:,idx,:]

            # Q å…¨éƒ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ â†’ global tokens ã®ã‚¹ã‚³ã‚¢
            score = torch.einsum("htd,hgd->htg", q[b], k_g) / (self.head_dim**0.5)
            global_scores.append(score)     # (H,T,G)

        # ============================================================
        # 4) Local + Global ã‚’çµåˆã—ã¦ softmax
        # ============================================================
        out_heads = torch.zeros(B,self.num_heads,T,self.head_dim, device=x.device)

        for b in range(B):
            for h in range(self.num_heads):
                for t in range(T):
                    # local
                    L = max(0, t-self.window)
                    R = min(T, t+self.window+1)
                    local_s = local_scores[b,h,t,:R-L]

                    if global_scores[b] is None:
                        # local only
                        attn = F.softmax(local_s, dim=-1)
                        v_loc = v[b,h,L:R,:]
                        ctx = torch.sum(attn.unsqueeze(-1) * v_loc, dim=-2)
                    else:
                        # local + global
                        g_score = global_scores[b][h,t]   # (G)
                        s = torch.cat([local_s, g_score], dim=-1)   # (win+G)
                        attn = F.softmax(s, dim=-1)

                        # åˆ†å‰²
                        attn_loc = attn[:R-L]
                        attn_g   = attn[R-L:]

                        v_loc = v[b,h,L:R,:]
                        v_g   = v[b,h, global_mask[b], :]

                        ctx = (
                            torch.sum(attn_loc.unsqueeze(-1)*v_loc, dim=-2) +
                            torch.sum(attn_g.unsqueeze(-1)*v_g, dim=-2)
                        )

                    out_heads[b,h,t] = ctx

        # ---- merge heads ----
        out = out_heads.transpose(1,2).reshape(B,T,D)
        out = self.out_proj(out)
        return out
```

---

# âœ… **ã“ã® Sparse Global Attention ã®å®Ÿè£…ãŒå†ç¾ã—ã¦ã„ã‚‹å†…å®¹**

## **â‘  ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆLocal / Sliding Windowï¼‰**

* å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯
  **è¿‘å‚ã® 2Ã—window + 1 å€‹ã® token ã«ã®ã¿ attention**
  â†’ è¨ˆç®—é‡ã‚’ **O(n * window)** ã«å‰Šæ¸›

---

## **â‘¡ ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆGlobal Tokenï¼‰**

global_mask=True ã®ä½ç½®ï¼ˆä¾‹ï¼š
CLS, è¦‹å‡ºã—æ–‡ã€ç‰¹åˆ¥ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã©ï¼‰ã¯â€¦

### **âœ” å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã¨ attention ã§ãã‚‹ï¼ˆGlobal â†’ Allï¼‰**

### **âœ” å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ attention ã‚’å—ã‘ã‚‰ã‚Œã‚‹ï¼ˆAll â†’ Globalï¼‰**

ã“ã‚Œã¯ Longformer / BigBird ãªã©ãŒæ¡ç”¨ã€‚

---

# âœ… **ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã©ã‚“ãªå‹•ä½œã‚’ã™ã‚‹ï¼Ÿ**

### **ğŸ“Œ Global token ã®ç‰¹å¾´**

* å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ç›¸äº’æ³¨æ„
* ã¤ã¾ã‚Š **å¯†ãª attention** ã‚’è¡Œã†
* æ–‡æ›¸å…¨ä½“ã®æƒ…å ±ã‚’é›†ç´„ãƒ»ä¼æ¬ã™ã‚‹å½¹å‰²
* Global token ã®ä¾‹ï¼š

  * **[CLS]**
  * è¦ç´„ã‚¿ã‚¹ã‚¯ã§ã®ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³
  * ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
  * è³ªå•å¿œç­”ã§ã®ã€Œè³ªå•ãƒˆãƒ¼ã‚¯ãƒ³ã€

### **ğŸ“Œ å®Ÿè£…ä¸Šã®å‹•ä½œ**

* global_mask=True ã® token ã¯ k / v ã‚’ç‰¹åˆ¥ã«æŠ½å‡ºã—
  å…¨ Query ãŒãã“ã«æ³¨æ„ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

ã‚³ãƒ¼ãƒ‰ã®è©²å½“ç®‡æ‰€ï¼š

```python
idx = torch.nonzero(global_mask[b], as_tuple=False).squeeze(-1)
k_g = k[b,:,idx,:]
v_g = v[b,:,idx,:]
score = torch.einsum("htd,hgd->htg", q[b], k_g)
```

---

# ã¾ã¨ã‚

| æ©Ÿèƒ½                   | å®Ÿè£…ã§ã®å‡¦ç†                                               |
| -------------------- | ---------------------------------------------------- |
| **Local Attention**  | sliding window ã® k_local / v_local ã‚’ä½¿ç”¨               |
| **Global Attention** | global_mask=True ã® k,v ã‚’æŠ½å‡ºã—ã€å…¨ token ã® Q ã‹ã‚‰ attention |
| **ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³**           | global_mask=True ã® tokenã€‚å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã¨åŒæ–¹å‘ attention ãŒå¯èƒ½     |
| è¨ˆç®—é‡                  | O(n Ã— window + n Ã— G)ï¼ˆG ã¯ global token ã®æ•°ï¼‰           |


