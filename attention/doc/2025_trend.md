LLM（大規模言語モデル）の性能を飛躍的に向上させたTransformerアーキテクチャの核となるのが**アテンション（Attention）機構**です。このアテンション機構には、計算効率の改善、シーケンス長への対応、そしてモデルの表現力向上を目的として、様々な「工夫（改良）」が施されてきました。

LLMで用いられる最近のアテンションの主な工夫点を、目的別に解説します。

---

## 1. 効率性とスケーラビリティの向上 (計算量の削減)

オリジナルのMulti-Head Attentionは、シーケンス長 **$L$** に対して **$O(L^2)$** の計算量がかかるため、長いシーケンス（文脈）を扱う際のボトルネックでした。最近の工夫の多くは、この計算量を削減することに焦点を当てています。

### A. スパース・アテンション (Sparse Attention)

フルアテンション（すべてのトークンが他のすべてのトークンに注目する）ではなく、特定のトークンのみに注目するように制約を設けることで、計算量を **$O(L\sqrt{L})$** や **$O(L \log L)$** に削減します。

* **Patcher/Strided Attention** : 注目範囲を局所的に限定したり、固定の間隔でトークンをスキップしたりすることで、計算量を削減します。
* **BigBird (Google)** : 局所的なアテンション、ランダムなアテンション、そしてグローバルなトークンへのアテンションを組み合わせることで、**情報損失を最小限に抑えつつ**計算量を **$O(L)$** に削減しました。

### B. 線形アテンション (Linear Attention)

アテンションの計算において、行列の掛け算の順序を工夫したり、核関数（Kernel Function）を利用したりすることで、計算量をシーケンス長に比例する **$O(L)$** に削減します。

* **Performer (Google)** : アテンションの計算 **$softmax(QK^T)$** を、**FAVOR (Fast Attention Via Positive Orthogonal Random Features)** という手法で線形化し、計算量を **$O(L^2)$** から **$O(L)$** にします。これは、アテンション行列を明示的に計算しないことが特徴です。

## 2. 位置情報のエンコーディング強化 (長い文脈の理解)

Transformerはアテンションを介してトークン間の関係を捉えますが、トークンの「絶対的な位置」や「相対的な距離」に関する情報が、長いシーケンスでも効果的に機能するように工夫されています。

### A. 相対位置エンコーディング (RPE: Relative Position Encoding)

オリジナルのTransformerは絶対位置エンコーディング（sin/cos関数）を用いていましたが、トークン間の**相対的な距離**をアテンションスコアに組み込むことで、性能が向上します。

* **T5 (Google)** : **$Q$** と **$K$** の類似度計算時に、トークン間の距離に基づくバイアス項を導入します。
* **Transformer-XL (Google/CMU)** : セグメント間の状態（Segment-Level Recurrence）を再利用するとともに、相対位置エンコーディングを導入し、**文脈の切れ目**を越えたアテンションを可能にしました。

### B. 回転式位置エンコーディング (RoPE: Rotary Position Embedding)

LLaMA、GPT-NeoX、Falconなど、多くの最新LLMで採用されている標準的な手法です。

* クエリ（**$Q$**）とキー（**$K$**）ベクトルに回転行列を適用することで、**相対的な位置情報**を組み込みます。
* 計算効率が高く、モデルの線形性（スケーリング特性）を維持しやすいため、**長いシーケンス長への外挿性（Extrapolation）** に優れています。

## 3. モデルの表現力と安定性の向上

### A. マルチクエリ・アテンション (MQA: Multi-Query Attention)

オリジナルのMulti-Head Attentionでは、各ヘッドが **$Q, K, V$** をそれぞれ独自に持ちますが、MQAでは**複数のヘッドが共通の **$K$** と **$V$** を共有**します。

* **目的** : **$K$** と **$V$** の行列サイズを大幅に削減することで、**推論速度を向上**させ、特にバッチサイズが小さい場合の**メモリ帯域幅のボトルネックを解消**します。
* **デメリット** : わずかに精度が低下する場合がありますが、推論時の効率性から多くの最新LLM（例：Falcon 40B）で採用されています。

### B. グループ化されたクエリ・アテンション (GQA: Grouped-Query Attention)

MQAとMHAの中間的なアプローチです。

* 複数のヘッド（**$Q$**）をいくつかのグループに分け、**グループ内のヘッドが共通の **$K$** と **$V$** を共有**します。
* MQAよりも高い精度を維持しつつ、MHAよりも推論時のメモリ効率を向上させます。LLaMA 2やMixtralなどの最新モデルで採用されています。

### C. フラッシュアテンション (FlashAttention)

これはアルゴリズムの工夫というよりも、**計算実装の工夫**です。

* GPUのメモリ階層（SRAMとHBM）を効率的に利用するようにアテンション計算を再構成します。
* アテンションスコアの計算に必要な中間データを、低速なHBM（大容量メモリ）ではなく、高速な**SRAM（オンチップメモリ）** に保持することで、I/Oアクセス時間を劇的に削減します。
* **効果** : 計算速度を大幅に向上させ、メモリ使用量を削減することで、より長いシーケンス長の訓練を可能にしました。

これらの工夫は、LLMがより大規模なデータセットと長い文脈を、より高速かつ効率的に処理できるように進化していることを示しています。



マルチクエリ、クエリアテンションは未だ未実装

フラッシュアテンションもやったことない
