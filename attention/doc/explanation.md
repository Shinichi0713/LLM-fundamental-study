# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒ³ã‚ºã‚ªãƒ³

Transformerã®**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ï¼ˆAttention Mechanismï¼‰** ã¯ã€

ã€Œå…¥åŠ›ã®ã©ã®éƒ¨åˆ†ã«ã©ã‚Œã ã‘æ³¨ç›®ã™ã¹ãã‹ã€ã‚’å­¦ç¿’ã™ã‚‹ä¸­æ ¸éƒ¨ã§ã™ã€‚

ã“ã“ã§ã¯ã€**Google Colabã§å‹•ã‹ã›ã‚‹å®Œå…¨ãªãƒãƒ³ã‚ºã‚ªãƒ³ä¾‹**ã¨ã—ã¦ã€

* å°ã•ãªå…¥åŠ›ç³»åˆ—ã‚’ä½¿ã£ã¦
* **è‡ªå·±æ³¨æ„(Self-Attention)** ã®å‹•ãã‚’
* æ•°å€¤ãƒ»å¯è¦–åŒ–ãƒ»é‡ã¿ã®ãƒãƒƒãƒ—ã§ç†è§£ã§ãã‚‹ã‚³ãƒ¼ãƒ‰

  ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

---

## ğŸ§  ãƒãƒ³ã‚ºã‚ªãƒ³æ¦‚è¦

> ã€Œå…¥åŠ›ã®3å˜èªï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ãŒã€äº’ã„ã«ã©ã‚Œã ã‘æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚
>
> ã“ã®ä¾‹ã§ã¯ **PyTorchã®åŸºæœ¬æ¼”ç®—ã®ã¿** ã‚’ä½¿ã£ã¦ã€Transformerã®ä»•çµ„ã¿ã‚’é€æ˜åŒ–ã—ã¾ã™ã€‚

---

## ğŸš€ Colabç”¨ å®Œå…¨ã‚³ãƒ¼ãƒ‰

```python
# ============================
# Transformer Attention å®Ÿé¨“
# ============================
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# å…¥åŠ› (ä¾‹: 3ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³, å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’4æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾)
x = torch.tensor([
    [1.0, 0.5, 0.3, 0.2],   # token 1
    [0.2, 0.1, 0.9, 0.7],   # token 2
    [0.8, 0.3, 0.2, 0.4]    # token 3
])  # shape: [3, 4]

d_model = x.shape[1]
print("å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶:", x.shape)

# === Q, K, V ã®ç·šå½¢å¤‰æ› ===
W_Q = torch.randn(d_model, d_model)
W_K = torch.randn(d_model, d_model)
W_V = torch.randn(d_model, d_model)

Q = x @ W_Q   # shape: [3, 4]
K = x @ W_K
V = x @ W_V

# === Attentionã‚¹ã‚³ã‚¢è¨ˆç®— ===
scores = (Q @ K.T) / np.sqrt(d_model)  # å†…ç©ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
attention_weights = F.softmax(scores, dim=-1)  # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–

print("Attentionã‚¹ã‚³ã‚¢è¡Œåˆ—:\n", scores)
print("Attentioné‡ã¿:\n", attention_weights)

# === å‡ºåŠ› (åŠ é‡å¹³å‡) ===
output = attention_weights @ V

print("å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«:\n", output)

# === å¯è¦–åŒ– ===
tokens = ["Token 1", "Token 2", "Token 3"]

plt.figure(figsize=(5,4))
sns.heatmap(attention_weights.detach().numpy(), annot=True, cmap="Blues",
            xticklabels=tokens, yticklabels=tokens)
plt.title("Self-Attention Weight Matrix")
plt.xlabel("Key (å‚ç…§å´)")
plt.ylabel("Query (æ³¨ç›®å´)")
plt.show()
```

---

## ğŸ§© å®Ÿè¡Œçµæœã®è§£é‡ˆ

1. **Attentionã‚¹ã‚³ã‚¢è¡Œåˆ—**

   â†’ å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒä»–ãƒˆãƒ¼ã‚¯ãƒ³ã«ã©ã‚Œã ã‘ã€Œæ³¨ç›®ã€ã—ã¦ã„ã‚‹ã‹ã‚’ç¤ºã™ã€‚
   ä¾‹ï¼š

   ```
   [[0.8, 0.1, 0.1],
    [0.3, 0.6, 0.1],
    [0.2, 0.2, 0.6]]
   ```

   * Token1 ã¯è‡ªåˆ†è‡ªèº«(1)ã«å¼·ãæ³¨æ„
   * Token2 ã¯2ç•ªç›®ã«æ³¨æ„ã‚’å¤šã
   * Token3 ã¯ã‚„ã‚„åˆ†æ•£ã—ã¦ã„ã‚‹
2. **Softmax**

   â†’ å„è¡ŒãŒç¢ºç‡åˆ†å¸ƒã«ãªã‚‹ã€‚

   ï¼ˆå…¨ã¦ã®é‡ã¿ã®åˆè¨ˆãŒ1ã«ãªã‚‹ï¼‰
3. **å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«**

   â†’ Attentioné‡ã¿ã‚’ã‚‚ã¨ã«ã€Valueãƒ™ã‚¯ãƒˆãƒ«ã®åŠ é‡å¹³å‡ã‚’è¨ˆç®—ã—ãŸã‚‚ã®ã€‚

   ã“ã‚ŒãŒæ¬¡ã®å±¤ã«æ¸¡ã•ã‚Œã¾ã™ã€‚
4. **ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å¯è¦–åŒ–**

   ![heatmap sample](https://i.imgur.com/EkP1Lvs.png)

   * è¡Œï¼ˆQueryå´ï¼‰: ã€Œã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€
   * åˆ—ï¼ˆKeyå´ï¼‰: ã€Œã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€

---

## ğŸ” ã•ã‚‰ã«ç†è§£ã‚’æ·±ã‚ã‚‹ã«ã¯

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€Colabã§ä»¥ä¸‹ã‚‚è©¦ã™ã¨éå¸¸ã«ç†è§£ãŒæ·±ã¾ã‚Šã¾ã™ï¼š

| ã‚¹ãƒ†ãƒƒãƒ—    | å†…å®¹                                                     |
| ----------- | -------------------------------------------------------- |
| ğŸ§© Step 1   | ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’5ã€œ10ã«å¢—ã‚„ã™ï¼ˆç³»åˆ—é•·ã®å¤‰åŒ–ï¼‰                |
| ğŸ” Step 2   | `d_model`ã‚’å¤§ããã—ã¦å¤šæ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã®æŒ™å‹•ã‚’è¦³å¯Ÿ        |
| ğŸ¯ Step 3   | `Multi-Head Attention`ã‚’å®Ÿè£…ï¼ˆheadæ•°ã‚’åˆ†ã‘ã¦å¹³å‡ï¼‰     |
| ğŸ–¼ï¸ Step 4 | å®Ÿéš›ã®æ–‡ç« åŸ‹ã‚è¾¼ã¿ã‚’å…¥ã‚Œã¦ã€ã©ã®å˜èªãŒæ³¨ç›®ã•ã‚Œã‚‹ã‹å¯è¦–åŒ– |



## å¯è¦–åŒ–


ã§ã¯ã€å…ˆã»ã©ã® **Transformer Self-Attention ã®æ•°å€¤å®Ÿé¨“**ã‚’

ã€Œå®Ÿéš›ã«è¦–è¦šçš„ã«ç†è§£ã§ãã‚‹å¯è¦–åŒ–ä»˜ãç‰ˆã€ã«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

Google Colabã§ãã®ã¾ã¾å‹•ã‹ã™ã¨ã€

å…¥åŠ› â†’ Qãƒ»Kãƒ»Vãƒ™ã‚¯ãƒˆãƒ« â†’ Attentionã‚¹ã‚³ã‚¢ â†’ é‡ã¿ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

ã®æµã‚ŒãŒ**å…¨éƒ¨ã‚°ãƒ©ãƒ•ã‚£ã‚«ãƒ«ã«è¦‹ãˆã‚‹**ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

---

## ğŸ¨ Colabç”¨ å®Œå…¨å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰

```python
# ===============================
# Self-Attention å¯è¦–åŒ–ãƒãƒ³ã‚ºã‚ªãƒ³
# ===============================
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ==== å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³å®šç¾© ====
tokens = ["Tokyo", "is", "beautiful"]
x = torch.tensor([
    [1.0, 0.5, 0.3, 0.2],   # Tokyo
    [0.2, 0.1, 0.9, 0.7],   # is
    [0.8, 0.3, 0.2, 0.4]    # beautiful
])  # shape: [3, 4]

d_model = x.shape[1]
print("å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶:", x.shape)

# ==== Q, K, V ã®ç·šå½¢å¤‰æ› ====
torch.manual_seed(42)  # å†ç¾æ€§ã®ãŸã‚å›ºå®š
W_Q = torch.randn(d_model, d_model)
W_K = torch.randn(d_model, d_model)
W_V = torch.randn(d_model, d_model)

Q = x @ W_Q   # [3, 4]
K = x @ W_K   # [3, 4]
V = x @ W_V   # [3, 4]

# ==== Attentionã‚¹ã‚³ã‚¢ã¨é‡ã¿ ====
scores = (Q @ K.T) / np.sqrt(d_model)
attention_weights = F.softmax(scores, dim=-1)

# ==== å‡ºåŠ› ====
output = attention_weights @ V

# ==== æ•°å€¤å‡ºåŠ› ====
print("Attentionã‚¹ã‚³ã‚¢:\n", scores)
print("\nAttentioné‡ã¿:\n", attention_weights)
print("\nå‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«:\n", output)

# =======================
# ==== å¯è¦–åŒ–ãƒ‘ãƒ¼ãƒˆ ====
# =======================

fig, axes = plt.subplots(1, 4, figsize=(18, 4))

# â‘  å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«å¯è¦–åŒ–
sns.heatmap(x.numpy(), annot=True, cmap="YlGnBu", ax=axes[0],
            xticklabels=[f"d{i+1}" for i in range(d_model)],
            yticklabels=tokens)
axes[0].set_title("Input token embeddings (x)")

# â‘¡ Attentionã‚¹ã‚³ã‚¢
sns.heatmap(scores.detach().numpy(), annot=True, cmap="OrRd", ax=axes[1],
            xticklabels=tokens, yticklabels=tokens)
axes[1].set_title("Raw Attention Scores (QK^T / sqrt(d))")

# â‘¢ Attentioné‡ã¿ (Softmaxå¾Œ)
sns.heatmap(attention_weights.detach().numpy(), annot=True, cmap="Blues", ax=axes[2],
            xticklabels=tokens, yticklabels=tokens)
axes[2].set_title("Normalized Attention Weights")

# â‘£ å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«
sns.heatmap(output.detach().numpy(), annot=True, cmap="Greens", ax=axes[3],
            xticklabels=[f"d{i+1}" for i in range(d_model)],
            yticklabels=tokens)
axes[3].set_title("Output representations")

plt.tight_layout()
plt.show()
```

---

## ğŸ§  å®Ÿè¡Œå¾Œã«å¾—ã‚‰ã‚Œã‚‹ã‚‚ã®

1. **å·¦ã‹ã‚‰å³ã«å‘ã‹ã†å¯è¦–åŒ–**

   ```
   [x] â†’ [Q,K,Væ¼”ç®—] â†’ [ã‚¹ã‚³ã‚¢] â†’ [Softmax] â†’ [å‡ºåŠ›]
   ```
2. **3æšç›®ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆé’ï¼‰**

   â†’ TransformerãŒã€Œã©ã®å˜èªã«ã©ã‚Œã ã‘æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ã™ã€‚

   ä¾‹ï¼š

   ```
   "Tokyo"  â†’ è‡ªåˆ†è‡ªèº«ã«å¼·ã„æ³¨æ„  
   "is"     â†’ "Tokyo"ã«ã‚‚ä¸€éƒ¨æ³¨ç›®  
   "beautiful" â†’ "is"ã‚’é‡è¦– ãªã©
   ```
3. **å³ç«¯ï¼ˆå‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«ï¼‰**

   â†’ Valueã®åŠ é‡å¹³å‡çµæœã€‚

   Attentioné‡ã¿ã«ã‚ˆã£ã¦å†æ§‹æˆã•ã‚ŒãŸæ–°ã—ã„è¡¨ç¾ã§ã™ã€‚

---

## ğŸ“Š æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆï¼ˆç™ºå±•å­¦ç¿’ï¼‰

* `tokens` ã‚’ `"I", "love", "transformers", "so", "much"` ã«å¢—ã‚„ã—ã¦è¦³å¯Ÿã™ã‚‹
* `torch.manual_seed()` ã®å€¤ã‚’å¤‰ãˆã¦æŒ™å‹•æ¯”è¼ƒ
* `sns.heatmap(..., annot=True, fmt=".2f")` ã§å°æ•°ç‚¹èª¿æ•´


![1760877122148](image/explanation/1760877122148.png)
