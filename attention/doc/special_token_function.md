## 特殊トークンの機能

「トークンが `[CLS]` や `[SEP]` にばかり注目する」——

これは **BERT系モデルを観察したときによく見られる現象** で、モデル内部の「役割分担」や「情報統合の仕組み」を理解するうえで非常に重要です。

以下で、理由・状態・対処の3段階で丁寧に解説します👇

---

## 🧩 1. まず現象の意味

BERTなどのモデルでは、入力文は次のようにトークン化されます：

```
[CLS] The dog chased the cat . [SEP]
```

もしアテンションマップを見ると、

* 多くのトークン（例：`dog`, `cat`, `chased`）が **[CLS] に強い注意を向けている**
* ある層ではほとんどのヘッドが **[SEP] にも注目している**

こうなると一見「文の情報を見ていない」「おかしい」と思うかもしれませんが、

実はこれは**正常な挙動**です。

---

## 🧠 2. その理由：モデル内での「役割分担」

### 🟢 [CLS] トークン

* `[CLS]` は「文全体の代表（summary token）」として訓練されます。
* 特に  **分類タスク** （例：感情分析）では、出力層が `[CLS]` の埋め込みだけを使って予測します。
* そのため、モデルは次のような「情報集約ネットワーク」を学習します：

  ```
  各単語 → [CLS] に情報を集約
  ```

  結果として、下層のトークンは `[CLS]` をよく参照し、

  `[CLS]` は逆に全トークンを参照して文全体を要約するようになります。

---

### 🟣 [SEP] トークン

* `[SEP]` は「文の終わり」または「文と文の区切り」を示すマーカー。
* BERTは文対入力（例：「質問」「回答」）の学習をしており、

  どこまでが1文目・2文目かを区別する必要があります。
* そのため、**文境界を意識する層（中層～高層）** では `[SEP]` に注目することが多いです。

---

## 🔬 3. つまり「CLS/SEPにばかり反応する」とは？

| 状態                                    | 意味                     | 学習上の示唆                                                 |
| --------------------------------------- | ------------------------ | ------------------------------------------------------------ |
| **下層でCLS集中**                 | 文全体の集約を始めている | 正常。文意形成が始まる層                                     |
| **中層でSEP集中**                 | 文の構造を意識している   | 文境界の理解。2文タスクで特に重要                            |
| **全層・全ヘッドがCLS/SEPに集中** | 他トークンを無視している | 🔴**過学習**や**入力文の多様性不足**の可能性あり |

---

## 🧭 4. 実際の可視化例（BERTVizなどで見ると）

例文：「The dog chased the cat.」

* **Layer 1–3:** 対角構造（自分・近傍を見る）→ローカル構文理解
* **Layer 4–8:** `[CLS]` に強い集中 → 情報を集約する中間層
* **Layer 9–12:** `[CLS]` から他トークンへ注目 → 文全体の出力準備層

---

## 🧩 5. どう活かすか？

* `[CLS]` に集まる層を使えば「文の代表ベクトル」として使いやすい。
* `[SEP]` 注目層を見ると「文の切れ目」や「文間関係」を把握できる。
* 逆に、**全層でCLS/SEPばかりを見るモデルは「情報流通が詰まっている」**可能性があります。

  → fine-tuningでの層凍結や再初期化で改善する場合もあります。

---

## 🔍 まとめ

| トークン        | モデルでの役割               | 注意が集中する理由               |
| --------------- | ---------------------------- | -------------------------------- |
| **[CLS]** | 文全体の要約・出力用ベクトル | 他トークンの情報を集約するため   |
| **[SEP]** | 文の区切り・境界の指標       | 構文・文脈の切れ目を明示するため |

---

はい、スパースアテンション（疎な注意機構）の主要な種類と、それぞれの特徴についてご説明します。

スパースアテンションは、従来の完全なアテンション（Dense Attention, $O(N^2)$）の計算量を、シーケンス長 $N$ に対して線形に近いレベルに削減することを目的としています。

---

## 💡 スパースアテンションの主要な種類と特徴

スパースアテンションの分類は、**どのように注意の接続を制限するか（スパース化のパターン）**によって決まります。

### 1. 固定された窓ベースの疎化 (Fixed or Window-Based Sparsity)

注意を払う範囲を物理的な距離で固定する方法です。シンプルで実装が容易です。

| 種類 | 原理・特徴 | メリット/デメリット |
| :--- | :--- | :--- |
| **ローカルアテンション (Local Attention)**  | 各トークンが、自身とその**周囲の固定された窓（ウィンドウ）**内のトークンにのみ注目します。窓の外のトークンとの関係は計算しません。 | ✅ 計算量が $O(N \cdot W)$（$W$は窓サイズ）となり、線形に近い。/ ❌ 窓の外にある長距離の依存関係は捉えられない。|
| **ダイレーテッドアテンション (Dilated Attention)** | ローカルアテンションと似ていますが、窓内のトークンを**一定の間隔（ダイレーション率）で飛び飛びにサンプリング**して注目します。 | ✅ 窓サイズを変えずに、長距離のトークンにも効率的に注意を払える。/ ❌ 中間距離の文脈が欠落する可能性がある。|

---

### 2. グローバルとローカルの混合型疎化 (Mixed Attention)

局所的な文脈と、全体の重要な情報を同時に捉えるために、特定のトークンを優遇する手法です。

| 種類 | 原理・特徴 | メリット/デメリット |
| :--- | :--- | :--- |
| **グローバルアテンション (Global Attention)**  | 全てのトークンに注目する**「グローバルトークン」**（例：`[CLS]`トークンや、固定位置のトークン）を少数指定します。その他のトークンはローカルな範囲、またはグローバルトークンにのみ注目します。 | ✅ グローバルトークンが全体の文脈を要約することで、長距離の依存関係を効率的に捕捉できる。/ ❌ グローバルトークンの選定が重要で、その選定ミスが性能に影響を与える。|

---

### 3. データ依存型・適応型疎化 (Data-Dependent or Adaptive Sparsity)

入力データの内容に基づいて、どの接続に注目すべきかを動的に決定する高度な手法です。

| 種類 | 原理・特徴 | メリット/デメリット |
| :--- | :--- | :--- |
| **学習型疎化 (Learned Sparsity)** | トレーニング中に**ゲーティングメカニズム**などを使い、入力ごとに注意スコアの中で重要度の低い接続を動的にゼロにする（プルーニングする）ことで、スパースなパターンを学習します。 | ✅ データに応じて柔軟に重要な接続を選べるため、効率と精度の両立が期待できる。/ ❌ トレーニングが複雑化し、オーバーヘッドが増える可能性がある。|
| **クエリ/キー クラスタリング (LSH Attention)** | **LSH (Locality-Sensitive Hashing)** などの技術を使って、**類似したクエリ ($\mathbf{Q}$) とキー ($\mathbf{K}$)** をクラスター化します。注意計算はこの類似したクラスター内でのみ行われます。 | ✅ 計算量が $O(N \log N)$ などに削減される。/ ❌ クラスタリングの精度がモデルの性能に直結する。|

---

### まとめ

| スパース化の目的 | 該当する主な種類 | 計算量の目安 |
| :--- | :--- | :--- |
| **近接性の維持** | ローカルアテンション | $O(N \cdot W)$ |
| **長距離の捕捉** | グローバルアテンション、ダイレーテッドアテンション | $O(N \cdot \sqrt{N})$ など |
| **精度と効率の両立** | LSHアテンション、学習型疎化 | $O(N \log N)$ など |

どの種類のスパースアテンションを採用するかは、処理したいシーケンスの長さ、タスクの性質（局所的な情報が重要か、全体的な文脈が重要か）、そして利用可能な計算資源によって決定されます。
