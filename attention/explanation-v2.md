ã„ã„ã§ã™ã­ â€” ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æŒ¯ã‚‹èˆã„ã‚’**è¦‹ã¦ã€è§¦ã£ã¦ã€å®Ÿé¨“ã™ã‚‹**ã®ã¯ LLMï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒï¼‰ã‚’ç†è§£ã™ã‚‹æœ€ã‚‚è‰¯ã„æ–¹æ³•ã®ä¸€ã¤ã§ã™ã€‚ä»¥ä¸‹ã¯ **å®Ÿè·µçš„ã§å†ç¾å¯èƒ½ãªæ‰‹é †ï¼‹ã‚³ãƒ¼ãƒ‰ï¼‹è§£èª¬** ã‚’ä¸€å¼ã§ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚‚ Colab ã§ã‚‚å‹•ãã‚ˆã†ã«æ›¸ã„ã¦ã‚ã‚Šã¾ã™ã€‚ã¾ãšã¯å°ã•ã‚ã®ãƒ¢ãƒ‡ãƒ«ã§å§‹ã‚ã€æ…£ã‚ŒãŸã‚‰å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã§è©¦ã—ã¦ãã ã•ã„ã€‚

---

# æº–å‚™ï¼ˆå¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰

ãƒ­ãƒ¼ã‚«ãƒ«ã‚„ Colab ã® Python ç’°å¢ƒã§ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```bash
pip install transformers torch matplotlib seaborn tokenizers
# å¯¾è©±çš„ã«å¯è¦–åŒ–ã—ãŸã‘ã‚Œã°ï¼ˆä»»æ„ï¼‰
pip install bertviz
```

---

# 1) ã‚„ã£ã¦ã¿ã‚‹ â€” BERT/GPT ç³»ã§ attention ã‚’æŠ½å‡ºã—ã¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º

ä»¥ä¸‹ã¯ Hugging Face `transformers` ã‚’ä½¿ã£ãŸæœ€å°é™ã®ä¾‹ã§ã™ï¼ˆ`bert-base-uncased` ã‚’ä¾‹ã«ï¼‰ã€‚`output_attentions=True` ã§ attention ã‚’å–å¾—ã—ã€å„ layerãƒ»head ã® attention è¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚

```python
# attention_viz.py
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer, AutoModel

def plot_attention_matrix(attn, tokens, title="", vmax=None, save=None):
    """
    attn: (seq_len, seq_len) numpy array
    tokens: list of token strings (len seq_len)
    """
    plt.figure(figsize=(6,6))
    sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens,
                square=True, cbar=True, vmax=vmax)
    plt.title(title)
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    if save:
        plt.savefig(save, dpi=200)
    plt.show()

def visualize_attention(model_name="bert-base-uncased", text="The quick brown fox jumps over the lazy dog"):
    # Load
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name, output_attentions=True)
    model.eval()

    # Tokenize
    inputs = tokenizer(text, return_tensors="pt")
    input_ids = inputs["input_ids"]  # (1, seq_len)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

    # Forward (get attentions)
    with torch.no_grad():
        outputs = model(**inputs)
    # outputs.attentions: tuple of length num_layers, each (batch, num_heads, seq_len, seq_len)
    attentions = outputs.attentions

    num_layers = len(attentions)
    num_heads = attentions[0].shape[1]
    seq_len = attentions[0].shape[-1]

    print(f"Model: {model_name}, layers: {num_layers}, heads: {num_heads}, seq_len: {seq_len}")
    print("Tokens:", tokens)

    # Show a few heads (first layer, each head)
    # You can iterate layers/heads as you like
    for layer in range(min(4, num_layers)):  # show first 4 layers as demo
        for head in range(num_heads):
            mat = attentions[layer][0, head].cpu().numpy()  # (seq_len, seq_len)
            # optional normalization: already softmaxed in model -> sums to 1 across last dim
            plot_attention_matrix(mat, tokens, title=f"Layer {layer+1} Head {head+1}")

if __name__ == "__main__":
    visualize_attention()
```

**å®Ÿè¡Œæ–¹æ³•**

```bash
python attention_viz.py
```

ï¼ˆColab ãªã‚‰ã‚»ãƒ«å®Ÿè¡Œã§ãƒ—ãƒ­ãƒƒãƒˆãŒå‡ºã¾ã™ï¼‰

---

# 2) ä½•ã‚’è¦‹ã‚Œã°è‰¯ã„ã‹ï¼ˆè§£é‡ˆã®ãƒ’ãƒ³ãƒˆï¼‰

* è¡Œåˆ—ã®**è¡Œ i**ã¯ã€Œã‚¯ã‚¨ãƒªãŒãƒˆãƒ¼ã‚¯ãƒ³ i ã®ã¨ãã€ã©ã®ã‚­ãƒ¼ã«é‡ã¿ã‚’æ‰•ã£ã¦ã„ã‚‹ã‹ï¼ˆã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ï¼‰ã€ã‚’ç¤ºã—ã¾ã™ã€‚
* ä¸»ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼š
  * **å¯¾è§’å„ªå‹¢ï¼ˆdiagonalï¼‰** ï¼šãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸»ã«è‡ªåˆ†è‡ªèº«/è¿‘å‚ã«æ³¨ç›® â†’ å±€æ‰€çš„å‡¦ç†ï¼ˆè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ˆãè¦‹ã‚‹ï¼‰
  * **CLS / [EOS] ã«é›†ä¸­** ï¼šè¦ç´„ãƒ»æ–‡å…¨ä½“ã®æ–‡è„ˆå–å¾—ï¼ˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã®ç‰¹å¾´ï¼‰
  * **å¥æ§‹é€ ã«æ²¿ã£ãŸæ³¨ç›®** ï¼šå‹•è©ãŒç›®çš„èªã‚’å¼·ãå‚ç…§ã€å½¢å®¹è©ãŒè¢«ä¿®é£¾èªã‚’å‚ç…§ã€ãªã©ï¼ˆæ„å‘³çš„é–¢ä¿‚ï¼‰
  * **ãƒ˜ãƒƒãƒ‰é–“ã®åˆ†æ¥­** ï¼šã‚ã‚‹ãƒ˜ãƒƒãƒ‰ã¯å±€æ‰€ã€åˆ¥ãƒ˜ãƒƒãƒ‰ã¯é•·è·é›¢é–¢ä¿‚ã‚’æ‹¾ã†ã“ã¨ãŒã‚ã‚‹

---

# 3) è¦‹ã‚„ã™ãã¾ã¨ã‚ã¦è¡¨ç¤ºï¼ˆã‚°ãƒªãƒƒãƒ‰ã§å„ãƒ¬ã‚¤ãƒ¤xãƒ˜ãƒƒãƒ‰ã‚’ä¸€æ°—ã«è¡¨ç¤ºï¼‰

å¤§é‡ã®å›³ã‚’æ‰‹ã§è¦‹ã‚‹ã®ã¯å¤§å¤‰ãªã®ã§ã€1ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼å†…ã®å…¨ãƒ˜ãƒƒãƒ‰ã‚’ã‚°ãƒªãƒƒãƒ‰åŒ–ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚‚ç”¨æ„ã—ã¾ã™ã€‚

```python
def plot_layer_heads(attentions, tokens, layer, vmax=None, cols=8, save=None):
    """
    attentions: outputs.attentions (tuple)
    layer: zero-based layer index
    """
    mat_layer = attentions[layer][0].cpu().numpy()  # (num_heads, seq_len, seq_len)
    num_heads = mat_layer.shape[0]
    rows = (num_heads + cols - 1) // cols
    fig, axes = plt.subplots(rows, cols, figsize=(cols*2.5, rows*2.5))
    axes = axes.flatten()
    for h in range(len(axes)):
        ax = axes[h]
        if h < num_heads:
            sns.heatmap(mat_layer[h], ax=ax, cbar=False, vmax=vmax)
            ax.set_title(f"H{h}")
            ax.set_xticks([])
            ax.set_yticks([])
        else:
            ax.axis('off')
    fig.suptitle(f"Layer {layer+1} Heads")
    if save:
        fig.savefig(save, dpi=200)
    plt.show()
```

å‘¼ã³å‡ºã—ä¾‹ï¼š

```python
# after outputs = model(...)
plot_layer_heads(outputs.attentions, tokens, layer=0, cols=8)
```

---

# 4) ã‚ˆã‚Šã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«è¦‹ã‚‹ãªã‚‰ï¼š`bertviz`

`bertviz` ã¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ã®æ³¨ç›®ã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«æ¢ç´¢ã§ãã¾ã™ï¼ˆJupyter/Colabå‘ã‘ï¼‰ã€‚

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸã‚‰ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç°¡å˜ã«å¯è¦–åŒ–ã§ãã¾ã™ï¼ˆå…¬å¼ README ã‚’å‚ç…§ï¼‰ï¼š

```python
from bertviz import head_view
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)
text = "The quick brown fox jumps over the lazy dog"
inputs = tokenizer.encode_plus(text, return_tensors='pt')
outputs = model(**inputs)
attention = [att.cpu().numpy() for att in outputs.attentions]
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
head_view(attention, tokens)
```

ï¼ˆColab ã‚„ Jupyter ãƒãƒ¼ãƒˆã§å‹•ã‹ã™ã¨ãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«æ¢ã›ã¾ã™ï¼‰

---

# 5) å®Ÿé¨“ã‚¢ã‚¤ãƒ‡ã‚¢ï¼ˆå­¦ç¿’ã®ãŸã‚ã®ãƒãƒ³ã‚ºã‚ªãƒ³ï¼‰

1. **éƒ¨åˆ†èªï¼ˆsubwordï¼‰ã‚’å«ã‚€å˜èªã§æŒ™å‹•ã‚’è¦‹ã‚‹** ï¼šBPE ã«åˆ†ã‹ã‚Œã‚‹å˜èªã§ attention ãŒã©ã†åˆ†é…ã•ã‚Œã‚‹ã‹ç¢ºèªã€‚
2. **æ–‡ã‚’ã‚ã–ã¨é•·ãã™ã‚‹** ï¼šé•·è·é›¢ä¾å­˜ã‚’ã©ã®ãƒ¬ã‚¤ãƒ¤ã§æ‹¾ã†ã‹ã€‚
3. **ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥ã‚Œæ›¿ãˆã‚‹ï¼ˆèªé †ã‚’å¤‰ãˆã‚‹ï¼‰** ï¼šä¸€éƒ¨ãƒ˜ãƒƒãƒ‰ãŒèªé †ã«æ•æ„Ÿã‹ã‚’ç¢ºèªã€‚
4. **ãƒã‚¹ã‚¯ã—ãŸã¨ãã®æŒ™å‹•** ï¼ˆãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¨ã€**ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è‡ªå·±æ³¨æ„**ã§ã®é•ã„ã‚’æ¯”è¼ƒã€‚
5. **å¹³å‡åŒ– vs ç‰¹å®šãƒ˜ãƒƒãƒ‰** ï¼šå…¨ãƒ˜ãƒƒãƒ‰ã‚’å¹³å‡åŒ–ã—ã¦å‡ºã‚‹æ³¨æ„ã¨ã€ç‰¹å®šãƒ˜ãƒƒãƒ‰ã®æ³¨æ„ã‚’æ¯”è¼ƒã—ã¦ã€Œã©ã®ãƒ˜ãƒƒãƒ‰ãŒé‡è¦ã‹ã€ã‚’è©•ä¾¡ã€‚
6. **Attention rollout / attribution** ï¼šå±¤ã‚’ã¾ãŸã„ã é›†ç´„ï¼ˆå…¥åŠ›â†’å‡ºåŠ›ã«ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¯„ä¸ã—ãŸã‹ï¼‰ã‚’è¨ˆç®—ã—ã¦å¯è¦–åŒ–ã™ã‚‹ï¼ˆä¸‹è¨˜ã«ç°¡å˜ãªå®Ÿè£…æ¡ˆï¼‰ã€‚

---

# 6) Attention Rolloutï¼ˆç°¡æ˜“ç‰ˆï¼‰

å±¤ã‚’ã¾ãŸã„ã§æ³¨æ„ã‚’å¤šæ®µçš„ã«åˆæˆã—ã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ€çµ‚çš„ã«ã©ã‚Œã ã‘å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã™ã‚‹æ‰‹æ³•ï¼ˆå‚è€ƒå®Ÿè£…ã‚’ç°¡ç•¥åŒ–ï¼‰ï¼š

```python
def attention_rollout(attentions, start_layer=0, discard_ratio=0.0):
    # attentions: tuple of (batch, heads, seq, seq). We'll average over heads.
    num_layers = len(attentions)
    result = np.eye(attentions[0].shape[-1])
    for i in range(start_layer, num_layers):
        attn = attentions[i][0].mean(axis=0)  # avg over heads -> (seq, seq)
        # Optionally zero small weights
        if discard_ratio:
            flat = attn.flatten()
            cutoff = np.quantile(flat, discard_ratio)
            attn = np.where(attn < cutoff, 0, attn)
        attn = attn / attn.sum(axis=-1, keepdims=True)  # re-normalize
        result = attn @ result
    return result  # (seq, seq)

# usage:
roll = attention_rollout([a.cpu().numpy() for a in outputs.attentions])
plot_attention_matrix(roll, tokens, title="Attention Rollout (avg heads)")
```

---

# 7) æ³¨æ„ç‚¹ãƒ»è½ã¨ã—ç©´ï¼ˆInterpretabilityï¼‰

* **Attention â‰  Explanation** ï¼šAttention ãŒé«˜ã„ã‹ã‚‰å¿…ãšé‡è¦ã¨ã„ã†å˜ç´”ãªè§£é‡ˆã¯å±é™ºã§ã™ã€‚ç ”ç©¶ã§ã¯ attention ã®é‡è¦åº¦ã¯å ´åˆã«ã‚ˆã‚Šé™å®šçš„ã¨ã•ã‚Œã¦ã„ã¾ã™ï¼ˆãŸã ã—ç›´æ„Ÿçš„ãªæ‰‹æ›ã‹ã‚Šã«ã¯ãªã‚‹ï¼‰ã€‚
* **Softmax ã®æ€§è³ª** ï¼šæ³¨æ„é‡ã¿ã¯æ­£è¦åŒ–ã•ã‚Œã‚‹ã®ã§ã€1ç®‡æ‰€ã«æ¥µç«¯ã«é›†ä¸­ã™ã‚‹ã‹ã€åˆ†æ•£ã™ã‚‹ã‹ã§è§£é‡ˆãŒå¤‰ã‚ã‚Šã¾ã™ã€‚
* **ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®å½±éŸ¿** ï¼šã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ãŒè¦–è¦šçš„ã«â€œåˆ†è£‚â€ã—ã¦è¦‹ãˆã‚‹ãŸã‚ã€å˜èªå˜ä½ã§ã¾ã¨ã‚ã¦è¦‹ã‚‹ã¨ã‚ã‹ã‚Šã‚„ã™ã„ã€‚
* **ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å·®** ï¼šBERTï¼ˆãƒã‚¹ã‚¯ï¼‰ã¨ GPTï¼ˆè‡ªå·±å›å¸°ï¼‰ã§ã¯ attention ã®å½¹å‰²ãŒç•°ãªã‚‹ï¼ˆBERT ã¯åŒæ–¹å‘æ–‡è„ˆã€GPT ã¯å·¦ã‹ã‚‰å³ã¸ã®å› æœæ³¨æ„ï¼‰ã€‚

---

# 8) å®Ÿè·µãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãŠã™ã™ã‚ï¼‰

1. Colab / ãƒ­ãƒ¼ã‚«ãƒ«ã§ä¸Šã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‹•ã‹ã—ã¦ã€çŸ­æ–‡ã§å…¨ãƒ˜ãƒƒãƒ‰ãƒ»å…¨ãƒ¬ã‚¤ãƒ¤ã‚’ç¢ºèªã€‚
2. `bertviz` ã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«æ¢ç´¢ã€‚
3. èˆˆå‘³ãŒæ¹§ã„ãŸã‚‰ attention-rolloutã€ãƒ˜ãƒƒãƒ‰é™¤å»å®Ÿé¨“ï¼ˆç‰¹å®šãƒ˜ãƒƒãƒ‰ã‚’ã‚¼ãƒ­ã«ã—ã¦ä¸‹æµã‚¿ã‚¹ã‚¯æ€§èƒ½ã‚’è¦‹ã‚‹ï¼‰ã‚’è©¦ã™ã€‚
4. æœ€å¾Œã«ã€Œãªãœãã®ãƒ˜ãƒƒãƒ‰ãŒé‡è¦ã‹ï¼Ÿã€ã‚’è¨€èªå­¦çš„ã«è§£é‡ˆã—ã¦ã¿ã‚‹ï¼ˆæ§‹æ–‡ã€ä¿®é£¾é–¢ä¿‚ãªã©ï¼‰ã€‚



# BertViz

BERT ã® **Attentionï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰ã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã®å®šç•ªãƒ„ãƒ¼ãƒ«ãŒ [BertViz](https://github.com/jessevig/bertviz)** ã§ã™ã€‚

ä»¥ä¸‹ã«ã€**Google Colab ã§ãã®ã¾ã¾å‹•ã‹ã›ã‚‹ã‚»ãƒ«ä¾‹**ã‚’ã‚»ãƒƒãƒˆã§ç¤ºã—ã¾ã™ğŸ‘‡

---

## ğŸ¯ ç›®çš„

BERT ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¯è¦–åŒ–ã—ã¦ã€ã€Œã©ã®å˜èªãŒã©ã®å˜èªã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€ã‚’ç›´æ„Ÿçš„ã«å­¦ã¶ã€‚

---

## âœ… æ‰‹é †æ§‹æˆ

1ï¸âƒ£ ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

2ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿

3ï¸âƒ£ ã‚µãƒ³ãƒ—ãƒ«æ–‡ã®è¨­å®š

4ï¸âƒ£ BertViz ã®å¯è¦–åŒ–

---

### ğŸ§© Colab ã‚»ãƒ«ä¾‹ï¼ˆã‚³ãƒ”ãƒ¼ã—ã¦é †ã«å®Ÿè¡Œã§ãã¾ã™ï¼‰

#### âœ… ã‚»ãƒ«1ï¼šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```python
!pip install transformers==4.44.0 bertviz==1.4.0 torch
```

#### âœ… ã‚»ãƒ«2ï¼šãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿

```python
from transformers import BertTokenizer, BertModel
from bertviz import head_view, model_view
import torch
```

#### âœ… ã‚»ãƒ«3ï¼šãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿

```python
# è‹±èªãƒ¢ãƒ‡ãƒ«ã‚’ä¾‹ã«ï¼ˆæ—¥æœ¬èªç‰ˆã«å¤‰ãˆã‚‹ã“ã¨ã‚‚å¯èƒ½ï¼‰
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)
```

#### âœ… ã‚»ãƒ«4ï¼šå…¥åŠ›æ–‡ã®è¨­å®š

```python
sentence_a = "The cat sat on the mat"
sentence_b = "The cat lay on the rug"

inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)
input_ids = inputs['input_ids']
attention = model(**inputs).attentions
```

#### âœ… ã‚»ãƒ«5ï¼šBertVizã«ã‚ˆã‚‹Head Viewï¼ˆå˜èªã”ã¨ã®æ³¨ç›®ã‚’å¯è¦–åŒ–ï¼‰

```python
# å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ãŒã©ã®å˜èªã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–
tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
head_view(attention, tokens)
```

#### âœ… ã‚»ãƒ«6ï¼šBertVizã«ã‚ˆã‚‹Model Viewï¼ˆå±¤å…¨ä½“ã®é–¢ä¿‚ã‚’ä¿¯ç°ï¼‰

```python
# å„å±¤ãƒ»ãƒ˜ãƒƒãƒ‰å…¨ä½“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ§‹é€ ã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«è¡¨ç¤º
model_view(attention, tokens)
```

---

## ğŸ’¡ è£œè¶³

* Colab ã§å®Ÿè¡Œå¾Œã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ãŒé–‹ãã¾ã™ã€‚

  ï¼ˆ`head_view` ã¯å˜èªé–“ã®æ³¨ç›®ç·šã‚’è¡¨ç¤ºã€`model_view` ã¯å±¤ã¨ãƒ˜ãƒƒãƒ‰ã®å…¨ä½“æ§‹é€ ï¼‰
* æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ãŸã„å ´åˆï¼š

  ```python
  model_name = "cl-tohoku/bert-base-japanese-v2"
  ```

---

## ğŸš€ å¿œç”¨ä¾‹

* å…¥åŠ›æ–‡ã‚’å¤‰ãˆã¦æ¯”è¼ƒï¼ˆä¾‹ï¼šã€ŒçŠ¬ãŒèµ°ã‚‹ã€ã€ŒçŒ«ãŒæ­©ãã€ãªã©ï¼‰
* å„å±¤ãƒ»ãƒ˜ãƒƒãƒ‰ãŒã©ã®å˜èªã‚’ã©ã®ç¨‹åº¦æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã‚’è¦³å¯Ÿã—ã€

  BERTã®å†…éƒ¨æ§‹é€ ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã§ãã¾ã™ã€‚



# LLMã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç†è§£ã®ãƒ†ãƒ¼ãƒ

ã§ã¯ã€æ¬¡ã®ä¸­ã‹ã‚‰ã©ã®æ–¹å‘ã§æ˜ã‚Šä¸‹ã’ãŸã„ã‹æ•™ãˆã¦ãã ã•ã„ã€‚

ã©ã‚Œã‚‚ **ã€ŒLLMã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã™ã‚‹ã€ãŸã‚ã®å®Ÿé¨“ãƒ†ãƒ¼ãƒ**ã§ã™ğŸ‘‡

---

### ğŸ§­ é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼

#### ğŸŸ¢ **â‘  æ—¥æœ¬èªBERTã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¯è¦–åŒ–**

* ãƒ¢ãƒ‡ãƒ«ï¼š`cl-tohoku/bert-base-japanese-v2`
* æ–‡ä¾‹ï¼šã€ŒçŠ¬ãŒèµ°ã‚‹ã€ã¨ã€ŒçŒ«ãŒæ­©ãã€ãªã©
* Colabã§æ—¥æœ¬èªã®Attentionç·šã‚’å¯è¦–åŒ–

  ğŸ§© â†’ *è‹±èªä¾‹ã¨ã»ã¼åŒã˜æ‰‹é †ã§å‹•ä½œ*

---

#### ğŸ”µ **â‘¡ Attentionã®é‡ã¿ã‚’æ•°å€¤ã§åˆ†æ**

* ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã‚’**æ•°å€¤ã¨ã—ã¦æŠ½å‡ºãƒ»æç”»**
* ã€Œã©ã®å˜èªãŒã©ã‚Œã«æœ€ã‚‚æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ã«ã—ã¦ç¢ºèª

  ğŸ“Š â†’ *head_viewã®è£å´ã‚’æ•°å€¤ã§ç†è§£ã™ã‚‹*

---

#### ğŸŸ£ **â‘¢ GPTç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¯è¦–åŒ–**

* OpenAIç³»ãƒ¢ãƒ‡ãƒ«ã‚„LLaMAãªã©ã€**Decoder-onlyå‹**ã®å¯è¦–åŒ–
* Hugging FaceçµŒç”±ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã—ã€

  *â€œã©ã®éå»ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‚ç…§ã—ã¦ã„ã‚‹ã‹â€* ã‚’è¦‹ã‚‹

  ğŸ¤– â†’ *LLMã®ç”ŸæˆæŒ™å‹•ã®ç†è§£ã«ç›´çµ*

---

#### ğŸŸ  **â‘£ BertVizä»¥å¤–ã®å¯è¦–åŒ–æ³•**

* **Attention Rollout** ï¼ˆå±¤ã‚’ã¾ãŸã„ã ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¼æ’­ï¼‰
* **TransformerLens** ï¼ˆOpenAIç³»ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ§‹é€ è§£æï¼‰
* *ç ”ç©¶å¯„ã‚Šã§ã€LLMã®æ€è€ƒçµŒè·¯ã‚’è¿½è·¡ã™ã‚‹æ–¹æ³•*

---

ã©ã®ãƒ†ãƒ¼ãƒã‹ã‚‰é€²ã‚ã¦ã¿ãŸã„ã§ã™ã‹ï¼Ÿ

ï¼ˆç•ªå·ã§ã‚‚ã€ã€Œæ—¥æœ¬èªBERTã€ãªã©ã®è¨€è‘‰ã§ã‚‚OKã§ã™ï¼‰
