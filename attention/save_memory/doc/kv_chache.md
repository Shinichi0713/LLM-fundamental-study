 **KVキャッシュ（Key-Value Cache）** は、LLMが文章を生成する際の「計算の無駄」を省き、生成速度を劇的に向上させるための最も重要な推論最適化技術の一つです。

なぜこの技術が必要なのか、LLMの「生成の仕組み」に潜む課題から解説します。

### 1. 解決しようとした課題：再計算の無駄

LLM（デコーダオンリーモデル）は、 **「自己回帰（Autoregressive）」** という性質を持っています。これは、次の1単語を予測するために、過去のすべての単語を読み直す必要があるという仕組みです。

#### KVキャッシュがない場合の問題点

例えば、「私は / リンゴ / を」の後に続く言葉（食べた）を予測する場合を考えます。

1. 「私は」から「リンゴ」を予測する際、**「私は」**の K（Key）と V（Value）を計算します。
2. 「私は リンゴ」から「を」を予測する際、再び **「私は」** と **「リンゴ」** の両方の K と V を計算し直します。
3. 「私は リンゴ を」から「食べた」を予測する際、また最初から **「私は」「リンゴ」「を」** の K と V をすべて計算し直します。

このように、文章が長くなればなるほど、**過去に一度計算したはずの K と V を何度も何度も計算し直す**ことになり、計算量が雪だるま式に増えてしまいます。これが原因で、生成が非常に遅くなってしまうのです。

### 2. KVキャッシュの概要：計算結果の「保存」

KVキャッシュは、この **「二度手間」を解消するメモ帳** のような役割を果たします。

#### 仕組み

1. **保存**: あるトークン（単語）を処理した際に生成された **K（Key）ベクトル** と **V（Value）ベクトル** を、GPUのメモリ上に保存しておきます。
2. **再利用**: 次のトークンを生成する際は、 **新しく追加されたトークンの K と V だけ** を計算します。
3. **結合**: 保存しておいた過去の K, V キャッシュと、新しい K, V をガッチャンコ（結合）してアテンション計算を行います。

> KVキャッシュは過去のアテンションを覚えておいて2度目に計算しないようにする仕組み

### 3. KVキャッシュ導入の効果

| 項目                | キャッシュなし                     | キャッシュあり                               |
| ------------------- | ---------------------------------- | -------------------------------------------- |
| **計算量**    | トークンが増えるたびに爆発的に増加 | **常に一定（最新の1語分のみ）**        |
| **生成速度**  | 文章が長くなるほど遅くなる         | **最後まで高速なまま**                 |
| **GPUの役割** | 演算ユニット（演算）を酷使する     | **メモリ帯域（読み出し）が主役になる** |

### 4. 新たな課題：メモリ（VRAM）の圧迫

KVキャッシュによって「計算速度」の課題は解決されましたが、代わりに従うのが **「メモリ消費量」** という課題です。

* **VRAMを大量に喰う**: 過去の情報をすべてメモリに載せ続ける必要があるため、長い文章（ロングコンテキスト）を扱うほど、GPUのメモリを圧迫します。
* **共有化技術への繋がり**: この「メモリが足りない！」という問題を解決するために、最初にお話しした **MQA** や **GQA**（KeyとValueの数を減らしてキャッシュを節約する技術）が誕生したのです。

> KVキャッシュによりトークン再計算が解消されたがメモリが不足する問題が生じた

### まとめ：推論エンジンの心臓部

KVキャッシュは、 **「計算時間をメモリで買う」** というトレードオフの技術です。これがあるおかげで、ChatGPTのようなスムーズな文字生成が実現できています。

## KVキャッシュの効果

KVキャッシュの効果を定量的に評価した研究は数多くあり、その結論は一貫して　 **「計算量を劇的に削減する一方で、メモリ帯域（読み出し速度）と容量が新たなボトルネックになる」**　というものです。

主要な論文や技術レポートから、いくつかの具体的な指標を挙げて解説します。

### 1. 理論的な計算量の削減

最も基本的な評価は、計算の計算量（アルゴリズムの複雑性）の変化です。

* **キャッシュなし**: 1トークン生成するごとに過去の全トークンを再処理するため、生成ステップごとの計算量はシーケンス長  に対して  **で増加します（個のトークンを生成する総計算量は** ）。
* **キャッシュあり**: 過去の情報を再利用するため、1ステップあたりの計算量は **** に抑えられます（総計算量は ****）。

ある解析（Mandeep Singh, 2024）では、7Bクラスのモデルにおいて、文脈が長くなるほどキャッシュによる計算効率の向上は **4,000倍〜16,000倍** に達すると試算されています。

### 2. レイテンシ（遅延）の劇的な低下

「Keyformer（MLSys 2024）」などの論文では、キャッシュなしの状態との比較が示されています。

* **推論遅延の増加**: キャッシュを使用しない場合、文脈が長くなるにつれて推論レイテンシは**最大で50倍以上**に膨れ上がることが報告されています。
* **データ移動のコスト**: 一方で、キャッシュを使用すると「計算」自体は速くなりますが、推論時間の**約40%**が「過去のKVキャッシュをメモリから読み出すためのデータ移動」に費やされるようになります。これが、現代のLLMが「演算性能」よりも「メモリ帯域（HBM）」を重視する最大の理由です。

### 3. スループット（処理能力）の向上

システムレベルでの評価としては、**vLLM**（PagedAttention）の論文（Kwon et al., 2023）が有名です。

* **スループットの改善**: 効率的なKVキャッシュ管理（PagedAttention）を導入することで、従来の標準的な推論エンジンと比較して、**2倍〜4倍のスループット向上**を達成したことが定量的に示されています。
* **メモリの有効活用**: 従来の固定的なキャッシュ確保では、メモリの **60%〜80% が未使用（断片化）** でしたが、これを改善することでより大きなバッチサイズでの処理を可能にしました。

### 4. メモリ消費のトレードオフ

計算速度と引き換えに、メモリ消費量は以下のように定量化されます。

> **計算例（FP16精度の場合）**:
> 1トークンあたりのKVキャッシュサイズ =  (bytes)
>
> * **Llama-7B (32層, 4096次元)** の場合：
>   1トークンごとに約 **0.5MB** のメモリを消費します。
> * **1000トークン保持する場合**：
>   約 **500MB** がキャッシュだけで占有されます。

## 実装

KVキャッシュを自作モデルやPyTorchで実装する場合、主な変更点は **「Attention（注意機構）層」** と **「推論ループ（生成ループ）」** の2箇所です。

実装の肝は、各レイヤーで計算した  と  をリスト（またはタプル）として保持し、次のステップの入力に結合することです。

### 1. Attention層での実装（PyTorch）

通常のAttentionに、過去の K, V を受け取り、新しい K,V を返す機能を追加します。

コード中のここがキャッシュを利用している個所です。

```python
if kv_cache is not None:
    # 過去 KV と結合
    k = torch.cat([kv_cache["k"], k], dim=2)
    v = torch.cat([kv_cache["v"], v], dim=2)
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttentionWithKVCache(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        assert self.head_dim * num_heads == embed_dim

        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x, kv_cache=None):
        """
        x: (B, T, D)  ※ 推論時は T=1
        kv_cache: dict or None
          {
            "k": (B, H, T_cache, Hd),
            "v": (B, H, T_cache, Hd)
          }
        """

        B, T, D = x.shape

        # QKV projection
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # reshape -> (B, H, T, Hd)
        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        # === KV Cache logic ===
        if kv_cache is not None:
            # 過去 KV と結合
            k = torch.cat([kv_cache["k"], k], dim=2)
            v = torch.cat([kv_cache["v"], v], dim=2)

        # 更新された KV を保存
        new_kv_cache = {
            "k": k.detach(),
            "v": v.detach()
        }

        # Attention
        attn_scores = torch.matmul(q, k.transpose(-2, -1))
        attn_scores /= self.head_dim ** 0.5

        attn_weights = F.softmax(attn_scores, dim=-1)

        context = torch.matmul(attn_weights, v)
        context = context.transpose(1, 2).contiguous()
        context = context.view(B, T, D)

        out = self.out_proj(context)

        return out, new_kv_cache
```

### 2. 推論ループでの実装

実際にダミーの入力を使って実際に計算させると以下のようになります。

```python
torch.manual_seed(0)

embed_dim = 32
num_heads = 4

attn = SelfAttentionWithKVCache(embed_dim, num_heads)

# 初期 KV キャッシュ
kv_cache = None

# 擬似的に 5 トークン逐次生成
for step in range(5):
    x = torch.randn(1, 1, embed_dim)  # 1 token input

    out, kv_cache = attn(x, kv_cache)

    print(f"Step {step}")
    print("Output shape:", out.shape)
    print("KV cache K shape:", kv_cache["k"].shape)
    print()

```

出力イメージはこの通りです。

```
Step 0
KV cache K: (1, 4, 1, 8)

Step 1
KV cache K: (1, 4, 2, 8)

Step 2
KV cache K: (1, 4, 3, 8)
```

### 3. 実装のポイントと注意点

#### ① 入力サイズの削減

KVキャッシュを使う最大のメリットは、2ステップ目以降の入力 `x` のサイズを `[batch, 1, d_model]`（つまり **1トークンだけ** ）にできることです。キャッシュがない場合は常に `[batch, 現在の全長さ, d_model]` を入力し、全計算をやり直す必要があります。

#### ② 位置エンコーディング（RoPEなど）の扱い

位置エンコーディング（Rotary Positional Embeddings）を使用している場合、 **「今のトークンが全体の何番目か」** という絶対的な位置情報を正しく渡す必要があります。

* キャッシュを使う場合、`input_ids` は1つですが、その位置インデックス（`position_ids`）は `5` や `10` といった進んだ値にする必要があります。

#### ③ メモリ管理

`torch.cat` は計算のたびに新しいメモリを確保するため、非常に長い文章では非効率になることがあります。商用の高速推論エンジン（vLLMなど）では、**PagedAttention** という、OSの仮想メモリのようにキャッシュを飛び飛びのメモリブロックで管理する、より高度な実装が使われています。

### まとめ

1. **保存**: `(k, v)` を各層で計算して返す。
2. **再利用**: 次の入力時に前回の `(k, v)` を `past_key_value` として受け取る。
3. **結合**: `torch.cat` で過去分と今回分をくっつける。


## KVキャッシュが学習で使えない理由

> **KV キャッシュは「自己回帰・逐次生成」という前提が成立して初めて意味を持つ最適化であり、
> 学習（training）ではその前提が根本的に成立しないため使えません。**

これは実装都合ではなく、**計算グラフ・学習目的・並列性の本質的差異**によるものです。

以下、段階的に整理します。

---

## 1. KV キャッシュが成立する前提条件

KV キャッシュは、次の条件が揃ったときのみ有効です。

1. **過去トークンの K,V が将来ステップで再利用される**
2. **過去トークンの表現が固定（変化しない）**
3. **逐次的に1トークンずつ処理される**

これは **推論（inference）時の自己回帰生成**に完全に一致します。

---

## 2. 学習時に起きていること（決定的な違い）

### 学習時（teacher forcing）

```text
入力:  x1, x2, x3, ..., xT
出力:  y1, y2, y3, ..., yT
```

* **全トークンが同時に入力される**
* **全位置の損失を同時に計算**
* **全位置の attention を並列に計算**

つまり：

> **「未来を含めた完全な系列」を一括処理する**

---

## 3. 学習時には「過去」が存在しない

推論時：

```text
t=1 → t=2 → t=3 → ...
```

学習時：

```text
t=1..T が同時に存在
```

### 重要なポイント

* 学習では「t=5 の K,V を先に計算して保存」などという概念がない
* 全トークンの K,V は **一回の forward で同時に計算される**

👉 **再利用する「次のステップ」が存在しない**

---

## 4. 計算量の観点：キャッシュしても得しない

### 推論（without KV cache）

* ステップ t ごとに
  * K,V を 1..t 全部再計算
* 計算量：
  **O(T²)**

### 推論（with KV cache）

* 各ステップで K,V を 1トークン分だけ追加
* 計算量：
  **O(T)**

---

### 学習時

* 最初から：
  * K₁..Kₜ
  * V₁..Vₜ
  * Q₁..Qₜ
* **すべて一度だけ計算**

計算量はすでに最適：

```text
O(T²) ← attention本体
```

👉 **KV 再計算問題が存在しない**

---

## 5. 勾配計算（backprop）が致命的に相性が悪い

これが「本質的理由」です。

### KV キャッシュは「値を固定する」最適化

* 推論：OK（勾配不要）
* 学習：❌

理由：

* 学習では
  * K,V は **重みの関数**
  * 勾配を通す必要がある

もし KV をキャッシュすると：

* 古い K,V を使う
* しかし重みは更新されている
* **計算グラフが壊れる**

---

## 6. 「過去の K,V」は学習中に不正になる

学習では各 iteration ごとに：

```text
W → W'
```

つまり：

* 同じトークンでも
* iteration が変われば
* K,V は別物

👉 **キャッシュの再利用が理論的に不正**

---

## 7. 並列性が最優先される世界

学習では：

* GPU/TPU の並列演算を最大化
* メモリアクセスより FLOPs を優先

KV キャッシュは：

* メモリ読み書きが増える
* 並列性を壊す

👉 **学習では「最悪の最適化」**

---

## 8. 「じゃあ causal mask とは何が違う？」

良い疑問です。

| 要素        | 役割                     |
| ----------- | ------------------------ |
| causal mask | 情報遮断（理論制約）     |
| KV cache    | 計算再利用（工学最適化） |

* mask は **学習目標の一部**
* cache は **推論高速化専用**

---

## 9. 例外：KV cache 風のものが研究で使われるケース

限定的に存在します。

### ① Streaming / chunked training

* 長文を分割
* 過去 chunk の K,V を固定
* **近似学習**

👉 正確な勾配ではない

---

### ② RNN-like memory models

* 状態を明示的に切る
* Transformer とは別物

---

## 10. 一文でまとめると

> **学習では「未来を含む全系列を一度に処理し、勾配を流す」ため、
> 再利用可能な「過去の KV」という概念が存在しない。**

---

## 思考の到達点（重要）

この理解ができると：

* なぜ inference と training のコードが全く別になるか
* なぜ FlashAttention は training に効くが KV cache は効かないか
* なぜ online learning は難しいか
* なぜ Transformer は本質的に「非状態的」か

が一本につながります。


## VS SSM

KV cache と State-Space Model（SSM）の関係は、
**「Transformer が本質的に苦手なことを、SSM が自然に解決している」**という対比で理解すると非常に明確になります。

結論を先に述べます。

> **KV cache は Transformer を“擬似的に状態機械化”するための後付け最適化であり、
> SSM は最初から「状態」を中心に設計されたモデルである。**

以下、段階的に整理します。

---

## 1. 両者が解こうとしている共通の問題

### 共通課題

* 長系列の処理
* 過去情報の保持
* ストリーミング / オンライン処理
* 推論時の計算量削減

Transformer と SSM は、**同じ問題に全く異なる思想でアプローチ**しています。

---

## 2. Transformer の本質：状態を持たないモデル

Transformer は構造的に次の性質を持ちます。

* 各トークンは独立したベクトル
* 時間方向の「内部状態」を持たない
* 過去はすべて **明示的に再参照** する

数式的には：

```
y_t = Attention(x_1, x_2, ..., x_t)
```

👉 過去は「保存された状態」ではなく、「毎回参照される履歴」

---

## 3. KV cache の正体：後付けの状態

KV cache を導入するとどうなるか。

### 推論時の構造変化

```
state_t = {K_1..K_t, V_1..V_t}
y_{t+1} = Attention(Q_{t+1}, state_t)
```

これは実質的に：

> **「状態 state_t を持つ逐次モデル」**

になっています。

つまり：

* 本来 stateless な Transformer
* KV cache により **状態付きモデルとして振る舞わせている**

---

## 4. SSM の本質：状態が第一級概念

SSM は最初から以下を前提に設計されています。

```
s_{t+1} = A s_t + B x_t
y_t     = C s_t
```

特徴：

* 状態 `s_t` がモデルの中心
* 状態は固定次元
* 更新は O(1)

👉 **「過去を圧縮した要約」が常に存在**

---

## 5. KV cache と SSM の構造的対応

| 観点       | KV cache (Transformer) | SSM      |
| ---------- | ---------------------- | -------- |
| 状態の有無 | 擬似的                 | 本質的   |
| 状態サイズ | O(T)                   | O(1)     |
| 更新コスト | 増加し続ける           | 定数     |
| 記憶の性質 | 生履歴                 | 圧縮状態 |
| 設計思想   | 後付け                 | 最初から |

重要なのは：

> **KV cache は「履歴そのもの」を状態として持つが、
> SSM は「履歴の要約」を状態として持つ**

---

## 6. なぜ Transformer は SSM 的になれないのか

Transformer が「完全な状態圧縮」を持てない理由：

1. Attention は「全履歴参照」を前提
2. 可変長の依存関係
3. 明示的な状態遷移方程式が存在しない

そのため：

* 状態 = K,V 全部
* 次元は時刻とともに増大

---

## 7. SSM が KV cache を必要としない理由

SSM では：

* 状態 `s_t` がすでに過去を保持
* 次のステップに必要なのは `s_t` のみ

```text
KV cache 相当 = s_t
```

しかも：

* 次元固定
* GPU/CPU/NVMe にオフロード不要
* 学習時にも推論時にも同じ構造

---

## 8. 「KV cache = 可変長状態」「SSM = 固定長状態」

この対比は非常に重要です。

* Transformer + KV cache
  → **非圧縮・高精度・高コスト**
* SSM
  → **圧縮・高速・低メモリ**

---

## 9. 実モデルでの対応関係

### Transformer 系

* GPT / LLaMA
* KV cache が必須
* 長文でメモリ破綻

---

### SSM 系

* S4
* Mamba
* RWKV（Attention-free Transformer）

特徴：

* 真のストリーミング
* 長文安定
* 推論 O(T)

---

## 10. ハイブリッドが次の主戦場

現在の研究潮流は：

* Transformer（高表現力）
* SSM（長期状態）

を組み合わせる方向です。

### 例

* Attention + SSM block
* 局所 Attention + グローバル SSM
* MoE × SSM

---

## 11. 一文でまとめると

> **KV cache は Transformer を無理やり「状態付き」にするための工学的トリックであり、
> SSM は最初から「状態を持つ知能」を数学的に定義したモデルである。**

---

## 次に繋がる問い

この理解があると、次の疑問が自然に出ます。

* なぜ RWKV は KV cache を不要にできるのか
* なぜ Mamba は長文で崩れにくいのか
* なぜ Attention-free が再評価されているのか
* Transformer は将来「状態モデル」に置き換わるのか

どこまで掘りますか？
