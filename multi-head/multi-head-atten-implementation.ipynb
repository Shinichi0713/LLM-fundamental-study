{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 目的\n",
        "\n",
        "Multi-Head Attention（多頭注意）は、\n",
        "異なる観点（head）から系列内の依存関係を学ぶ 仕組みです。\n",
        "\n",
        "たとえば：\n",
        "\n",
        "1つのheadは「主語と動詞」の関係を見ている\n",
        "\n",
        "別のheadは「文の時制」や「修飾関係」を見ている\n",
        "\n",
        "このように、情報を並列に複数視点で抽出できます。"
      ],
      "metadata": {
        "id": "zXmLCoBCA3eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 数式の流れ\n",
        "入力ベクトルを ( X \\in \\mathbb{R}^{(L, d_{\\text{model}})} ) とします。\n",
        "（L: シーケンス長、d_model: 埋め込み次元）\n",
        "\n",
        "### (1) Q, K, V を作る\n",
        "\n",
        "3つの線形変換を行います：\n",
        "\n",
        "$$\n",
        "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
        "$$\n",
        "\n",
        "ここで\n",
        "( W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} )\n",
        "\n",
        "### (2) スコアを計算する\n",
        "\n",
        "Attentionスコアは次のように求めます：\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "このスコアは「どの単語に注目すべきか」を表します。\n",
        "\n",
        "\n",
        "### (3) Multi-Head化\n",
        "\n",
        "上のAttentionを複数headで並列に行い、結合します：\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(X) = \\text{Concat}(head_1, ..., head_h)W_O\n",
        "$$"
      ],
      "metadata": {
        "id": "Rxs-G8USBGIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-6yPQk1VAuLs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q,K,V: [batch, heads, seq_len, head_dim]\n",
        "        d_k = Q.size(-1)\n",
        "        scores = (Q @ K.transpose(-2, -1)) / d_k**0.5  # スコア計算\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        output = attn @ V\n",
        "        return output, attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Q,K,Vを作る線形層\n",
        "        self.W_Q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_K = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_V = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # 出力結合\n",
        "        self.W_O = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, L, C = x.size()  # (batch, seq_len, embed_dim)\n",
        "\n",
        "        # Q,K,V作成して [B, heads, seq_len, head_dim] にreshape\n",
        "        Q = self.W_Q(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.W_K(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.W_V(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 各headでAttention\n",
        "        out, attn = self.attention(Q, K, V, mask=mask)\n",
        "\n",
        "        # 結果を結合\n",
        "        out = out.transpose(1, 2).contiguous().view(B, L, C)\n",
        "        return self.W_O(out), attn"
      ],
      "metadata": {
        "id": "W3GeVNTfBa9r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# サンプル入力\n",
        "batch_size, seq_len, embed_dim, num_heads = 2, 5, 128, 4\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "out, attn = mha(x)\n",
        "\n",
        "print(\"出力の形状:\", out.shape)\n",
        "print(\"Attentionスコア:\", attn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVV4-ZlxBeir",
        "outputId": "08a7007f-45b5-4a74-a694-850b86ebdf2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "出力の形状: torch.Size([2, 5, 128])\n",
            "Attentionスコア: torch.Size([2, 4, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K01wFUQjBgnh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}