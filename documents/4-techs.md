# 目的

VLM関係の技術を浅く広く収集する。

## VLM-LENS

### 課題

VLMは、あたかもブラックボックスのように、入力と出力の関係性しか見えない存在でした。

従来のVLMの評価は、主に**精度**に基づいて行われてきました。例えば、画像に対する質問応答タスクであれば、正解率が高いほど高性能なモデルと評価されます。

しかし、精度だけでは、VLMがどのような情報を利用して判断しているのか、本当に理解しているのかを判断することはできません。

VLMは、データセットに含まれる**偏り（バイアス）**を利用して、表面的な特徴に頼った判断をしてしまうことがあります。

例えば、「空の写真には必ず鳥が写っている」というデータセットで学習した場合、VLMは空の写真を見ると、鳥が写っていなくても「鳥がいる」と判断してしまう可能性があります。このような場合、精度は高くても、VLMが本質的な理解をしているとは言えません。

### 解決法

VLM-LENSは、VLMの**内部表現**を抽出し、分析・解釈するためのツールキットです。

内部表現とは、VLMが画像やテキストを処理する過程で生成する、数値データの集合です。

この内部表現を分析することで、VLMがどのような情報を重視し、どのように判断しているのかを理解することができます。

VLM-LENSは、以下の課題を解決し、新たな可能性を切り開きます。

* **既存の評価方法の限界の克服**：精度だけでなく、内部表現に基づいた評価を可能にすることで、VLMの本質的な理解度を評価します。
* **モデル固有の複雑さの抽象化**：様々なVLMに対して、統一的なインターフェースを提供することで、モデルごとの複雑な設定や前処理を不要にします。
* **解釈可能性ツールキットの拡張**：TransformerLensなどの既存のツールキットを拡張し、VLMsをサポートすることで、より高度な分析を可能にします。


VLM-LENSを実行した結果は、**SQLiteデータベース**に格納される、モデルの**中間表現（テンソル）**が中心のデータです。** **

あなたの実行結果の `DataFrame`から、以下のことがわかります。

VLM-LENSの出力データの構造** **

1. **メタデータ** **: 実行に関する情報が格納されています。**

* **`id`** **: データのユニークなID。**
* **`name`** **: 使用したモデルの名前（例:** `Salesforce/blip2-opt-2.7b`）。
* **`architecture`** **: モデルのアーキテクチャ（例:** `blip2`）。
* **`timestamp`** **: 実行日時。**
* **`image_path`** **: 入力画像のパス。**
* **`prompt`** **: 入力プロンプト（例:** `"Describe the color in this image in one word."`）。
* **`layer`** **: データを抽出したモデルのレイヤーの名前（例:** `vision_model.post_layernorm`, `language_model.lm_head`）。

1. **中間表現（テンソル）** **: これがVLM-LENSの核心となるデータです。**

* **`tensor`** **: これが抽出された中間表現のデータそのものです。あなたの** `DataFrame`では、`b'PK...'`という `bytes`形式で保存されています。これは、NumPy配列が圧縮され、データベースにバイナリデータとして格納されていることを示しています。
* **`tensor_dim`** **: テンソル（配列）の次元数。**`language_model.lm_head`の `50304`は、語彙サイズに対応している可能性が高いです。** **

なぜデータが `b'PK...'`と表示されるのか？

`b'PK...'`は、`pandas`がSQLiteデータベースからバイナリデータを読み込んだ際に、そのままのバイト文字列として表示しているためです。このバイナリデータは、NumPy配列を `np.save()`などでシリアライズ（直列化）したものです。


![](https://anishk23733.github.io/vl-interp/images/teaser.png)

***Interpreting VLM internal image representations.** (a) Given a VLM, (b) we unembed the latent representations from image embeddings to the vocabulary and classify hallucinations. We remove hallucinations by (c) linearly editing them out of the latent representations.*



## On the Perception Bottleneck of VLMs for Chart Understanding

1. どんなもの?

この論文「On the Perception Bottleneck of VLMs for Chart Understanding」は、現代のビジュアル・ランゲージ・モデル（LVLMs）が抱えるグラフ理解における認識ボトルネックについて探求した研究です。ビジュアル・ランゲージ・モデルは、数値データ、テキスト要素、複雑なビジュアルコンポーネントを分析・推論する能力を持つことが求められます。特にグラフという視覚的かつ情報量が多い要素を正確に解釈するためには、モデルが高いレベルの認識能力を持たなければなりません。しかし、現行のLVLMsはこの過程においてしばしばボトルネックに直面します。本研究は、ビジョンエンコーダーのボトルネックと抽出ボトルネックという2つの主な障害を特定し、それぞれに対して改善策を提案することで、より高度なグラフ理解を可能にすることを目指しています。

2. 先行研究と比べてどこがすごい?

この研究が先行研究と比べて特に優れているのは、LVLMsに共通する課題へのフォーカスをより詳細に行っているという点です。多くの従来の研究が、モデルの性能向上を主にパラメータ数やネットワークの構造に基づいて行ってきました。しかし本研究は、具体的に視覚的ビジョンエンコーダーのボトルネックとグラフからの情報抽出における障害に注目し、それぞれのボトルネックを克服するためのカスタマイズされたCLIPs（Contrastive Language-Image Pretraining）を用いた新しいアプローチを提案しています。これは特定の課題領域に特化したソリューションの一例であり、汎用的なモデルにおいても効果的に機能する可能性を示しています。

3. 技術や手法のキモはどこ?

技術の要点としては、2つのボトルネックに対するアプローチが挙げられます。まず、視覚的な情報のエンコードにおけるボトルネックの解決には、グラフ理解に特化したCLIPsを構築します。これにより、視覚情報のより効果的な処理が可能になります。次に、グラフからの情報抽出におけるボトルネック克服のために、グラフ特有のチューニングを施されたモデルが用いられています。これらの技術的改善は、視覚的かつ言語的な情報の高度な処理を可能にし、モデルの理解能力を飛躍的に向上させる鍵となっています。

4. どうやって有効だと検証した?

本研究の有効性の検証には、LVLMsを用いた複数の実験を通じて行われています。具体的には、視覚エンコーダーの性能に関して、CLIPsを用いた新しいアプローチがどの程度グラフ理解に寄与するかを比較するテストが施されています。また、グラフ特有のチューニングの効果についても、チューニング前後のモデルのパフォーマンスを比較し、明確な改善が見られることを証明しています。これらのアプローチが実際にモデルの性能向上に寄与していることをデータを用いて示したことにより、理論的な機能改善の実際の適用可能性が支持されました。

5. 議論はある?

この研究は、LVLMsの性能を大幅に向上させる可能性を示す一方で、いくつかの議論の余地を残しています。まず、提案された手法がどの程度他の領域にも応用可能かという点です。グラフ理解以外のタスクにこの手法が適用可能であるか、どの程度のカスタマイズが必要かなど、さらなる研究が必要です。また、異なる種類のチャートやビジュアル形式に対する影響も考慮する必要があります。加えて、計算コストやリソースにおける効率性の問題も取り上げる必要があります。これらの議論は、さらなる研究の深化を促進するための出発点とも言えるでしょう。

6. 次読むべき論文は?

次に読むべき論文を探す際のキーワードとしては、「Vision-Encoding Bottleneck」、「Chart-Specific Tuning」、「LVLMs Improvement」、「Visual Data Interpretation」、「Cross-Modal Learning」が挙げられるでしょう。これらのテーマに関連する研究を深く理解することで、LVLMsの更なる性能向上へのヒントが得られるかもしれません。
