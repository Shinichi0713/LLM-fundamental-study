## 構成検討

VLMの視覚を分析するという能力をハンズオンで伝える構成とする。

ゴールポイントの設定と、ゴールポイントに到達するための構成について検討する。

## 最近の技術動向

近年、VLM（Vision-Language Model）は、テキストと画像（または動画）を統合的に理解するマルチモーダルAIの一分野として、急速な進歩を遂げています。特に、単なる物体認識を超えた複雑な推論やタスク実行が可能になっています。** **

主な技術動向は以下の通りです。

**1. エージェント機能の強化**

* **GUI操作とロボット制御：** **VLMは、コンピューターやスマートフォンのユーザーインターフェース（UI）操作、さらにはロボットの物理世界での動きを制御するエージェント機能の基盤として活用されています。**
* **マルチモーダルな推論と対話：** **テキストと視覚情報を組み合わせた、より複雑な推論や対話が可能になっています。**
* **代表的なモデル：** **ByteDanceの「UI-TARS-1.5」や「MAGMA-8B」などが、UI操作エージェントとしての高い性能を示しています。**

**2. 効率性と軽量化**

* **高速化技術：** **モデルの推論速度を向上させる技術開発が進んでおり、パナソニックはVLMの推論速度を約2倍にする「SparseVLM」を開発しました。**
* **リソース効率化：** **モデルの枝刈り（プルーニング）や知識蒸留、エッジコンピューティングなどの技術により、計算リソースの少ない環境でも動作する軽量なVLMアーキテクチャが研究されています。**
* **エッジデバイスへの展開：** **ARグラスやIoTデバイスといった、より多くのデバイス上でのVLMの実用化が進んでいます。**

**3. より高度な理解と生成能力**

* **生成モデルの進化：** **複雑なコンテンツを生成する能力が向上しています。**
* **文脈理解の深化：** **視覚情報とテキストに埋め込まれた感情的・文化的なニュアンスを理解し、より文脈に即したシステムへと進化しています。**
* **多言語対応：** **グローバルな利用に対応するため、多言語に対応したVLMの開発が進んでいます。**

**4. 応用分野の拡大**

* **ヘルスケア：** **レントゲンやMRIなどの医用画像と医師のテキストを組み合わせて学習し、疾患の発見や画像診断レポートの作成を支援します。**
* **eコマース・小売：** **視覚的検索やAIによるレコメンデーションにより、直感的なショッピング体験を実現します。**
* **リモートセンシング：** **衛星画像などの複雑なデータを分析し、災害監視や都市計画、農業管理といった分野で応用されています。**
* **教育とアクセシビリティ：** **視覚障害のあるユーザー向けに画像を説明したり、視覚的文脈を加味した教材の作成・翻訳を行ったりするなど、学習のインタラクティブ性を高めます。**

これらの技術動向は、VLMが単なる学術研究から、より実用的で社会に貢献するAIへと進化していることを示しています。今後、VLMのさらなる性能向上と効率化により、多様な産業や日常生活で革新的な応用が生まれていくと期待されています。

## グラフ分析

グラフや表の読み取り・理解は、VLM（Vision-Language Model）の重要な応用分野として、活発に研究・開発が進められています。特に最近の技術動向では、単なるデータ抽出を超え、より複雑な推論や対話に対応する能力が向上しています。** **

主な技術動向は以下の通りです。

**1. 高精度なデータ抽出**

* **構造化されたデータへの変換：** **VLMは、グラフや表を画像として認識するだけでなく、その内容をMarkdownやHTML、あるいは構造化されたJSON形式のデータに変換する能力を高めています。**
* **NVIDIA NeMo Retriever Parse：** **NVIDIAが提供するこの技術は、複雑なドキュメントから表を抽出し、使いやすいデータに変換する能力で注目されています。**

**2. 複雑な推論と多段階の質問応答**

* **Chain-of-Thought（CoT）：** **複数ステップの推論が必要な質問に対し、VLMは「思考の連鎖」を促すプロンプトを利用することで、段階的な思考プロセスを経て回答を導き出す能力が向上しています。**
* **マルチチャートQA：** **複数のグラフや表にまたがる情報を統合し、比較や時系列の変化に関する質問に答える技術も進化しています。これにより、複数のデータソースを横断した高度な分析が可能になります。**

**3. ロバスト性の向上**

* **さまざまな形式への対応：** **手書きのグラフや表、さまざまな色使いやレイアウトのグラフなど、多様な視覚的変化に対応する頑健性（ロバスト性）を高める研究が進んでいます。**
* **幻覚（ハルシネーション）の軽減：** **CoTプロンプティングなどの技術により、生成された回答がグラフや表のデータと一致しない「ハルシネーション」を減らす努力がなされています。**

**4. 応用分野の拡大**

* **ビジネスインテリジェンス（BI）：** **経営層がダッシュボードのグラフについて自然言語で質問し、瞬時に回答を得るような活用が進んでいます。これにより、データ分析の民主化が加速しています。**
* **科学研究：** **科学論文に含まれるグラフや表をVLMが分析し、質問に答えたり、要約したりする技術が開発されています。これにより、研究者や学生が論文から情報を得る手間が軽減されます。**
* **PDFとのチャット：** **PDFドキュメントに含まれる画像や表、テキストをすべて考慮した上で、質問に答えるシステムが構築されています。**

**5. ベンチマークの進化**

* **mChartQA、MultiChartQA：** **複数のグラフを扱うなど、より複雑な現実世界のシナリオに対応するベンチマークデータセットが登場しています。これらのベンチマークは、モデルの性能向上を促す上で重要な役割を果たしています。**

これらの技術動向は、VLMがグラフや表を単に画像として処理するだけでなく、その背後にある意味や文脈を理解し、人間のように推論する能力を獲得しつつあることを示しています。今後、さらに高精度で効率的なグラフ・表理解技術が、さまざまな分野で実用化されていくと期待されています。

## 具体的な手法

グラフや表を読み解くVLMの具体的な手法名やアプローチは、目的やタスクによって様々です。以下に、主要な手法や技術的なキーワードをいくつかご紹介します。** **

**1. グラフ・表のデータ抽出**

* **TableVLM:** **複雑なテーブル構造の認識に特化したマルチモーダルな事前学習モデルです。2ストリームのトランスフォーマーベースのエンコーダー・デコーダー構造を用いており、テーブル構造に関する豊富な特徴を学習します。**
* **PromptingとFine-Tuning:** **既存のVLMに対し、グラフや表のデータ抽出に特化したプロンプトを与えたり、少量のデータでファインチューニングを行ったりする手法です。**

**2. 複雑な推論と質問応答**

* **Chain-of-Thought（CoT）プロンプティング:** **モデルに複数の推論ステップを段階的に実行させることで、グラフや表から複雑な情報を引き出す手法です。これにより、複数ステップにわたる計算や比較が必要な質問にも対応できます。**
* **Retrieval-Augmented Generation（RAG）:** **外部の知識源（例：グラフデータベース）から関連情報を検索し、それを生成タスクに組み込むことで、推論の精度と信頼性を向上させるフレームワークです。訓練不要でゼロショット推論に対応する「GRRAF」のような手法もあります。**
* **SIMPLOT:** **グラフ画像から推論に必要な要素のみを抽出し、簡素なプロット画像を模倣することで、グラフからテーブルへの変換精度を高める手法です。これにより、無関係な情報が推論の妨げになるのを防ぎます。**

**3. マルチグラフの理解**

* **Multi-graph Joint Reasoning:** **複数の異なる種類のグラフ（ナレッジグラフ、フローチャート、マインドマップ、ルートマップなど）を統合して理解し、推論を行うためのベンチマークと手法です。**
* **Multi-Chart Question Answering (MultiChartQA):** **複数のグラフにまたがる情報を比較したり、時系列の変化を追跡したりするような、より複雑な質問に答えるためのベンチマークと手法です。**

**4. その他のアプローチ**

* **Transfer Learning (LLM-to-VLM):** **大規模言語モデル（LLM）が持つ推論能力を、VLMに転移学習させることで、グラフや表の質問応答性能を向上させる手法です。**
* **Visual-TableQA:** **LaTeXでレンダリングされた複雑なテーブル画像に対する質問応答に特化した、大規模なベンチマークデータセットと、それを活用するモデル構築のアプローチです。**

これらの手法は、単一のVLMに組み込まれたり、複数のモジュールを組み合わせたパイプラインとして実装されたりします。技術開発は非常に速いペースで進んでおり、今後も新しい手法やベンチマークが登場すると予想されます。

## 主要技術

VLM（Vision-Language Model）がグラフを理解できるようになった背景には、主に以下のような技術やアプローチの組み合わせがあります。これらの技術は、視覚情報をテキスト情報に変換するだけでなく、その背後にある論理的な構造や意味を理解するために重要です。** **

1. Vision Encoderの進化** **

* **ViT（Vision Transformer）:**
  * **画像をパッチ（小さなブロック）に分割し、Transformerベースのモデルで処理することで、画像全体の大域的なコンテキストを捉えることができます。**
  * **従来のCNN（Convolutional Neural Network）に比べて、グラフ上の点や線、ラベルといった要素間の空間的な関係性をより効果的に学習することが可能です。**

2. マルチモーダルな事前学習

* **画像とテキストの豊富なデータセット:**
  * **ウェブから収集した膨大な画像とテキストのペアデータ（例：LAION）を用いて、画像とテキスト間の関連性を学習します。**
  * **これにより、モデルは「棒グラフ」や「折れ線グラフ」といった概念と、それらを表すテキストを結びつけることができます。**

3. データ抽出と構造化

* **DePlot:**
  * **Googleによって開発されたモデルで、グラフ画像を入力として受け取り、そのデータをMarkdown形式のテーブルに変換する能力に特化しています。**
  * **この抽出された構造化データは、その後の推論や質問応答タスクに利用されます。**
* **MatCha:**
  * **Pix2Structモデルを拡張し、数学的推論やグラフの逆レンダリング（グラフからデータを復元するタスク）に特化した事前学習を行うことで、グラフ理解能力を向上させています。**
* **UniChart:**
  * **データテーブル生成、数値推論、視覚的推論などのタスクで事前学習を行うことで、グラフ理解能力を獲得しています。**

4. 推論能力の強化

* **Chain-of-Thought（CoT）プロンプティング:**
  * **モデルに複雑な問題を解かせる際、中間的な推論ステップを生成するように指示する手法です。**
  * **これにより、単に答えを出力するだけでなく、グラフからデータを読み取り、計算を行い、結果を導くという思考プロセスを模倣させることが可能になります。**
* **Retrieval-Augmented Generation (RAG):**
  * **質問に対する回答を生成する際、関連する情報を外部の知識ベースから検索して利用する技術です。**
  * **グラフをテーブルに変換し、それをRAGの知識ベースとして利用することで、グラフのデータに基づいた正確な回答を生成できます。**
* **Scene Graph Reasoning:**
  * **画像のオブジェクトとその関係性をグラフ構造で表現する「シーングラフ」を利用することで、VLMの視覚的理解を深める手法です。**
  * **グラフの要素（ノード）と関係性（エッジ）を明示的に扱うことで、複雑な推論タスクが可能になります。**

5. ベンチマークの進化

* **専用のベンチマークデータセット:**
  * **グラフ理解の性能を評価するための専用データセット（例：MatCha、UniChart、DePlot）が開発され、モデルの進歩を加速させています。**
  * **mChartQA**や**MultiChartQA**といったベンチマークは、複数のグラフを扱うなど、より複雑な現実世界のシナリオに対応しています。** **

これらの技術は単独で利用されるだけでなく、組み合わせてより高度なグラフ理解を実現しています。たとえば、まずDePlotでグラフをテーブルに変換し、次にCoTプロンプティングを適用したLLMで推論を行うといったパイプラインが一般的です。

## 手順

とても良い方向性です。
「VLMがグラフを分析できるようになる」というのは、AIの**知覚 → 理解 → 言語化 → 推論**という流れの総合的な訓練過程そのものです。
したがって、「VLM（Vision-Language Model）の基礎から、AIとしての理解能力をどう育てていくか」を段階的に整理すると、まるで**AIの知能発達プロセス**のような形になります。

---

# 🧠 VLMがグラフを分析できるようになるための学習体系（基礎から応用まで）

---

## 🌱 第1段階：AIの基礎理解（知覚と表現の基礎）

### 目的

AIが「画像」や「言語」をどう表現するのかを理解する。

### 学ぶべき概念

| 項目                                          | 内容                                                                   |
| --------------------------------------------- | ---------------------------------------------------------------------- |
| **特徴抽出（Feature Extraction）**      | CNN, ViTなどが画像を数値ベクトルに変換する仕組み。                     |
| **表現学習（Representation Learning）** | 同じ意味を持つ画像・言語を似たベクトル空間に埋め込む技術（CLIPなど）。 |
| **言語モデル（LLM）**                   | テキストを理解・生成するTransformer構造の理解。                        |
| **マルチモーダル統合**                  | 画像とテキストを同じ空間で扱う基礎（Vision-Language Alignment）。      |

### 学習アクション

* 画像特徴抽出の体験：
  → ResNetやCLIPを使って画像のベクトルを抽出してみる。
* テキスト埋め込み体験：
  → 同じ意味の文のベクトル距離を比較する。
* これらを組み合わせて「画像と言葉の意味が近づく」原理を理解。

---

## 👁️‍🗨️ 第2段階：VLM（Vision-Language Model）の仕組み理解

### 目的

AIが「画像を見て説明する」技術を理解する。

### 学ぶべき構造

| モジュール                                  | 内容                                         |
| ------------------------------------------- | -------------------------------------------- |
| **ビジョンエンコーダ**                | ViT, Swin Transformerなどが画像特徴を抽出。  |
| **言語デコーダ**                      | LLaMA, GPT系がテキストを生成。               |
| **アライメント層（Projection Head）** | 画像特徴を言語空間に写像して、両者を接続。   |
| **指示学習（Instruction Tuning）**    | 「画像を見て文章で答える」指示を大量に学習。 |

### 学習アクション

* LLaVA や BLIP-2 のアーキテクチャを学ぶ。
* 「画像→説明文」生成を試す（例：Caption生成）。
* 「画像→質問応答（VQA）」を試す。

これにより、VLMが**視覚と言語の関係性を学ぶ構造**を体感的に理解できる。

---

## 📈 第3段階：グラフ・表という「構造化視覚データ」への拡張

### 目的

VLMが「グラフを視覚情報として理解できる」ようにする。

### グラフ画像の特殊性

| 通常の画像と違う点                 | 必要な理解                                 |
| ---------------------------------- | ------------------------------------------ |
| 意味が「形」ではなく「関係」にある | 軸・系列・値・単位などの抽象関係を読む     |
| テキスト情報が含まれる             | OCRや文字認識を併用                        |
| 数的比較・傾向理解が必要           | 概念的な「上昇」「減少」「比例」などの学習 |

### 学習内容

1. **グラフ構造解析**

   * 軸（x, y）と系列を分離して理解。
   * 棒・線・点などの「意味的パターン」を捉える。
2. **数値的関係学習**

   * 比率・差分・最大最小のような定量関係を説明文に変換。
3. **視覚テキスト融合**

   * グラフ内のラベルや凡例の文字情報をOCRで抽出し、テキスト情報として統合。

---

## 💬 第4段階：VLMによる言語化能力の強化（Chart-to-Text）

### 目的

AIがグラフから「説明文」や「要約文」を生成できるようにする。

### 学習方法

1. **ペアデータによる学習**

   * グラフ画像＋説明文（キャプション）のペアを使う。
2. **要約・傾向表現のパターン学習**

   * 「A年→B年で上昇」「Xの方がYより高い」などの自然言語テンプレート。
3. **指示学習（Instruction Tuning）**

   * 「このグラフの特徴を説明してください」などの指示文に対して自然に応答できるよう調整。

### 代表データセット

* **ChartQA**：グラフに対する質問応答
* **PlotQA / Chart-to-Text**：グラフ説明文生成
* **FigureQA**：図表の関係性理解（比較・一致など）

---

## 🧮 第5段階：数的推論・比較の学習（Vision+Reasoning）

### 目的

単に「見る」だけでなく、「数的・論理的に理解する」段階へ。

### 学ぶ内容

| 能力                   | 具体例                                                             |
| ---------------------- | ------------------------------------------------------------------ |
| **数値認識**     | 軸やバーの高さから値を推定                                         |
| **比較推論**     | 「A > B」「増加率が高いのはC」など                                 |
| **トレンド分析** | 「右肩上がり」「急減」「周期性」などを文章化                       |
| **メタ認知**     | 「このグラフは売上と利益の関係を示している」など、図全体の意味推定 |

### 技術手法

* 視覚特徴と数値テキストを併用する**マルチモーダル推論層**を追加。
* Chain-of-Thought（思考プロンプト）で、数値的推論を明示的に誘導。
  例：「まず軸を確認 → 次に最高値を特定 → 傾向を要約」など。

---

## 🔬 第6段階：自己評価と再学習（Reflection Loop）

### 目的

AIが自分の出力を自己検証し、改善できるようにする。

### 方法

1. **出力の根拠を明示させる**

   * 「どの部分のグラフを根拠にそう判断したか？」を説明させる。
2. **自己修正プロンプト**

   * 「他に見落としている情報はないか？」などを再質問。
3. **評価データによる再チューニング**

   * 誤答を再学習させ、視覚と論理の対応精度を高める。

---

## 🧩 第7段階：応用・統合

### 目的

実タスクで使える「VLMによるグラフ分析AI」を完成させる。

### 応用例

| 分野                               | 応用内容                                        |
| ---------------------------------- | ----------------------------------------------- |
| **ビジネスレポート自動生成** | 売上グラフから自動でレポート文を生成            |
| **研究論文解析**             | 図表から実験結果を説明文に変換                  |
| **教育支援**                 | 学生の提出グラフをAIが自動採点・フィードバック  |
| **データ監視**               | IoTや経済データのグラフをAIが継続監視・異常検知 |

---

## 🔁 学習プロセス全体の流れ

```mermaid
flowchart TD
A[AI基礎理解<br>(特徴抽出・表現学習)] --> B[VLM構造理解<br>(視覚と言語の結合)]
B --> C[グラフ解析拡張<br>(構造・関係・数値理解)]
C --> D[言語化学習<br>(Chart-to-Text, QA)]
D --> E[数値推論・比較<br>(Vision Reasoning)]
E --> F[自己評価・改善<br>(Reflection Loop)]
F --> G[応用展開<br>(レポート生成・教育・研究分析)]
```

---

## 🧭 学び方の実践ステップ（あなたが進める場合）

| 段階          | 学習リソース              | 実践課題例                   |
| ------------- | ------------------------- | ---------------------------- |
| 1. AIの基礎   | fast.ai, CourseraのDL講座 | CLIPで画像と文の類似度を試す |
| 2. VLM理解    | 論文：LLaVA, BLIP-2       | 画像キャプション生成を試す   |
| 3. グラフ解析 | ChartQA Dataset           | グラフに対して質問応答       |
| 4. 言語化     | 自作データで要約          | グラフ説明文生成             |
| 5. 推論       | 自然言語推論＋画像推論    | 「どちらが高い？」を問う     |
| 6. 応用       | OpenAI APIなど            | 売上グラフの自動分析レポート |


## BLIPの理解までを詳しく


とても良い質問です。

BLIP（Bootstrapped Language-Image Pretraining）は、**画像とテキストを同時に理解・生成する「Vision-Language Model（VLM）」** の代表的なモデルで、マルチモーダルAIの中核的な技術です。

BLIPやBLIP-2を「本質的に理解する」ためには、以下のような**技術階層（基礎 → 応用 → 専門）**の順で知識を積み上げるのが理想です👇

---

## 🧱 **レベル1：基礎AI・数学・信号処理の理解**

BLIPを理解するための“地盤”となる分野です。

### 💡 必須基礎

| 分野                               | 学ぶべき内容                          | 理解のゴール                                 |
| ---------------------------------- | ------------------------------------- | -------------------------------------------- |
| **線形代数**                 | 行列・ベクトル、内積、固有値分解、SVD | TransformerのAttention計算を理解できる       |
| **確率・統計**               | 確率分布、期待値、尤度、ベイズ推定    | モデルの出力確率や学習のロス関数を理解できる |
| **微分積分**                 | 勾配・偏微分・最適化                  | 勾配降下法の仕組みを理解できる               |
| **Python / NumPy / PyTorch** | 配列演算、テンソル操作、自動微分      | 実験やモデル検証ができるようになる           |
| **信号処理 / 画像処理**      | 畳み込み、フィルタ、周波数変換        | CNN部分の理解につながる                      |

---

## 🔧 **レベル2：深層学習の中核技術**

BLIPはここを前提に設計されています。

### 🧠 学ぶべき分野

| 分野                                                  | 内容                                            | 関連するBLIPの要素               |
| ----------------------------------------------------- | ----------------------------------------------- | -------------------------------- |
| **ニューラルネット基礎**                        | 活性化関数・損失関数・勾配計算                  | 全てのネットワーク設計の土台     |
| **CNN (Convolutional Neural Networks)**         | ResNet, Vision Transformer (ViT)                | 画像エンコーダ部分               |
| **RNN / Transformer / Attention**               | Self-Attention, Multi-Head Attention, LayerNorm | テキスト処理・マルチモーダル統合 |
| **自己教師あり学習 (Self-Supervised Learning)** | CLIPやSimCLRなどの類似度学習                    | BLIPの事前学習戦略の理解に必須   |
| **マルチモーダル学習**                          | 異なるデータ型を共通空間にマッピング            | BLIPの中心概念                   |
| **Contrastive Learning (対照学習)**             | 正例・負例間の距離最小化                        | 画像とテキストを関連づける       |

---

## 🧩 **レベル3：Vision-Languageモデルの構造理解**

ここでようやくBLIP特有の設計を掘り下げます。

### 📘 理解すべき内容

| 項目                             | 具体的な内容                                                   | 理解の目的                                    |
| -------------------------------- | -------------------------------------------------------------- | --------------------------------------------- |
| **CLIPモデル**             | 画像Encoder + テキストEncoderを共通埋め込み空間にマップ        | BLIPの基礎となる設計                          |
| **BLIPの構成**             | Vision Encoder（ViT）＋ Text Decoder（Transformer）＋ Q-Former | モジュールの役割を理解する                    |
| **BLIPの学習戦略**         | ①Image-Text Contrastive ②Image-Text Matching ③Captioning    | 3つのタスクをどのように組み合わせて学習するか |
| **Q-Former（BLIP-2特有）** | テキストクエリを通じて画像特徴を抽出                           | 高性能な統合機構を理解                        |
| **生成モデルの接続**       | Q-Former → LLM (例: Flan-T5, Vicuna)                          | テキスト生成部分を理解する                    |

---

## 🧠 **レベル4：実践・応用スキル**

理論を現場で活かすための段階。

### 🔬 必要スキル

| 分野                                | 内容                                                                           |
| ----------------------------------- | ------------------------------------------------------------------------------ |
| **PyTorch実装解析**           | `torchvision`,`transformers`でBLIPやCLIPのコードを読む                     |
| **Hugging Face Transformers** | `from transformers import BlipProcessor, BlipForConditionalGeneration`の実践 |
| **モデル評価**                | BLEU, CIDEr, CLIPScoreなどの評価指標                                           |
| **データ前処理**              | 画像とテキストのペアデータセット（COCOなど）の理解                             |
| **ファインチューニング**      | 特定ドメイン（例: 医療画像, 製造検査）への適用                                 |

---

## 🚀 **レベル5：最先端トピックへ**

BLIPを起点に次の研究テーマへ進む流れ。

| 分野                                                         | 内容                                           |
| ------------------------------------------------------------ | ---------------------------------------------- |
| **BLIP-2 / LLaVA / MiniGPT-4**                         | 大規模言語モデルとの接続構造の理解             |
| **VLMの指示追従学習 (Instruction Tuning)**             | 画像を見て質問に答える仕組み                   |
| **マルチモーダル生成（Image-to-Text, Text-to-Image）** | 生成AI（Stable Diffusion, Flamingo）との橋渡し |
| **LoRA / Adapter / Q-LoRA**                            | 軽量ファインチューニング技術                   |
| **推論最適化**                                         | ONNX / TensorRT / 量子化 など                  |

---

## 📚 学習の進め方（おすすめステップ）

1. **Python × PyTorch基礎** → KaggleやPyTorch公式チュートリアル
2. **CNN & Transformer基礎** → Vision Transformerの論文 or YouTube講義
3. **CLIPを実装してみる** → 画像とキャプションを類似度で対応付け
4. **BLIPモデルをHugging Faceで動かす**
   ```python
   from transformers import BlipProcessor, BlipForConditionalGeneration
   model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
   processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
   ```
5. **BLIPのアーキテクチャを読解**
   * 論文：[https://arxiv.org/abs/2201.12086](https://arxiv.org/abs/2201.12086)
   * BLIP-2：[https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)
6. **独自データでファインチューニング**
7. **LLaVAやMiniGPT-4など次世代VLMへ進む**

---

ご希望があれば、

👉「この流れを半年で習得する学習ロードマップ（教材＋順序付き）」

として具体的な教材と実習コード付きで整理することもできます。

作成しましょうか？
