# RMS Norm

RMSNorm（Root Mean Square Layer Normalization、二乗平均平方根層正規化）は、

=機械学習、特にTransformerモデルのようなニューラルネットワークにおいて使用される**正規化（Normalization）手法の一つ**=です。** **

標準的なLayer Normalizationをよりシンプルかつ効率的にしたもので、モデルの学習を安定させ、高速化するのに役立ちます。** **

主な特徴は以下の通りです。** **

1. 定義と計算方法** **

RMSNormは、入力ベクトルの各要素を、そのベクトルの二乗平均平方根（RMS: Root Mean Square）で割ることによって正規化します。** **

数式で表すと以下のようになります。** **



```math
RMSNorm(x)=\frac{x}{\text{RMS}(x)}=\frac{x}{\sqrt{\frac{1}{n}\sum _{i=1}^{n}x_{i}^{2}}}
```


ここで、

- x: 入力ベクトル
- n: ベクトルの次元数
- $x_i$: ベクトルの各要素


1. 標準的なLayerNormとの違い** **

最も重要な違いは、**平均値の減算を行わない**点です。** **

* **Layer Normalization:** 入力から平均値を引き、標準偏差で割ります。（平均を0、分散を1にする）
* **RMSNorm:** 入力から平均値を引かず、RMS（一種のベクトルの長さや大きさを示す値）で割ります。（分散のみを調整するイメージ）** **

この「平均を引く」操作を省略することで、計算コストが削減され、処理が高速化されます。** **

3. メリット** **

* **計算効率の向上:** 平均値の計算と減算のステップが省略されるため、標準のLayerNormよりも高速に計算できます。
* **性能の維持:** Transformerモデルのようなアーキテクチャでは、平均値の減算がなくても、LayerNormと同等の、あるいは場合によってはそれ以上の学習安定化効果が得られることが経験的に知られています。
* **シンプルさ:** 実装がシンプルです。** **

4. 主な用途** **

主に自然言語処理（NLP）分野のTransformerベースのモデル（BERTやGPTシリーズなど）で利用されています。特に、推論速度が重視される場面や、計算資源を節約したい場合に有効な手法です。** **

まとめると、RMSNormは**計算コストを抑えつつ、ニューラルネットワークの学習を安定させる効率的な正規化手法**であると言えます。** **




