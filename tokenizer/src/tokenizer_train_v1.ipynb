{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar -xzf ldcc-20140209.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCtRyGWkt3HY",
        "outputId": "ac24691d-1e88-45e4-8a60-c040274a2657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-08 09:07:59--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
            "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
            "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8855190 (8.4M) [application/x-gzip]\n",
            "Saving to: â€˜ldcc-20140209.tar.gzâ€™\n",
            "\n",
            "ldcc-20140209.tar.g 100%[===================>]   8.44M  4.70MB/s    in 1.8s    \n",
            "\n",
            "2025-11-08 09:08:01 (4.70 MB/s) - â€˜ldcc-20140209.tar.gzâ€™ saved [8855190/8855190]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SentencePiece (Unigram LM) å­¦ç¿’å‰å¾Œã®æ¯”è¼ƒ\n",
        "# ==========================================\n",
        "import sentencepiece as spm\n",
        "import os"
      ],
      "metadata": {
        "id": "nowC4IqmsaJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1. å­¦ç¿’ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
        "# -------------------------\n",
        "import json, glob\n",
        "\n",
        "with open(\"ja_corpus.txt\", \"w\", encoding=\"utf-8\") as out:\n",
        "    for file in glob.glob(\"text/*/*.txt\"):\n",
        "        with open(file, encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()[3:]  # æœ€åˆ3è¡Œã¯ãƒ˜ãƒƒãƒ€\n",
        "            out.write(\"\".join(lines) + \"\\n\")\n",
        "\n",
        "print(\"âœ… Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqIG3mdRsh7z",
        "outputId": "74c89fdd-6651-4360-ca38-25850ce3e6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 2. å­¦ç¿’å‰ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰: æœªå­¦ç¿’çŠ¶æ…‹\n",
        "# -------------------------\n",
        "def naive_tokenize(text):\n",
        "    \"\"\"æ–‡å­—å˜ä½ã§å˜ç´”ã«åˆ†å‰²ã™ã‚‹ï¼ˆå­¦ç¿’å‰ã®åŸºæº–ï¼‰\"\"\"\n",
        "    return list(text)\n",
        "\n",
        "test_sentence1 = \"ç§ã¯è‡ªç„¶è¨€èªå‡¦ç†ã‚’å‹‰å¼·ã—ã¦ã„ã¾ã™ã€‚\"\n",
        "test_sentence2 = \"ãã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯å¤šãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰è³›åŒã‚’å¾—ã¦ã„ã‚‹ã€‚\"\n",
        "print(\"ğŸ”¹ å­¦ç¿’å‰ï¼ˆæ–‡å­—å˜ä½åˆ†å‰²ï¼‰:\")\n",
        "print(naive_tokenize(test_sentence1))\n",
        "print()\n",
        "print(naive_tokenize(test_sentence2))\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GVLq75kssda",
        "outputId": "836e7d38-95af-4b94-fb40-03c63e2e2db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ å­¦ç¿’å‰ï¼ˆæ–‡å­—å˜ä½åˆ†å‰²ï¼‰:\n",
            "['ç§', 'ã¯', 'è‡ª', 'ç„¶', 'è¨€', 'èª', 'å‡¦', 'ç†', 'ã‚’', 'å‹‰', 'å¼·', 'ã—', 'ã¦', 'ã„', 'ã¾', 'ã™', 'ã€‚']\n",
            "\n",
            "['ã', 'ã®', 'ã‚½', 'ãƒ•', 'ãƒˆ', 'ã‚¦', 'ã‚§', 'ã‚¢', 'ã¯', 'å¤š', 'ã', 'ã®', 'ãƒ¦', 'ãƒ¼', 'ã‚¶', 'ãƒ¼', 'ã‹', 'ã‚‰', 'è³›', 'åŒ', 'ã‚’', 'å¾—', 'ã¦', 'ã„', 'ã‚‹', 'ã€‚']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 3. SentencePiece ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ (Unigram LM)\n",
        "# -------------------------\n",
        "if not os.path.exists(\"ja_spm.model\"):\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=\"ja_corpus.txt\",\n",
        "        model_prefix=\"ja_spm\",\n",
        "        vocab_size=2600,\n",
        "        model_type=\"unigram\",\n",
        "        character_coverage=0.9995,\n",
        "        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
        "    )\n",
        "    print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚\")\n",
        "else:\n",
        "    print(\"âœ… æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hprwcA5Fst_7",
        "outputId": "86ce4fc5-4d5b-4952-ecbc-79c7b446dde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4. å­¦ç¿’å¾Œãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "# -------------------------\n",
        "sp = spm.SentencePieceProcessor(model_file=\"ja_spm.model\")\n",
        "\n",
        "# -------------------------\n",
        "# 5. å­¦ç¿’å¾Œã®åˆ†å‰²ã‚’æ¯”è¼ƒ\n",
        "# -------------------------\n",
        "tokens_after = sp.encode(test_sentence1, out_type=str)\n",
        "\n",
        "print(\"ğŸ”¹ å­¦ç¿’å¾Œï¼ˆSentencePiece åˆ†å‰²ï¼‰:\")\n",
        "print(tokens_after)\n",
        "\n",
        "tokens_after = sp.encode(test_sentence2, out_type=str)\n",
        "print(tokens_after)\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emk__j2PswZi",
        "outputId": "30a639ce-9d36-49e3-a90e-5451ea61ddda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ å­¦ç¿’å¾Œï¼ˆSentencePiece åˆ†å‰²ï¼‰:\n",
            "['â–', 'ç§', 'ã¯', 'è‡ª', 'ç„¶', 'è¨€', 'èª', 'å‡¦', 'ç†', 'ã‚’', 'å‹‰', 'å¼·', 'ã—ã¦', 'ã„', 'ã¾ã™', 'ã€‚']\n",
            "['â–', 'ãã®', 'ã‚½', 'ãƒ•', 'ãƒˆ', 'ã‚¦', 'ã‚§', 'ã‚¢', 'ã¯', 'å¤š', 'ã', 'ã®', 'ãƒ¦', 'ãƒ¼', 'ã‚¶', 'ãƒ¼', 'ã‹ã‚‰', 'è³›', 'åŒ', 'ã‚’', 'å¾—', 'ã¦ã„ã‚‹', 'ã€‚']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvqeeZMAwebP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}