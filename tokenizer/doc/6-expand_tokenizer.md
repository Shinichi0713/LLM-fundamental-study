äº†è§£ã—ã¾ã—ãŸï¼
ä¸Šè¨˜ã® **Sparse + RoPE Hybrid Transformer LLM** ç”¨ã«ã€å®Ÿéš›ã«åˆ©ç”¨ã§ãã‚‹ **ã‚«ã‚¹ã‚¿ãƒ  BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆsentencepiece / HuggingFace tokenizersï¼‰** ã‚’ä½œã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚
ã“ã“ã§ã¯ **Hugging Face `tokenizers` ãƒ©ã‚¤ãƒ–ãƒ©ãƒª** ã‚’ç”¨ã„ãŸ BPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä½œæˆä¾‹ã‚’æç¤ºã—ã¾ã™ã€‚

---

## ğŸ”§ æ‰‹é †æ¦‚è¦

1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å­¦ç¿’ã«ä½¿ã†ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”¨æ„ï¼ˆä»»æ„ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ `.txt`ï¼‰
2. `tokenizers` ã§ BPE ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
3. Special Tokenï¼ˆPAD, CLS, MASK ãªã©ï¼‰è¿½åŠ 
4. ä¿å­˜ & PyTorch ã‹ã‚‰å‘¼ã³å‡ºã™

---

## âœ¨ å®Ÿè£…ä¾‹ï¼šBPE ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´

### requirements

```
pip install tokenizers
```

### tokenizer_train.py

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors
from tokenizers.normalizers import NFD, StripAccents, Sequence
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing


def train_bpe_tokenizer(
        files,
        vocab_size=20000,
        save_path="bpe-tokenizer.json",
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[MASK]", "[SEP]"]
):
    # BPE tokenizer
    tokenizer = Tokenizer(models.BPE())

    # Normalization: unicode canonical form + accents strip
    tokenizer.normalizer = Sequence([
        NFD(),
        StripAccents()
    ])
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

    # Trainer
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        min_frequency=2,
        special_tokens=special_tokens
    )

    # Training
    tokenizer.train(files, trainer)

    # Add post-processing <CLS> ... <SEP>
    tokenizer.post_processor = TemplateProcessing(
        single="[CLS] $0 [SEP]",
        pair="[CLS] $A [SEP] $B [SEP]",
        special_tokens=[("[CLS]", 2), ("[SEP]", 4)]
    )

    tokenizer.decoder = decoders.BPEDecoder()

    tokenizer.save(save_path)
    print(f"Tokenizer saved to {save_path}")


if __name__ == "__main__":
    train_bpe_tokenizer(
        files=["dataset.txt"],   # å­¦ç¿’ç”¨ãƒ†ã‚­ã‚¹ãƒˆ
        vocab_size=20000,
        save_path="rope_sparse_tokenizer.json"
    )
```

---

## ğŸ§ª å‹•ä½œãƒ†ã‚¹ãƒˆ

```python
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_file("rope_sparse_tokenizer.json")

sample = "Sparse RoPE Hybrid Attention ã‚’è©¦ã—ã¦ã„ã¾ã™ã€‚"
encoded = tokenizer.encode(sample)

print("tokens:", encoded.tokens)
print("ids:", encoded.ids)
```

---

## ğŸš€ LLM ã¸çµ„ã¿è¾¼ã¿

```python
def tokenize_inputs(texts, tokenizer, max_len=128):
    batch_ids = []
    global_mask = []

    for text in texts:
        enc = tokenizer.encode(text)
        ids = enc.ids[:max_len]
        pad_len = max_len - len(ids)
        ids += [0] * pad_len  # [PAD]

        # Global token: æœ€åˆã¨æ–‡é ­ punctuation ã‚’ global ã¨ã™ã‚‹ä¾‹
        gmask = [False] * max_len
        gmask[0] = True  # CLS ã¯å¸¸ã« global
        global_mask.append(gmask)
        batch_ids.append(ids)

    import torch
    return torch.tensor(batch_ids), torch.tensor(global_mask)


input_ids, global_mask = tokenize_inputs(
    ["ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡ç« ã§ã™ã€‚Sparse Attention å®Ÿé¨“ä¸­ã§ã™ã€‚"],
    Tokenizer.from_file("rope_sparse_tokenizer.json")
)
```

LLM ã¸ï¼š

```python
logits, attn = model(input_ids.to(device), global_mask.to(device))
```

---

## âš™ Special Tokens ã®è¨­è¨ˆãƒã‚¤ãƒ³ãƒˆ

| Token    | å½¹å‰²            | global mask æ¨å¥¨ |
| -------- | ------------- | -------------- |
| `[CLS]`  | æ–‡å…¨ä½“ã®è¦ç´„ï¼ã‚°ãƒ­ãƒ¼ãƒãƒ«é ­ | True           |
| `[SEP]`  | æ–‡åˆ†å‰²           | Optional       |
| `[MASK]` | MLM ã® mask    | False          |
| `[PAD]`  | padding       | False          |
| `[UNK]`  | unknown       | False          |

â†’ Hybrid Sparse + RoPE ã®å ´åˆã€**CLS ã‚„ section header ã‚’ global token** ã«ã™ã‚‹ã¨å­¦ç¿’åŠ¹ç‡ãŒéå¸¸ã«è‰¯ããªã‚Šã¾ã™ã€‚

---

## ğŸ“¦ ã‚‚ã—è¿½åŠ ã§å¿…è¦ãªã‚‰

* SentencePiece ç‰ˆ tokenizer
* å­¦ç¿’ãƒ‡ãƒ¼ã‚¿è‡ªå‹•å‰å‡¦ç†ï¼ˆWikitext / Japanese Wikipedia / Livedoorï¼‰
* ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ LLM ã® HuggingFace Transformers åŒ–
* ç”Ÿæˆç”¨ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¢ãƒ‡ãƒ«ï¼ˆCausal attention åŒ–ï¼‰

