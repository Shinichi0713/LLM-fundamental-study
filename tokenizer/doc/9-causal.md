## サブワード
サブワードとは、 **「単語（Word）」と「文字（Character）」の中間に位置する単位**で分割する仕組みのことです。
「よく使われる言葉の塊」は一つの単位として扱い、馴染みのない言葉は「意味を持つ小さなパーツ」に分解します。


### サブワードの基本的な仕組み

サブワード化のプロセスは、主に **「出現頻度」** に基づいています。最新のLLMで主流の **BPE（Byte Pair Encoding）** というアルゴリズムを例に説明します。

#### 1. 頻出するパターンの合体

学習データの中で、隣り合う文字の組み合わせをカウントし、よく出てくる組み合わせを一つの「新しい単位」として登録していきます。

* **例：** 「h」と「e」が頻繁に隣り合うなら、それを合体させて「he」という一つのトークンにします。さらに「he」と「lp」がよくセットになるなら「help」というトークンにします。

#### 2. レアな単語の分解

逆に、めったに出てこない単語や新しい単語は、合体が進まないため、細かいパーツに分解されたままになります。

* **例：** 「unhelpfully」という珍しい単語
* 単語単位：辞書にない（エラーになる）
* **サブワード単位：** `un` + `help` + `fully` に分解
* モデルは「否定」＋「助ける」＋「〜の状態」というパーツごとの意味から、全体の意味を推測できます。



### 視覚的なイメージ：ブロック遊び

サブワードの仕組みは、**レゴブロック**に例えると分かりやすいです。

* **文字単位：** バラバラの最小パーツ。何でも作れるが、組み立て（計算）が大変。
* **単語単位：** 完成品の家や車。そのまま使えるが、持っていないデザイン（未知語）には対応できない。
* **サブワード：** 「窓」「タイヤ」「屋根」といった、**汎用性の高い中間パーツ**。これらを組み合わせることで、どんな複雑な形でも効率よく、かつ柔軟に組み立てられます。

## 使われる理由

特に、Googleが開発した「WordPiece」や、現代のLLMで広く使われる「BPE（Byte Pair Encoding）」がその代表例です。

1. 未知語（OOV）問題の解消

単語単位のトークナイザでは、辞書に載っていない単語（新語、造語、誤字など）に出会うと「未知語（Unknown）」として処理され、情報の意味が完全に失われてしまうという弱点がありました。

* **単語単位:** 「ChatGPT」という言葉を辞書に持っていないと、ただの「不明な記号」になってしまいます。
* **サブワード単位:** 「Chat」と「GPT」に分解して処理できます。たとえ「GPT」を知らなくても、「G」「P」「T」という文字レベルまで分解すれば、**「エラーを出さずに何らかの計算を継続できる」**ようになります。

2. 語彙数（ボキャブラリーサイズ）と計算効率のバランス

モデルが覚える「辞書のサイズ」は、計算コストに直結します。

* **単語単位の問題:** あらゆる単語（running, runs, ran...）を個別に登録すると、辞書が数十万〜数百万語に膨れ上がり、メモリを大量に消費します。
* **文字単位の問題:** 辞書は最小（アルファベット数十文字など）になりますが、一文字には意味がほとんどないため、文章が非常に長くなり、計算が非効率になります。
* **サブワードの解決策:** 「run」と「ning」のように、意味を持つ最小単位を組み合わせます。これにより、**数万程度の語彙数で、実質的に無限の単語を表現できる**ようになり、計算効率と表現力の「いいとこ取り」が実現しました。


3. 形態的・意味的特徴の維持

英語の「-ing（進行形）」や「-ed（過去形）」、日本語の接頭辞・接尾辞など、言葉のパーツには共通の意味があります。

* **共通性の活用:** 「play」「playing」「player」を全く別の単語として扱うのではなく、「play」という共通のサブワードを持たせることで、モデルはこれらが関連していることを学習しやすくなります。
* **言語の壁を越える:** 多言語モデルにおいても、サブワード化することで異なる言語間での共通パーツ（例：ラテン語由来の語根など）を効率的に学習できるようになります。

---

## 💡 まとめ：なぜサブワードなのか？

サブワードは、**「辞書のサイズをコンパクトに抑えつつ、どんな複雑な単語が来ても意味を壊さずに処理できる柔軟性」**を持っていたため、LLMの標準となりました。

現在は、文字（Unicode）よりもさらに深い **Byteレベル（Byte-level BPE）** でトークナイズを行う手法が、Llama 3などの最新LLMでは主流となっています。

次は、実際にサブワードがどのように切り分けられるのか、その具体的なアルゴリズム（BPEなど）について詳しく見てみますか？