ä»¥ä¸‹ã¯ã€**ã€Œæ–‡ç«  â†’ ãƒˆãƒ¼ã‚¯ãƒ³ â†’ ãƒ™ã‚¯ãƒˆãƒ« â†’ å¯è¦–åŒ–ã€**ã®æµã‚Œã‚’
**å®Ÿéš›ã«ç›®ã§è¦‹ã¦ç†è§£ã§ãã‚‹æœ€å°ã‹ã¤æ•™è‚²çš„ãªå®Ÿè£…ä¾‹**ã§ã™ã€‚

ç›®çš„ã¯ã€Œç²¾åº¦ã€ã§ã¯ãªãã€
**LLMå†…éƒ¨ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ã‚’ç›´æ„Ÿçš„ã«ç†è§£ã™ã‚‹ã“ã¨**ã«ã‚ã‚Šã¾ã™ã€‚

---

# å…¨ä½“åƒï¼ˆã¾ãšæ¦‚å¿µï¼‰

```
æ–‡ç« 
 â†“
ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ï¼ˆã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ï¼‰
 â†“
ãƒˆãƒ¼ã‚¯ãƒ³ID
 â†“
Embeddingå±¤
 â†“
é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¾‹ï¼š768æ¬¡å…ƒï¼‰
 â†“
æ¬¡å…ƒå‰Šæ¸›ï¼ˆPCA / t-SNEï¼‰
 â†“
2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆã§å¯è¦–åŒ–
```

---

# ä½¿ç”¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

* transformersï¼ˆBERTï¼‰
* torch
* scikit-learn
* matplotlib

Colab / ãƒ­ãƒ¼ã‚«ãƒ«ã©ã¡ã‚‰ã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚

---

# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æº–å‚™

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

from transformers import BertTokenizer, BertModel
from sklearn.decomposition import PCA
```

---

# 2. å¯è¦–åŒ–ã—ãŸã„æ–‡ç« 

```python
sentence = "The cat sits on the mat."
```

---

# 3. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®å¯è¦–åŒ–

```python
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

tokens = tokenizer.tokenize(sentence)
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print("Tokens:")
for t, i in zip(tokens, token_ids):
    print(f"{t:>10} -> {i}")
```

### å‡ºåŠ›ä¾‹ï¼ˆé‡è¦ï¼‰

```
the        -> 1996
cat        -> 4937
sits       -> 7719
on         -> 2006
the        -> 1996
mat        -> 13523
.          -> 1012
```

ã“ã“ã§ç†è§£ã™ã¹ãç‚¹ï¼š

* å˜èª â‰  ãƒˆãƒ¼ã‚¯ãƒ³ID
* åŒã˜å˜èªï¼ˆtheï¼‰ã¯åŒã˜ID
* ãƒ¢ãƒ‡ãƒ«ã¯ **æ–‡å­—åˆ—ã‚’ç›´æ¥ç†è§£ã—ã¦ã„ãªã„**

---

# 4. BERTã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–

```python
model = BertModel.from_pretrained("bert-base-uncased")
model.eval()

inputs = tokenizer(sentence, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

# last_hidden_state: [batch, seq_len, hidden_dim]
embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_dim]
```

ç¢ºèªï¼š

```python
print("Embedding shape:", embeddings.shape)
```

```
Embedding shape: torch.Size([8, 768])
```

ğŸ‘‰ **å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒ768æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«ãªã£ã¦ã„ã‚‹**

---

# 5. é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã‚’2æ¬¡å…ƒã«åœ§ç¸®ï¼ˆPCAï¼‰

```python
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings.numpy())
```

---

# 6. ãƒ™ã‚¯ãƒˆãƒ«ã®å¯è¦–åŒ–ï¼ˆæ ¸å¿ƒï¼‰

```python
plt.figure(figsize=(8, 6))

for i, token in enumerate(tokens):
    x, y = embeddings_2d[i]
    plt.scatter(x, y)
    plt.text(x + 0.01, y + 0.01, token)

plt.title("Token Embeddings Visualization (PCA)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.show()
```

---

# 7. ä½•ãŒã€Œè¦‹ãˆã¦ã„ã‚‹ã€ã®ã‹

ã“ã®ãƒ—ãƒ­ãƒƒãƒˆã§ç†è§£ã§ãã‚‹ã“ã¨ï¼š

### â‘  æ„å‘³ã®è¿‘ã„å˜èªã¯è¿‘ãã«é…ç½®ã•ã‚Œã‚‹

* `the` åŒå£«ãŒè¿‘ã„
* æ©Ÿèƒ½èªã¨å†…å®¹èªãŒåˆ†ã‹ã‚Œã‚‹å‚¾å‘

---

### â‘¡ æ–‡è„ˆä¾å­˜ã§ãƒ™ã‚¯ãƒˆãƒ«ãŒå¤‰ã‚ã‚‹

BERTã¯ **æ–‡è„ˆä¾å­˜Embedding** ãªã®ã§ã€

```text
bank (river)
bank (money)
```

ã¯ **ç•°ãªã‚‹ä½ç½®** ã«ãªã‚Šã¾ã™ã€‚

---

### â‘¢ ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€Œç‚¹ã€ã§ã¯ãªãã€Œæ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã€

LLMã¯

> å˜èªã‚’è¦šãˆã¦ã„ã‚‹
> ã§ã¯ãªã
> å˜èªã®æ„å‘³ç©ºé–“ä¸Šã®ä½ç½®ã‚’æ“ä½œã—ã¦ã„ã‚‹

---

# 8. CLSãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ–‡ç« å…¨ä½“ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã®å¯è¦–åŒ–

```python
cls_embedding = embeddings[0]  # [CLS]
print("CLS vector shape:", cls_embedding.shape)
```

* `[CLS]` ã¯ **æ–‡ç« å…¨ä½“ã®è¦ç´„**
* æ–‡åˆ†é¡ãƒ»æ¤œç´¢ã§ä½¿ã‚ã‚Œã‚‹

---

# 9. å¿œç”¨ï¼šè¤‡æ•°æ–‡ã‚’ä¸¦ã¹ã¦å¯è¦–åŒ–

```python
sentences = [
    "The cat sits on the mat.",
    "A dog lies on the floor.",
    "I love deep learning."
]

all_embeddings = []
labels = []

for s in sentences:
    inputs = tokenizer(s, return_tensors="pt")
    with torch.no_grad():
        out = model(**inputs)
    cls = out.last_hidden_state[0][0]
    all_embeddings.append(cls.numpy())
    labels.append(s)

pca = PCA(n_components=2)
emb_2d = pca.fit_transform(np.array(all_embeddings))

plt.figure(figsize=(8,6))
for i, label in enumerate(labels):
    x, y = emb_2d[i]
    plt.scatter(x, y)
    plt.text(x+0.01, y+0.01, label[:15])

plt.title("Sentence Embeddings (CLS token)")
plt.show()
```

ğŸ‘‰ **æ„å‘³ã®è¿‘ã„æ–‡ãŒè¿‘ãã«é…ç½®ã•ã‚Œã‚‹**

---

# 10. ä¸€è¨€ã§ã¾ã¨ã‚ã‚‹ã¨

> **LLMã¯æ–‡ç« ã‚’ã€Œå˜èªã®é›†åˆã€ã§ã¯ãªã
> ã€Œæ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ã®è»Œè·¡ã€ã¨ã—ã¦å‡¦ç†ã—ã¦ã„ã‚‹**

* ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼šé›¢æ•£åŒ–
* Embeddingï¼šé€£ç¶šç©ºé–“ã¸å†™åƒ
* Attentionï¼šãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®é–¢ä¿‚è¨ˆç®—

![1766270179459](image/8-sentence_vector/1766270179459.png)


![1766271215577](image/8-sentence_vector/1766271215577.png)
