ã„ã„ã§ã™ã­ â€” å®Ÿå‹™ã§ä½¿ãˆã‚‹ **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶å­¦ç¿’ã‚³ãƒ¼ãƒ‰**ã‚’äºŒé€šã‚Šç”¨æ„ã—ã¾ã™ã€‚

ç”¨é€”ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã¦ãã ã•ã„ï¼š

* **A. SentencePieceï¼ˆå…¬å¼ã§æ‰‹è»½ã€Unigram/BPEå¯¾å¿œï¼‰** â€” å¤šè¨€èªãƒ»æ—¥æœ¬èªã«å¼·ã„ã€‚
* **B. ğŸ¤— Tokenizersï¼ˆHugging Face ã®é«˜é€Ÿãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰** â€” Byte-level BPE / WordPiece / Unigram ã‚’æŸ”è»Ÿã«é«˜é€Ÿã«å­¦ç¿’ã§ãã‚‹ã€‚

ã©ã¡ã‚‰ã‚‚ã€Œå·¨å¤§ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ï¼‰ã€ã‚’å…¥åŠ›ã«ã—ã¦èªå½™ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

ã‚³ãƒãƒ³ãƒ‰ã¯ Colab / ãƒ­ãƒ¼ã‚«ãƒ«ã§ãã®ã¾ã¾å®Ÿè¡Œã§ãã¾ã™ã€‚

---

## A. SentencePiece ã‚’ä½¿ã†ï¼ˆç°¡å˜ãƒ»å®‰å®šï¼‰

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install sentencepiece
```

### å­¦ç¿’ã‚³ãƒ¼ãƒ‰ï¼ˆBPE ã¾ãŸã¯ Unigramï¼‰

```python
import sentencepiece as spm
import pathlib

# --- æº–å‚™ ---
# è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦å­¦ç¿’ç”¨ input.txt ã‚’ä½œã‚‹ã®ãŒä¸€èˆ¬çš„
# ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ data/ ãƒ•ã‚©ãƒ«ãƒ€ã® *.txt ã‚’å­¦ç¿’ã«ä½¿ã†
files = [str(p) for p in pathlib.Path("data").glob("*.txt")]
assert files, "data/*.txt ã‚’ç”¨æ„ã—ã¦ãã ã•ã„"

input_files = ",".join(files)

# å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
model_prefix = "spm_model"   # å‡ºåŠ›: spm_model.model, spm_model.vocab
vocab_size = 32000
model_type = "unigram"      # 'unigram' or 'bpe' or 'word' or 'char'
character_coverage = 0.9995 # æ—¥æœ¬èªãªã‚‰ 1.0 / 0.9995 ãªã©

# --- å­¦ç¿’ ---
spm.SentencePieceTrainer.Train(
    input=input_files,
    model_prefix=model_prefix,
    vocab_size=vocab_size,
    character_coverage=character_coverage,
    model_type=model_type,
    user_defined_symbols=["<s>","</s>","<pad>","<unk>"]  # å¿…è¦ãªã‚‰
)
print("trained:", model_prefix + ".model")
```

### ä½¿ã„æ–¹ï¼ˆãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰

```python
import sentencepiece as spm
sp = spm.SentencePieceProcessor(model_file="spm_model.model")

text = "äººå·¥çŸ¥èƒ½ã¨æ©Ÿæ¢°å­¦ç¿’ã¯é¢ç™½ã„ã€‚"
pieces = sp.encode_as_pieces(text)
ids = sp.encode_as_ids(text)
print("pieces:", pieces)
print("ids:", ids)
print("decoded:", sp.decode_ids(ids))
```

---

## B. Hugging Face `tokenizers`ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãƒ»é«˜é€Ÿï¼‰

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install tokenizers
```

### 1) Byte-Level BPEï¼ˆGPTã‚¹ã‚¿ã‚¤ãƒ«ï¼‰

```python
from tokenizers import ByteLevelBPETokenizer
from pathlib import Path

# ç”¨æ„: data/*.txt
paths = [str(p) for p in Path("data").glob("*.txt")]

tokenizer = ByteLevelBPETokenizer()

# å­¦ç¿’ (vocab_size, special_tokens)
tokenizer.train(files=paths, vocab_size=50000, min_frequency=2,
                special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"])

# ä¿å­˜
tokenizer.save_model(".", "my_bytebpe")

# ä½¿ç”¨ä¾‹
enc = tokenizer.encode("äººå·¥çŸ¥èƒ½ãŒé€²åŒ–ã—ã¦ã„ã¾ã™ã€‚")
print(enc.tokens)
print(enc.ids)
```

### 2) Unigramï¼ˆSentencePieceã¨åŒã˜ç™ºæƒ³ï¼‰ via `tokenizers`

```python
from tokenizers import Tokenizer
from tokenizers.models import Unigram
from tokenizers.trainers import UnigramTrainer
from tokenizers.pre_tokenizers import Whitespace

paths = [str(p) for p in Path("data").glob("*.txt")]

tokenizer = Tokenizer(Unigram())
tokenizer.pre_tokenizer = Whitespace()  # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ç©ºç™½å˜ä½ã§åˆ†å‰²ã—ã¦å€™è£œã‚’ä½œã‚‹å ´åˆ
trainer = UnigramTrainer(vocab_size=32000, special_tokens=["<s>", "<pad>", "</s>", "<unk>"])

tokenizer.train(paths, trainer)
tokenizer.save("tokenizer-unigram.json")

# ä½¿ç”¨ä¾‹
encoded = tokenizer.encode("è‡ªç„¶è¨€èªå‡¦ç†ã‚’å­¦ã¶ã€‚")
print(encoded.tokens)
```

### 3) WordPieceï¼ˆBERTç³»ï¼‰

```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.normalizers import NFKC
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
tokenizer.normalizer = NFKC()
tokenizer.pre_tokenizer = Whitespace()
trainer = WordPieceTrainer(vocab_size=30000, special_tokens=["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"])

paths = [str(p) for p in Path("data").glob("*.txt")]
tokenizer.train(paths, trainer)
tokenizer.save("tokenizer-wordpiece.json")
```

---

## å®Ÿå‹™ã®æ³¨æ„ç‚¹ï¼ˆãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰

* **ã‚³ãƒ¼ãƒ‘ã‚¹ã¯å‰å‡¦ç†ã‚’ä¸å¯§ã«** ï¼šHTMLã‚¿ã‚°é™¤å»ã€æ­£è¦åŒ–ï¼ˆUnicode NFKCï¼‰ã€ä¸è¦è¡Œå‰Šé™¤ãªã©ã€‚
* **vocab_size** ã¯ 8kã€œ100k ã®é–“ã§ç”¨é€”ã«å¿œã˜ã¦ã€‚æ—¥æœ¬èªãªã‚‰ 8kã€œ32k ãŒå¤šã„ã€‚
* **special tokens** ï¼ˆ`<pad>` `<unk>` `<s>` `</s>`ï¼‰ã¯å¿…ãšè¨­è¨ˆã—ã¦ãŠãã€‚
* **byte-level** ï¼ˆãƒã‚¤ãƒˆå˜ä½ï¼‰ã‚’ä½¿ã†ã¨çµµæ–‡å­—ã‚„ã‚³ãƒ¼ãƒ‰ã«å¼·ã„ï¼ˆGPTç³»æ¨å¥¨ï¼‰ã€‚
* SentencePiece ã® `character_coverage` ã‚’æ—¥æœ¬èªãªã‚‰ 1.0 ã«è¿‘ãè¨­å®šã€‚
* å­¦ç¿’ã¯ CPUã§ã‚‚å¯èƒ½ã ãŒå¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã€‚

---

## å‚è€ƒï¼šå°ã•ãªãƒ‡ãƒ¼ã‚¿ã§è©¦ã™ãƒ•ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆã¾ã¨ã‚ã‚³ãƒ¼ãƒ‰ï¼‰

å¿…è¦ãªã‚‰ã€Œ1ãƒ•ã‚¡ã‚¤ãƒ«ã§å…¨éƒ¨ã‚„ã‚‹ã€ã‚µãƒ³ãƒ—ãƒ«ã‚‚æç¤ºã—ã¾ã™ï¼ˆå­¦ç¿’â†’ä¿å­˜â†’ãƒ­ãƒ¼ãƒ‰â†’æ¯”è¼ƒï¼‰ã€‚ä½œã‚Šã¾ã™ã‹ï¼Ÿ

ã©ã®æ–¹å¼ã‚’æœ¬æ ¼çš„ã«ä½¿ã„ãŸã„ã§ã™ã‹ï¼Ÿï¼ˆ`sentencepiece` / `ByteLevelBPE` / `Unigram` / `WordPiece`ï¼‰

å¸Œæœ›ã«åˆã‚ã›ã¦ã€Colabãƒãƒ¼ãƒˆé¢¨ã«å®Ÿè¡Œã‚»ãƒ«ã‚’æ•´ãˆã¾ã™ã€‚
