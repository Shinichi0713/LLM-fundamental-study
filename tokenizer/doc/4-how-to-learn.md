# 学習の仕組み

SentencePieceが文章分割（サブワード分割）を**学習**する仕組みを、できるだけわかりやすく説明します。

---

## ✅ SentencePieceとは？

SentencePieceは、文章を **単語の境界（スペース）に頼らず** 、文字列として扱い、その中から**よく出てくる文字のまとまり（サブワード）を学習**する仕組みです。

> 例：
>
> 「国際交流 → 国 + 際 + 交流」
>
> 「International → Inter + nat + ional」

---

## ✅ なぜ学習が必要？

自然言語には**未知語問題**があります。

* 新しい固有名詞（例：ChatGPT、令和、iPhone 16）
* 命名規則が曖昧（漢字・英語・数字混在）

すべての単語を辞書に入れるのは無理 ⇒ **サブワードで表現**すれば未知語にも対応できます。

---

## ✅ 学習の流れ（シンプル版）

### 1) 全ての文字を基本単位にする

まず文章を**文字単位**にバラす

例: 「国際交流」→「国」「際」「交」「流」

### 2) 出現頻度を調べる

テキスト中で**よく現れる文字の組み合わせ**を探す

例:

| 文字列 | 出現数      |
| ------ | ----------- |
| 国     | 100         |
| 際     | 100         |
| 国際   | 95 ←頻出！ |
| 交流   | 80          |

### 3) 頻度の高いペアを1つのトークンにまとめる（マージ）

**国 + 際 → 国際**

**交 + 流 → 交流**

のように大きな単位を作っていく

### 4) 語彙サイズになるまで繰り返す

例えば**32,000語**になるまで結合していく

---

## ✅ どんな手法で学んでる？

SentencePieceは主に **2方式**を実装：

| 手法                     | 説明                                               | 特徴                            |
| ------------------------ | -------------------------------------------------- | ------------------------------- |
| BPE (Byte Pair Encoding) | 頻出ペアを結合                                     | 安定・高速、Transformer系で主流 |
| Unigram LM               | サブワード候補を作り、確率モデルで最適な集合を選ぶ | より柔軟、T5やALBERTで使用      |

---

## ✅ MeCabとの違い

|            | SentencePiece | MeCab                |
| ---------- | ------------- | -------------------- |
| 分割単位   | サブワード    | 日本語の語（形態素） |
| 辞書       | 学習で生成    | 固定辞書             |
| 言語依存   | なし          | 日本語特化           |
| 未知語対応 | 強い          | 辞書にない語は苦手   |

> MeCab: **人間が作った辞書＋ルール**
>
> SentencePiece: **データから自動学習（統計・確率）**

---

## ✅ まとめ

SentencePieceは、

* 文章を文字列として扱い
* よく出てくる部分をサブワードとして学習し
* 未知語も柔軟に扱える

という**データドリブンな文章分割法**です。


# BPE VS Unigram

BPEとUnigramの違いを**数式レベルの理解**につながる形で、できるだけやさしく説明します。

---

## ✅ ざっくり結論

| 手法                     | 発想                   | 数学的特徴                          | 例えると                       |
| ------------------------ | ---------------------- | ----------------------------------- | ------------------------------ |
| BPE (Byte Pair Encoding) | 頻度の高い文字列を結合 | **貪欲法（greedy）**          | よく使う単語を辞書に入れる方式 |
| Unigram LM               | 分割の確率を最大化     | **確率モデル + 期待値最適化** | 文章全体の最適な辞書を選ぶ方式 |

---

## ✅ BPE（Byte Pair Encoding）の数学的考え方

### **目的**

最も頻度が高いペア（文字列）を繰り返しマージし語彙を作る。

### 数式イメージ

```
語彙 V を作るために  
最も頻度の高いペア (a, b) = argmax count(a b)
を選び、ab を新しいトークンとして単語辞書に追加
```

* 入力文からペアの頻度を数える
* 一番出現率の高いペアを結合
* それを繰り返す

> **Greedy（局所最適）**
>
> 「今いちばん使われてる組み合わせ」をひたすら採用。

### 特徴

* シンプルで高速
* 一度結合したルールは**戻せない**
* 近似解（最適とは限らない）

---

## ✅ Unigram Language Model の数学的考え方

### **目的**

トークン集合Vから、

 **文の生成確率が最大になる語彙集合を選択する** 。

### 数式（イメージ）

文章 x のトークン分割 T に対して

```
P(x | V) = Σ（x の分割 T） Π（トークン t∈T） P(t)
```

目標：

```
V を選んで、P(x | V) を最大化
```

* 全トークンに**確率値 P(t)** がある
* 文全体をいろんな形で分割して、
* **総確率が最大になるように辞書を最適化**

### 実際の学習

**EMアルゴリズム**を使う

（E-stepで分割確率を計算、M-stepで語彙確率更新）

> **グローバル最適へ近づく方式**
>
> BPEより理論的に美しい

### 特徴

* 複数の分割候補を比較し最適な語彙を決める
* 不要なサブワードは確率が低くなり自動で削除
* BPEより柔軟だが計算は重い

---

## ✅ 例で直感理解

### 文

```
international
```

### BPE

頻度の高い順にくっつける：

```
i n → in
t e r → ter
...
international → inter + national
```

> **手順に従うだけ** （貪欲）

### Unigram

「どんな分割なら文全体の確率が最も高いか？」

候補例：

```
international
inter + national
in + ter + nation + al
```

それぞれの確率を計算し、

**最も確率が高くなる語彙集合**を選ぶ。

---

## ✅ まとめ

| 観点           | BPE                         | Unigram LM                                  |
| -------------- | --------------------------- | ------------------------------------------- |
| 数学的性質     | Greedy (頻度最大のペア結合) | 確率モデル（最大化）                        |
| 最適化         | 局所最適                    | 準グローバル最適                            |
| 方式           | 単純な統計                  | EMアルゴリズム                              |
| 柔軟性         | 低い                        | 高い                                        |
| 使ってるモデル | GPT-2, CLIPなど             | SentencePieceのデフォルト（T5, ALBERTなど） |

---

## ✅ 一言メモ

> **BPE = 「よく使う部品を作る方式」**
>
> **Unigram = 「最も良い部品セットを探す方式」**

だから最近はUnigramの人気が高いです。

---

## ✅ 次に深掘りできます

興味があれば以下も説明できます👇

* EMアルゴリズムの式展開
* Unigram VS WordPiece (BERT)
* BPE/Unigramを1文ずつ手で分割する例
* それぞれのPyTorch/NumPyコード実装

どれを見たいですか？
